{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6c49466",
   "metadata": {},
   "source": [
    "![All-test](http://drive.google.com/uc?export=view&id=1bLQ3nhDbZrCCqy_WCxxckOne2lgVvn3l)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5cd91af",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2204e2c4",
   "metadata": {},
   "source": [
    "# 2.7.3 Gradient Boosted Survival Model  {.unnumbered}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71dbbf98",
   "metadata": {},
   "source": [
    "\n",
    "Gradient Boosted Survival Models are a powerful class of machine learning models that extend traditional survival analysis techniques by incorporating gradient boosting methods. These models are particularly useful for predicting time-to-event outcomes while handling censored data effectively. This notebook provides an introduction to different types of Gradient Boosted Survival Models, focusing on their implementation in R.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ddab19",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54084d2c",
   "metadata": {},
   "source": [
    "\n",
    "A **Gradient Boosted Survival Model** is a machine learning approach that combines gradient boosting with survival analysis to predict the time until an event of interest occurs, such as failure, death, or churn. Survival models deal with time-to-event data, which is often censored (i.e., the event hasn’t occurred for some subjects during the observation period). Gradient boosting enhances these models by iteratively building an ensemble of weak learners (typically decision trees) to minimize a loss function tailored to survival data.\n",
    "\n",
    "There are several types of gradient boosted survival models, each tailored to specific survival analysis frameworks or assumptions. Thee most widely used models are **Cox Proportional Hazards-Based Gradient Boosting (CoxBoost)**,  **Gradient Boosting Survival Tree (GBST)**, and **Accelerated Failure Time (AFT) Gradient Boosting**. CoxBoost and GBST are fundamentally very similar, and in many contexts, GBST is simply a specific implementation of CoxBoost using decision trees as base learners. Coxboost can use various weak learners (trees, linear models, etc.), however, GBST is specifically designed for survival analysis and uses survival-specific splitting criteria in the trees. AFT Gradient Boosting, on the other hand, models the log-survival time directly and is based on a different assumption about how covariates affect survival time.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33396385",
   "metadata": {},
   "source": [
    "### Key Components"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c0c5e38",
   "metadata": {},
   "source": [
    "\n",
    "1. `Base Learners`: Typically decision trees, which are combined to form the final model.\n",
    "2. `Loss Function`: A survival-specific loss, such as partial likelihood (Cox model-inspired) or a pseudo-residual-based loss.\n",
    "3. `Gradient Descent`: Used to minimize the loss by adjusting the model in the direction of steepest descent.\n",
    "4. `Regularization`: Techniques like shrinkage (learning rate) or tree depth constraints prevent overfitting.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aade1053",
   "metadata": {},
   "source": [
    "## How it works "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4500740b",
   "metadata": {},
   "source": [
    "\n",
    "Gradient boosting iteratively adds decision trees, each correcting the errors of the previous ones, by optimizing a loss function (e.g., negative log-likelihood or a survival-specific loss). The model outputs predictions like survival probabilities, hazard functions, or cumulative risks over time, depending on the specific implementation.\n",
    "\n",
    "In survival analysis, the model accounts for:\n",
    "\n",
    "- `Censored data`: Observations where the event hasn’t occurred by the end of the study.\n",
    "- `Time-to-event`: Predicting not just if an event occurs but when.\n",
    "- `Risk or hazard`: Estimating the probability of the event at different time points.\n",
    " \n",
    " \n",
    "- $ T_i $: True survival time for subject $ i $\n",
    "- $ C_i $: Censoring time\n",
    "- $ t_i = \\min(T_i, C_i) $: Observed time\n",
    "- $ \\delta_i = \\mathbb{I}(T_i \\leq C_i) $: Event indicator (1 if event observed, 0 if censored)\n",
    "\n",
    "We aim to model survival using a set of covariates $ \\mathbf{x}_i \\in \\mathbb{R}^p $.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd08637",
   "metadata": {},
   "source": [
    "###  Cox Proportional Hazards-Based Gradient Boosting (CoxBoost)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "684ba169",
   "metadata": {},
   "source": [
    "\n",
    "Cox Proportional Hazards-Based Gradient Boosting is a machine learning approach that extends the Cox proportional hazards model by using gradient boosting to model the log-hazard function as a sum of decision trees. It assumes that the hazard ratios are constant over time (proportional hazards) and optimizes the negative log-partial likelihood to predict risk scores for survival data, accounting for censoring. This method excels in handling complex, non-linear relationships between covariates and survival outcomes, making it suitable for clinical applications like predicting patient survival in medical studies.\n",
    "\n",
    "This method combines the Cox proportional hazards model with gradient boosting, where the base learners (e.g., regression trees) are used to iteratively improve the estimate of the log partial likelihood.\n",
    "\n",
    "\n",
    "The hazard function for subject $ i $ is:\n",
    "\n",
    "$$\n",
    "h(t \\mid \\mathbf{x}_i) = h_0(t) \\exp(\\mathbf{x}_i^\\top \\boldsymbol{\\beta})\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $ h_0(t) $: baseline hazard (non-parametric)\n",
    "\n",
    "- $ \\exp(\\mathbf{x}_i^\\top \\boldsymbol{\\beta}) $: relative risk\n",
    "\n",
    "The partial log-likelihood (Cox, 1972) is:\n",
    "\n",
    "$$\n",
    "\\ell(\\boldsymbol{\\beta}) = \\sum_{i=1}^n \\delta_i \\left[ \\mathbf{x}_i^\\top \\boldsymbol{\\beta} - \\log \\left( \\sum_{j \\in \\mathcal{R}(t_i)} \\exp(\\mathbf{x}_j^\\top \\boldsymbol{\\beta}) \\right) \\right]\n",
    "$$\n",
    "\n",
    "where $ \\mathcal{R}(t_i) = \\{ j : t_j \\geq t_i \\} $ is the risk set at time $ t_i $.\n",
    "\n",
    "Gradient Boosting Framework\n",
    "\n",
    "Let $ f_m(\\mathbf{x}) $ be the additive model at iteration $ m $:\n",
    "\n",
    "$$\n",
    "f_m(\\mathbf{x}) = f_{m-1}(\\mathbf{x}) + \\nu \\cdot \\gamma_m h_m(\\mathbf{x})\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $ h_m(\\mathbf{x}) $: weak learner (e.g., regression tree)\n",
    "\n",
    "- $ \\gamma_m $: coefficient\n",
    "\n",
    "- $ \\nu $: learning rate\n",
    "\n",
    "In **CoxBoost**, the loss function is the negative log partial likelihood:\n",
    "\n",
    "$$\n",
    "L(y, f) = -\\ell(f) = -\\sum_{i=1}^n \\delta_i \\left[ f(\\mathbf{x}_i) - \\log \\left( \\sum_{j \\in \\mathcal{R}(t_i)} \\exp(f(\\mathbf{x}_j)) \\right) \\right]\n",
    "$$\n",
    "\n",
    "Gradients (pseudo-residuals) at step $ m $ are computed as:\n",
    "\n",
    "$$\n",
    "r_{im} = -\\left[ \\frac{\\partial L}{\\partial f(\\mathbf{x}_i)} \\right]_{f = f_{m-1}} = \\delta_i - \\sum_{j \\in \\mathcal{R}(t_i)} \\frac{ \\exp(f_{m-1}(\\mathbf{x}_j)) }{ \\sum_{k \\in \\mathcal{R}(t_i)} \\exp(f_{m-1}(\\mathbf{x}_k)) }\n",
    "$$\n",
    "\n",
    "This is the difference between the observed event indicator and the model-based risk weight (similar to a \"score\" residual).\n",
    "\n",
    "Then:\n",
    "\n",
    "- Fit a regression tree $ h_m(\\mathbf{x}) $ to the pseudo-residuals $ r_{im} $\n",
    "\n",
    "- Compute optimal $ \\gamma_m $ by line search in the original Cox likelihood\n",
    "\n",
    "- Update: $ f_m(\\mathbf{x}) = f_{m-1}(\\mathbf{x}) + \\nu \\gamma_m h_m(\\mathbf{x}) $\n",
    "\n",
    "> **Note**: The baseline hazard $ h_0(t) $ remains unspecified; boosting estimates the log-risk $ f(\\mathbf{x}) = \\log h(t|\\mathbf{x}) - \\log h_0(t) $\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ca8458",
   "metadata": {},
   "source": [
    "### Gradient Boosting Survival Trees (GBST)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "645aab89",
   "metadata": {},
   "source": [
    "\n",
    "Gradient Boosting Survival Tree (GBST) is a specialized gradient boosting approach for survival analysis that uses survival trees as base learners, designed to handle time-to-event data with censoring. These trees split nodes based on survival-specific criteria, such as log-rank tests, to maximize differences in survival outcomes. GBST iteratively combines these trees to optimize a survival-related loss function, producing survival probabilities or hazard functions. Its strength lies in its interpretability and ability to model complex survival patterns without strict assumptions like proportional hazards, making it ideal for medical research applications like predicting patient survival times.\n",
    "\n",
    "GBST is a **generic framework** for gradient boosting in survival analysis. It can be instantiated with different loss functions. The **CoxBoost** above is a special case of GBST.\n",
    "\n",
    "\n",
    "Use a survival-specific loss $ \\rho(t_i, \\delta_i, f(\\mathbf{x}_i)) $, e.g., Cox loss or others.\n",
    "\n",
    "General boosting steps:\n",
    "\n",
    "1. Initialize $ f_0(\\mathbf{x}) = \\arg\\min_{c} \\sum_{i=1}^n \\rho(t_i, \\delta_i, c) $\n",
    "\n",
    "2. For $ m = 1 $ to $ M $:\n",
    "   - Compute pseudo-residuals:\n",
    "   \n",
    "$$ r_{im} = -\\left[ \\frac{\\partial \\rho(t_i, \\delta_i, f_{m-1}(\\mathbf{x}_i))}{\\partial f_{m-1}(\\mathbf{x}_i)} \\right]   $$\n",
    "   - Fit a regression tree $ h_m(\\mathbf{x}) $ to $ r_{im} $\n",
    "   \n",
    "   - Compute $ \\gamma_m = \\arg\\min_\\gamma \\sum_{i=1}^n \\rho(t_i, \\delta_i, f_{m-1}(\\mathbf{x}_i) + \\gamma h_m(\\mathbf{x}_i)) $\n",
    "   \n",
    "   - Update: $ f_m(\\mathbf{x}) = f_{m-1}(\\mathbf{x}) + \\nu \\gamma_m h_m(\\mathbf{x}) $\n",
    "\n",
    "So **CoxBoost is a GBST with Cox partial likelihood loss**.\n",
    "\n",
    "Other losses in GBST may include:\n",
    "\n",
    "- Weighted log-likelihood\n",
    "- Rank-based losses (e.g., concordance)\n",
    "- Brier score for survival (integrated)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d720ca",
   "metadata": {},
   "source": [
    "### Accelerated Failure Time (AFT) Gradient Boosting (AFTBoost)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25eae9ab",
   "metadata": {},
   "source": [
    "\n",
    "Accelerated Failure Time (AFT) Gradient Boosting is a machine learning method for survival analysis that models the log-survival time as a function of covariates, assuming that predictors accelerate or decelerate the time to an event. It uses gradient boosting to combine decision trees, optimizing a likelihood-based loss function (e.g., Weibull or log-normal) that accounts for censored data. Unlike hazard-based models, AFT directly predicts survival times, making it intuitive for applications like predicting time to machine failure or patient recovery.\n",
    "\n",
    "The AFT model assumes:\n",
    "\n",
    "$$\n",
    "\\log T_i = \\mathbf{x}_i^\\top \\boldsymbol{\\beta} + \\sigma \\epsilon_i\n",
    "$$\n",
    "\n",
    "where $ \\epsilon_i $ is an error term (e.g., normal, extreme value, logistic), and $ \\sigma $ is a scale parameter.\n",
    "\n",
    "This implies that covariates **accelerate or decelerate time**:\n",
    "\n",
    "$$\n",
    "T_i = \\exp(\\mathbf{x}_i^\\top \\boldsymbol{\\beta}) \\cdot V_i\n",
    "$$\n",
    "\n",
    "where $ V_i = \\exp(\\sigma \\epsilon_i) $ is the baseline survival time.\n",
    "\n",
    "`Likelihood with Censoring`:\n",
    "\n",
    "Let $ y_i = \\log t_i $. The observed log-time and event indicator define the likelihood contribution:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_i = \\left[ \\frac{1}{\\sigma} f_\\epsilon\\left( \\frac{y_i - \\mathbf{x}_i^\\top \\boldsymbol{\\beta}}{\\sigma} \\right) \\right]^{\\delta_i} \\cdot \\left[ 1 - F_\\epsilon\\left( \\frac{y_i - \\mathbf{x}_i^\\top \\boldsymbol{\\beta}}{\\sigma} \\right) \\right]^{1 - \\delta_i}\n",
    "$$\n",
    "\n",
    "where $ f_\\epsilon $ and $ F_\\epsilon $ are the PDF and CDF of the error distribution.\n",
    "\n",
    "Common choices:\n",
    "- Normal: $ \\epsilon \\sim \\mathcal{N}(0,1) $ → log-normal AFT\n",
    "\n",
    "- Extreme value: $ \\epsilon \\sim \\text{Gumbel} $ → Weibull AFT\n",
    "\n",
    "`Gradient Boosting for AFT`:\n",
    "\n",
    "Let $ f(\\mathbf{x}) $ model $ \\mathbf{x}^\\top \\boldsymbol{\\beta} $, and optionally $ s(\\mathbf{x}) $ model $ \\log \\sigma $.\n",
    "\n",
    "**Case 1: Fixed $ \\sigma $**\n",
    "\n",
    "Use log-likelihood as loss:\n",
    "\n",
    "$$\n",
    "L(f) = -\\sum_{i=1}^n \\left[ \\delta_i \\log f_\\epsilon\\left( \\frac{y_i - f(\\mathbf{x}_i)}{\\sigma} \\right) + (1 - \\delta_i) \\log \\left(1 - F_\\epsilon\\left( \\frac{y_i - f(\\mathbf{x}_i)}{\\sigma} \\right)\\right) \\right]\n",
    "$$\n",
    "\n",
    "Pseudo-residuals:\n",
    "\n",
    "$$\n",
    "r_{im} = -\\left[ \\frac{\\partial L}{\\partial f(\\mathbf{x}_i)} \\right]_{f = f_{m-1}} = \\delta_i \\cdot \\frac{ f_\\epsilon'\\left( z_i^{(m-1)} \\right) }{ f_\\epsilon\\left( z_i^{(m-1)} \\right) } + (1 - \\delta_i) \\cdot \\frac{ f_\\epsilon\\left( z_i^{(m-1)} \\right) }{ 1 - F_\\epsilon\\left( z_i^{(m-1)} \\right) }\n",
    "$$\n",
    "\n",
    "where $ z_i^{(m-1)} = \\frac{y_i - f_{m-1}(\\mathbf{x}_i)}{\\sigma} $\n",
    "\n",
    "Then:\n",
    "- Fit tree $ h_m(\\mathbf{x}) $ to $ r_{im} $\n",
    "\n",
    "- Update $ f_m(\\mathbf{x}) = f_{m-1}(\\mathbf{x}) + \\nu \\gamma_m h_m(\\mathbf{x}) $\n",
    "\n",
    "**Case 2: Learn $ \\sigma $ jointly**\n",
    "\n",
    "Use two models:\n",
    "\n",
    "- $ f(\\mathbf{x}) $: location (mean)\n",
    "\n",
    "- $ s(\\mathbf{x}) $: log-scale\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f66d5b49",
   "metadata": {},
   "source": [
    "### Comparison: Similarities and Differences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "945580a5",
   "metadata": {},
   "source": [
    "\n",
    "| Feature | **CoxBoost** | **GBST** | **AFTBoost** |\n",
    "|--------|------------|----------|------------|\n",
    "| **Model Type** | Semi-parametric (PH) | Framework (can be PH or other) | Parametric (AFT) |\n",
    "| **Assumption** | Proportional hazards | Depends on loss | Accelerated time |\n",
    "| **Loss Function** | Negative partial Cox log-likelihood | Any survival loss (Cox, rank, etc.) | Full log-likelihood (with censoring) |\n",
    "| **Output** | Log-hazard ratio $ f(\\mathbf{x}) $ | Flexible (depends on loss) | $ \\log T_i $ prediction |\n",
    "| **Baseline** | $ h_0(t) $ unspecified | Depends on loss | Parametric baseline distribution |\n",
    "| **Gradient Form** | $ r_{im} = \\delta_i - \\sum_{j \\in \\mathcal{R}(t_i)} w_j $ | $ r_{im} = -\\partial \\rho / \\partial f $ | $ r_{im} = \\text{likelihood score} $ |\n",
    "| **Interpretability** | Hazard ratios | Depends on loss | Time acceleration factor $ \\exp(\\beta_j) $ |\n",
    "| **Tree Fitting** | On pseudo-residuals | On pseudo-residuals | On pseudo-residuals |\n",
    "| **Prediction** | Risk score, survival curves (via Breslow) | Depends on loss | Predict $ \\log T_i $, simulate survival |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d4e398",
   "metadata": {},
   "source": [
    "## Gradient Boosted Survival Models with R"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f91911b",
   "metadata": {},
   "source": [
    "\n",
    "This section provides a comprehensive guide to implementing the CoxBoost, GBST and AFTBoost using R, specifically utilizing the {CoxBoost}, {gbm} and {xgboost} packages. The steps include loading necessary libraries, preparing the dataset, fitting the model, validating it, and visualizing results.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e1aef1b",
   "metadata": {},
   "source": [
    "### Load and Check Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b127e78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install rpy2\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "## Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b4753e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "# install Coxboost\n",
    "if (!require(\"devtools\", quietly = TRUE))\n",
    "    install.packages(\"devtools\")\n",
    "devtools::install_github(\"binderh/CoxBoost\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d77a47",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb88d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "# Install pacman\n",
    "if (!requireNamespace(\"pacman\", quietly = TRUE)) {\n",
    "  install.packages(\"pacman\")\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee968c31",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92cd3355",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "packages <- c('tidyverse', \n",
    "              'plyr',\n",
    "              'survival',\n",
    "              'risksetROC',\n",
    "              'survivalROC',\n",
    "              'riskRegression',\n",
    "              'pracma',\n",
    "              'survminer',\n",
    "              'survcomp',\n",
    "              'CoxBoost',\n",
    "              'gbm',\n",
    "              'xgboost',\n",
    "              'pec'\n",
    "\n",
    "         )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f83d80b",
   "metadata": {},
   "source": [
    "### Install Missing Packages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f74f669d",
   "metadata": {},
   "source": [
    "\n",
    "```      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a461b36",
   "metadata": {},
   "source": [
    "# Install missing packages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed0682d3",
   "metadata": {},
   "source": [
    "new_packages <- packages[!(packages %in% installed.packages()[,\"Package\"])]\n",
    "if(length(new_packages)) install.packages(new_packages)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb4c6a6",
   "metadata": {},
   "source": [
    "### Verify Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e0891d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "# Verify installation\n",
    "cat(\"Installed packages:\\n\")\n",
    "print(sapply(packages, requireNamespace, quietly = TRUE))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78691a71",
   "metadata": {},
   "source": [
    "### Load Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe4c606",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "# Load packages with suppressed messages\n",
    "invisible(lapply(packages, function(pkg) {\n",
    "  suppressPackageStartupMessages(library(pkg, character.only = TRUE))\n",
    "}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b93b77",
   "metadata": {},
   "source": [
    "\n",
    "Or you can use `{pacman}` to load all packages at once, which is a convenient way to manage package dependencies in R. The `p_load` function loads the specified packages, installing them if they are not already installed, and suppresses messages during loading.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "671a548d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "# Load packages with suppressed messages\n",
    "pacman::p_load('tidyverse', \n",
    "              'plyr',\n",
    "              'survival',\n",
    "              'risksetROC',\n",
    "              'survivalROC',\n",
    "              'riskRegression',\n",
    "              'pracma',\n",
    "              'survminer',\n",
    "              'survcomp',\n",
    "              'CoxBoost',\n",
    "              'gbm',\n",
    "              'xgboost',\n",
    "              'pec'\n",
    "         )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e16152",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f2c16c",
   "metadata": {},
   "source": [
    "### Check Loaded Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93537743",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "# Check loaded packages\n",
    "cat(\"Successfully loaded packages:\\n\")\n",
    "print(search()[grepl(\"package:\", search())])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a2209e",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c116bcdd",
   "metadata": {},
   "source": [
    "## Cox Proportional Hazards-Based Gradient Boosting (CoxBoost)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "831c46d0",
   "metadata": {},
   "source": [
    "\n",
    "The Cox Proportional Hazards-Based Gradient Boosting Model combines the Cox proportional hazards framework with gradient boosting to model survival data, capturing non-linear relationships and interactions while handling censored observations. In R, packages like {CoxBoost}, {xgboost}, and {gbm} enable this approach. Below is a step-by-step implementation of CoxBoost model, including data preparation, model fitting, validation, and visualization.  It is a semi-parametric regression model widely used for such tasks. The classical Cox model assumes:\n",
    "\n",
    "- Linear effects of covariates\n",
    "\n",
    "- Proportional hazards\n",
    "\n",
    "- No interactions unless explicitly added\n",
    "\n",
    "To overcome these limitations, **gradient boosting** can be adapted to optimize the **partial likelihood** of the Cox model. This results in a **boosted Cox model** that learns complex, non-linear patterns in the data by sequentially fitting weak learners (typically decision trees) to improve survival risk predictions. This approach is implemented in libraries like `CoxBoost`,  `gbm` and `xgboost` using a custom objective function based on the **Cox partial log-likelihood**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9353c42f",
   "metadata": {},
   "source": [
    "### Data and Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c72ae8c",
   "metadata": {},
   "source": [
    "\n",
    "First, ensure you have the required packages installed and load the `lung` dataset from the `survival` package. The `lung` dataset contains survival data for 228 patients with advanced lung cancer, including variables like `time` (survival time in days), `status` (censoring indicator: 1 = censored, 2 = dead), and covariates like `age`, `sex`, `ph.ecog`, etc.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e875bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "# Set seed for reproducibility\n",
    "set.seed(123)\n",
    "\n",
    "# Prepare lung dataset\n",
    "data(lung)\n",
    "# Remove missing values\n",
    "lung_complete <- lung %>% na.omit()\n",
    "\n",
    "# Split into 70% training and 30% testing\n",
    "n <- nrow(lung_complete)\n",
    "train_idx <- sample(1:n, size = round(0.7 * n))\n",
    "train_data <- lung_complete[train_idx, ]\n",
    "test_data <- lung_complete[-train_idx, ]\n",
    "\n",
    "# Training data\n",
    "time_train <- train_data$time\n",
    "status_train <- train_data$status\n",
    "X_train <- train_data[, !(names(train_data) %in% c(\"time\", \"status\"))]\n",
    "X_train_matrix <- as.matrix(X_train)\n",
    "\n",
    "# Test data\n",
    "time_test <- test_data$time\n",
    "status_test <- test_data$status\n",
    "X_test <- test_data[, !(names(test_data) %in% c(\"time\", \"status\"))]\n",
    "X_test_matrix <- as.matrix(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc5d576",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8942eaf4",
   "metadata": {},
   "source": [
    "### Fit Initial CoxBoost Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44520d50",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "First, we fit an initial CoxBoost model using the training data. This model will be used to estimate the baseline cumulative hazard and compute survival curves.  `CoxBoost()` function is used to fit the model, specifying the time, status, and predictor matrix. The `stepno` parameter controls the number of boosting iterations, and `penalty` is set to 100 for regularization. The `criterion` is set to \"pscore\" for partial score optimization, and `standardize` is set to TRUE to standardize the predictors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a58a35a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "# Fit initial CoxBoost model\n",
    "initial_model <- CoxBoost(\n",
    "  time = time_train,\n",
    "  status = status_train,\n",
    "  x = X_train_matrix,\n",
    "  stepno = 100,\n",
    "  penalty = 100,\n",
    "  criterion = \"pscore\",\n",
    "  standardize = TRUE\n",
    ")\n",
    "\n",
    "summary(initial_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f489716d",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa311b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "# variables contained in the fitted object\n",
    "names(initial_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa61fd5",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f268383d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "# coefficient estimates\n",
    "dim(initial_model$coefficients)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c0c942e",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e35b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "# linear predictor estimates\n",
    "dim(initial_model$linear.predictor) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c626bc8c",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f747f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "# cumulative baseline hazard estimates\n",
    "dim(initial_model$Lambda)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ce8f31",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec0dacd",
   "metadata": {},
   "source": [
    "### Prediction and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb35f6a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "# Initial model validation on test data\n",
    "# Get risk scores\n",
    "initial_pred <- predict(initial_model, newdata = X_test_matrix, type = \"risk\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d963570",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "# Handle case where initial_pred is a matrix\n",
    "if (is.matrix(initial_pred)) {\n",
    "  cat(\"initial_pred is a matrix, selecting final column\\n\")\n",
    "  initial_pred_vec <- as.vector(initial_pred[, ncol(initial_pred)])\n",
    "} else {\n",
    "  initial_pred_vec <- as.vector(initial_pred)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "881a47c5",
   "metadata": {},
   "source": [
    "### C-Index of Initial Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd4b7b7f",
   "metadata": {},
   "source": [
    "\n",
    "C-index is a measure of the model's discriminatory ability, indicating how well the model can distinguish between individuals who experience the event and those who do not. A C-index of 0.5 indicates no discrimination (random prediction), while a C-index of 1.0 indicates perfect discrimination. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c93c3a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "# Custom C-index function as a fallback\n",
    "custom_cindex <- function(pred, time, status) {\n",
    "  concordant <- 0\n",
    "  total_pairs <- 0\n",
    "  n <- length(time)\n",
    "  for (i in 1:(n-1)) {\n",
    "    for (j in (i+1):n) {\n",
    "      if (status[i] == 1 && status[j] == 1) {\n",
    "        if (time[i] < time[j] && pred[i] > pred[j]) concordant <- concordant + 1\n",
    "        if (time[j] < time[i] && pred[j] > pred[i]) concordant <- concordant + 1\n",
    "        total_pairs <- total_pairs + 1\n",
    "      } else if (status[i] == 1 && status[j] == 0) {\n",
    "        if (time[i] < time[j] && pred[i] > pred[j]) concordant <- concordant + 1\n",
    "        total_pairs <- total_pairs + 1\n",
    "      } else if (status[j] == 1 && status[i] == 0) {\n",
    "        if (time[j] < time[i] && pred[j] > pred[i]) concordant <- concordant + 1\n",
    "        total_pairs <- total_pairs + 1\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "  if (total_pairs == 0) return(NA)\n",
    "  return(concordant / total_pairs)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec8bb09c",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bfdb0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "# Try C-index with survcomp on test data\n",
    "initial_cindex <- NA\n",
    "tryCatch({\n",
    "  initial_cindex <- concordance.index(\n",
    "    x = initial_pred_vec,\n",
    "    surv.time = time_test,\n",
    "    surv.event = status_test,\n",
    "    method = \"noether\"\n",
    "  )$c.index\n",
    "  cat(\"survcomp::concordance.index succeeded for initial model\\n\")\n",
    "}, error = function(e) {\n",
    "  cat(\"survcomp::concordance.index failed for initial model:\", e$message, \"\\n\")\n",
    "  # Fallback 1: Try survivalROC\n",
    "  tryCatch({\n",
    "    roc <- survivalROC(\n",
    "      Stime = time_test,\n",
    "      status = status_test,\n",
    "      marker = initial_pred_vec,\n",
    "      predict.time = median(time_test),\n",
    "      method = \"NNE\"\n",
    "    )\n",
    "    initial_cindex <<- roc$AUC\n",
    "    cat(\"Fallback C-index (AUC at median time) for initial model:\", initial_cindex, \"\\n\")\n",
    "  }, error = function(e2) {\n",
    "    cat(\"survivalROC failed for initial model:\", e2$message, \"\\n\")\n",
    "    # Fallback 2: Custom C-index\n",
    "    initial_cindex <<- custom_cindex(initial_pred_vec, time_test, status_test)\n",
    "    cat(\"Custom C-index for initial model:\", initial_cindex, \"\\n\")\n",
    "  })\n",
    "})\n",
    "\n",
    "print(paste(\"Initial model C-index:\", round(initial_cindex, 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c2d95b",
   "metadata": {},
   "source": [
    "### Integrated Brier Score (IBS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78412e37",
   "metadata": {},
   "source": [
    "\n",
    "The Integrated Brier Score (IBS) is a metric used to evaluate the predictive performance of survival models, such as the CoxBoost model in your script. It measures the mean squared error of predicted survival probabilities over a specified time range, accounting for censoring\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3760e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "# Manual IBS calculation using CIF on test data\n",
    "eval_times <- seq(0, max(time_test), length.out = 100)\n",
    "initial_cif <- predict(initial_model, newdata = X_test_matrix, type = \"CIF\", times = eval_times)\n",
    "initial_brier <- rep(NA, length(eval_times))\n",
    "for (i in 1:length(eval_times)) {\n",
    "  km_fit <- survfit(Surv(time_test, 1 - status_test) ~ 1, data = test_data)\n",
    "  G_t <- summary(km_fit, times = eval_times[i])$surv\n",
    "  if (length(G_t) == 0) G_t <- 1\n",
    "  initial_brier[i] <- mean((as.numeric(time_test > eval_times[i]) - (1 - initial_cif[, i]))^2 / G_t, na.rm = TRUE)\n",
    "}\n",
    "initial_ibs <- mean(initial_brier, na.rm = TRUE)\n",
    "print(paste(\"Initial model IBS:\", round(initial_ibs, 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f69a47e7",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ebeaf06",
   "metadata": {},
   "source": [
    "### Kaplan-Meier baseline IBS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ad3713",
   "metadata": {},
   "source": [
    "\n",
    "The Kaplan-Meier baseline Integrated Brier Score (IBS) is calculated using the Kaplan-Meier survival estimates as a baseline for comparison. This provides a reference point to evaluate the performance of the CoxBoost model against a non-parametric survival estimate.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f99f2f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "# Kaplan-Meier baseline IBS\n",
    "km_fit <- survfit(Surv(time_test, status_test) ~ 1, data = test_data)\n",
    "km_surv <- summary(km_fit, times = eval_times)$surv\n",
    "km_brier <- rep(NA, length(eval_times))\n",
    "for (i in 1:length(eval_times)) {\n",
    "  G_t <- summary(survfit(Surv(time_test, 1 - status_test) ~ 1), times = eval_times[i])$surv\n",
    "  if (length(G_t) == 0) G_t <- 1\n",
    "  km_brier[i] <- mean((as.numeric(time_test > eval_times[i]) - km_surv[i])^2 / G_t, na.rm = TRUE)\n",
    "}\n",
    "km_ibs <- mean(km_brier, na.rm = TRUE)\n",
    "print(paste(\"Kaplan-Meier baseline IBS:\", round(km_ibs, 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a32c96d",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d9771e",
   "metadata": {},
   "source": [
    "### Crosss-Validation for Penalty Parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b19a33b",
   "metadata": {},
   "source": [
    "\n",
    "Cross-validation is used to select the optimal penalty parameter for the CoxBoost model. The `optimCoxBoostPenalty()` function performs cross-validation, returning the mean log partial likelihood for different penalty values. The optimal penalty is chosen based on the minimum mean log partial likelihood.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9980bfac",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "cv_penalty <-optimCoxBoostPenalty(time = time_train,\n",
    "                                  status = status_train,\n",
    "                                  x = X_train_matrix,\n",
    "                                  trace=TRUE,\n",
    "                                  start.penalty=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49101e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    " # the optimal penalty\n",
    "optimal_penalty<-cv_penalty$penalty\n",
    "print(paste(\"Optimal Penalty:\", optimal_penalty, \"\\n\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b897b587",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ecc3eda",
   "metadata": {},
   "source": [
    "### Cross-Validation for Boosting Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5603ceff",
   "metadata": {},
   "source": [
    "\n",
    "Cross-validation is also performed to determine the optimal number of boosting steps. The `cv.CoxBoost()` function is used to perform cross-validation, returning the mean log partial likelihood for different step numbers. The optimal number of steps is chosen based on the minimum mean log partial likelihood.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7973b4fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "# Cross-validation for boosting steps\n",
    "cv_steps <- tryCatch({\n",
    "  cv.CoxBoost(\n",
    "    time = time_train,\n",
    "    status = status_train,\n",
    "    x = X_train_matrix,\n",
    "    maxstepno = 100,\n",
    "    penalty = 100,\n",
    "    K = 10,\n",
    "    type = \"verweij\"\n",
    "  )\n",
    "}, error = function(e) {\n",
    "  cat(\"cv.CoxBoost for steps failed:\", e$message, \"\\n\")\n",
    "  cat(\"Using default stepno = 100\\n\")\n",
    "  return(NULL)\n",
    "})\n",
    "if (!is.null(cv_steps) && !any(is.na(cv_steps$mean.logplik))) {\n",
    "  optimal_steps <- which.min(cv_steps$mean.logplik)\n",
    "} else {\n",
    "  optimal_steps <- 100\n",
    "}\n",
    "cat(\"Optimal Steps:\", optimal_steps, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03dab621",
   "metadata": {},
   "source": [
    "### Fit Final Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e904dd3c",
   "metadata": {},
   "source": [
    "\n",
    "Now that we have the optimal penalty and number of steps, we can fit the final CoxBoost model using these parameters. The `CoxBoost()` function is used again, this time with the optimal parameters obtained from cross-validation. The model is fitted on the training data, and a summary of the model is printed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7359fc4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "# Fit final model\n",
    "final_model <- tryCatch({\n",
    "  CoxBoost(\n",
    "    time = time_train,\n",
    "    status = status_train,\n",
    "    x = X_train_matrix,\n",
    "    stepno = optimal_steps,\n",
    "    penalty = optimal_penalty,\n",
    "    standardize = FALSE\n",
    "  )\n",
    "}, error = function(e) {\n",
    "  cat(\"CoxBoost final model failed:\", e$message, \"\\n\")\n",
    "  cat(\"Using default parameters: stepno = 100, penalty = 100\\n\")\n",
    "  CoxBoost(\n",
    "    time = time_train,\n",
    "    status = status_train,\n",
    "    x = X_train_matrix,\n",
    "    stepno = 100,\n",
    "    penalty = 100,\n",
    "    standardize = FALSE\n",
    "  )\n",
    "})\n",
    "summary(final_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b3d9146",
   "metadata": {},
   "source": [
    "### Final model validation on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e5f72f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "# prediction\n",
    "final_pred <- predict(final_model, newdata = X_test_matrix, type = \"risk\")\n",
    "if (is.matrix(final_pred)) {\n",
    "  final_pred_vec <- as.vector(final_pred[, ncol(final_pred)])\n",
    "} else {\n",
    "  final_pred_vec <- as.vector(final_pred)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ab178e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "# Try C-index for final model\n",
    "final_cindex <- NA\n",
    "tryCatch({\n",
    "  final_cindex <- concordance.index(\n",
    "    x = final_pred_vec,\n",
    "    surv.time = time_test,\n",
    "    surv.event = status_test,\n",
    "    method = \"noether\"\n",
    "  )$c.index\n",
    "  cat(\"survcomp::concordance.index succeeded for final model\\n\")\n",
    "}, error = function(e) {\n",
    "  cat(\"survcomp::concordance.index failed for final model:\", e$message, \"\\n\")\n",
    "  tryCatch({\n",
    "    roc <- survivalROC(\n",
    "      Stime = time_test,\n",
    "      status = status_test,\n",
    "      marker = final_pred_vec,\n",
    "      predict.time = median(time_test),\n",
    "      method = \"NNE\"\n",
    "    )\n",
    "    final_cindex <<- roc$AUC\n",
    "    cat(\"Fallback C-index (AUC at median time) for final model:\", final_cindex, \"\\n\")\n",
    "  }, error = function(e2) {\n",
    "    cat(\"survivalROC failed for final model:\", e2$message, \"\\n\")\n",
    "    final_cindex <<- custom_cindex(final_pred_vec, time_test, status_test)\n",
    "    cat(\"Custom C-index for final model:\", final_cindex, \"\\n\")\n",
    "  })\n",
    "})\n",
    "print(paste(\"Final model C-index:\", round(final_cindex, 3)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc542cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "# Manual IBS for final model\n",
    "final_cif <- predict(final_model, newdata = X_test_matrix, type = \"CIF\", times = eval_times)\n",
    "final_brier <- rep(NA, length(eval_times))\n",
    "for (i in 1:length(eval_times)) {\n",
    "  km_fit <- survfit(Surv(time_test, 1 - status_test) ~ 1, data = test_data)\n",
    "  G_t <- summary(km_fit, times = eval_times[i])$surv\n",
    "  if (length(G_t) == 0) G_t <- 1\n",
    "  final_brier[i] <- mean((as.numeric(time_test > eval_times[i]) - (1 - final_cif[, i]))^2 / G_t, na.rm = TRUE)\n",
    "}\n",
    "final_ibs <- mean(final_brier, na.rm = TRUE)\n",
    "print(paste(\"Final model IBS:\", round(final_ibs, 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a2d7665",
   "metadata": {},
   "source": [
    "### Survival Curves "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa7d573d",
   "metadata": {},
   "source": [
    "\n",
    "Finally, we can visualize the survival curves based on the predicted risk scores from the final CoxBoost model. The survival probabilities are computed from the cumulative incidence function (CIF) estimates, and a plot is generated to show the average survival curve across all test samples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a864ba18",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "\n",
    "surv_prob_final <- 1 - final_cif\n",
    "surv_data <- data.frame(\n",
    "  time = eval_times,\n",
    "  surv = colMeans(surv_prob_final)\n",
    ")\n",
    "ggplot(surv_data, aes(x = time, y = surv)) +\n",
    "  geom_line(color = \"blue\") +\n",
    "  labs(title = \"Average Survival Curve (CoxBoost) on Test Data\", x = \"Time\", y = \"Survival Probability\") +\n",
    "  theme_minimal()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e60c00c",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f223b597",
   "metadata": {},
   "source": [
    "### Variable Importance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e30e4cc",
   "metadata": {},
   "source": [
    "\n",
    "Variable importance can be assessed by examining the absolute values of the coefficients from the final CoxBoost model. This provides insights into which predictors have the most significant impact on survival risk.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60a8dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "#\n",
    "importance <- coef(final_model)\n",
    "imp_data <- data.frame(\n",
    "  Variable = colnames(X_train_matrix),\n",
    "  Importance = abs(importance)\n",
    ") %>% filter(Importance != 0)\n",
    "\n",
    "ggplot(imp_data, aes(x = reorder(Variable, Importance), y = Importance)) +\n",
    "  geom_bar(stat = \"identity\", fill = \"steelblue\") +\n",
    "  coord_flip() +\n",
    "  labs(title = \"Feature Importance (CoxBoost)\", x = \"Variable\", y = \"Absolute Coefficient\") +\n",
    "  theme_minimal()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9258d5a3",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e6bd54",
   "metadata": {},
   "source": [
    "## Gradient Boosting Survival Tree (GBST)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "470df770",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "GBST applies gradient boosting directly to survival data using tree-based models, but it typically uses `rank-based` or `log-likelihood` loss functions tailored to survival, such as the log partial likelihood (Cox-type) or concordance-based loss. It extends the gradient boosting framework—popularized by algorithms like Gradient Boosting Machines (GBM) and XGBoost—to handle survival outcomes by optimizing survival-specific loss functions. Unlike traditional survival models such as the **Cox proportional hazards model**, GBST does not assume proportional hazards. GBST can capture complex, non-linear relationships and interactions in the data. GBST may use different loss functions (e.g., ranking), while CoxBoost specifically targets the Cox partial likelihood.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "990fa448",
   "metadata": {},
   "source": [
    "### GBST with gbm "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5700364",
   "metadata": {},
   "source": [
    "\n",
    "GBST operates similarly to CoxBoost by integrating the Cox proportional hazards model with gradient boosting techniques for analyzing survival data. This approach allows for flexible modeling of survival times while accounting for censoring, making it suitable for complex survival datasets. The {gbm} package in R provides an implementation of this method, enabling users to fit models that optimize the Cox partial likelihood using gradient boosting.  Below is a step-by-step guide to fitting a GBST model using the {gbm} package, including data preparation, model fitting, validation, and visualization.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d2ce5fe",
   "metadata": {},
   "source": [
    "#### Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02fda782",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "set.seed(123)\n",
    "\n",
    "# Prepare lung dataset\n",
    "data(lung)\n",
    "# Remove missing values\n",
    "lung_complete <- lung %>% na.omit()\n",
    "\n",
    "# Split into 70% training and 30% testing\n",
    "n <- nrow(lung_complete)\n",
    "train_idx <- sample(1:n, size = round(0.7 * n))\n",
    "train_data <- lung_complete[train_idx, ]\n",
    "test_data <- lung_complete[-train_idx, ]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb15c41",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17bafa92",
   "metadata": {},
   "source": [
    "#### Fit GBST Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43691517",
   "metadata": {},
   "source": [
    "\n",
    "GBST is implemented using the `{gbm}` package, which allows for fitting gradient boosting models with various loss functions. For survival analysis, we can use the `distribution = \"coxph\"` option to specify that we want to fit a Cox proportional hazards model using gradient boosting.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b1f3a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "# Fit Gradient Boosting Survival Tree model\n",
    "set.seed(123)\n",
    "gbst_model <- gbm(\n",
    "  formula = Surv(time, status) ~ age + sex + ph.ecog + ph.karno + pat.karno + meal.cal + wt.loss,\n",
    "  data = train_data,\n",
    "  distribution = \"coxph\",           # Cox model\n",
    "  n.trees = 1000,                   # Number of boosting iterations\n",
    "  interaction.depth = 3,           # Tree depth\n",
    "  shrinkage = 0.01,                # Learning rate\n",
    "  bag.fraction = 0.5,              # Subsampling\n",
    "  cv.folds = 5,                    # 5-fold CV for optimal n.trees\n",
    "  n.minobsinnode = 10\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7050caff",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "048a5c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "print(gbst_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07bfeb6c",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e5930f",
   "metadata": {},
   "source": [
    "#### Cross-valiadtion "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1630b528",
   "metadata": {},
   "source": [
    "\n",
    "To determine the optimal number of trees, we can use cross-validation. The `{gbm}` package provides a built-in method to perform cross-validation and select the best number of trees based on the minimum cross-validated error.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917e08f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "# Cross-validation to find optimal number of trees\n",
    "best_iter <- gbm.perf(gbst_model, method = \"cv\")\n",
    "cat(\"Optimal number of trees:\", best_iter, \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20fddc70",
   "metadata": {},
   "source": [
    "#### Vraiable Importnace "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a16bc4",
   "metadata": {},
   "source": [
    "\n",
    "summary of the model provides information about the fitted model, including variable importance and the number of trees used.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd89496",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "# Variable importance\n",
    "summary(gbst_model, las = 2)  # las = 2 rotates axis labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d44e9f6",
   "metadata": {},
   "source": [
    "#### Predictioon and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a31e4372",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "# Predict risk scores for test data\n",
    "risk_scores <- predict(gbst_model, newdata = test_data, n.trees = best_iter, type = \"response\")\n",
    "print(paste(\"Cox GBM Risk Scores:\", round(risk_scores, 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "866e7a9c",
   "metadata": {},
   "source": [
    "#### Concordance index (C-index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b049fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "# Compute concordance index (C-index)\n",
    "cat(\"C-index:\", concordance(Surv(time, status) ~risk_scores , data = test_data)$concordance, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e85a38",
   "metadata": {},
   "source": [
    "#### Integrated Brier Score (IBS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fad541d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "# Calculate Integrated Brier Score (IBS)\n",
    "# Define evaluation times\n",
    "eval_times <- seq(0, max(test_data$time), length.out = 100)\n",
    "# Initialize Brier score vector\n",
    "brier_scores <- numeric(length(eval_times))\n",
    "# Loop through evaluation times\n",
    "for (i in seq_along(eval_times)) {\n",
    "  # Get survival probabilities at evaluation time\n",
    "  surv_probs <- predict(gbst_model, newdata = test_data, n.trees = best_iter, type = \"response\")\n",
    "  # Calculate Brier score\n",
    "  brier_scores[i] <- mean((as.numeric(test_data$time > eval_times[i]) - surv_probs)^2)\n",
    "}\n",
    "# Calculate Integrated Brier Score (IBS)\n",
    "ibs <- mean(brier_scores, na.rm = TRUE)\n",
    "cat(\"Integrated Brier Score (IBS):\", round(ibs, 3), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b91b66",
   "metadata": {},
   "source": [
    "#### Survival Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d4361e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "# Estimate baseline survival function using training data\n",
    "# Fit a Cox model to get baseline hazard (approximation for GBST)\n",
    "cox_model <- coxph(Surv(time, status) ~ 1, data = train_data)\n",
    "base_haz <- basehaz(cox_model, centered = FALSE)\n",
    "\n",
    "# Compute survival probabilities for test data\n",
    "surv_probs <- matrix(NA, nrow = nrow(test_data), ncol = length(base_haz$time))\n",
    "for (i in 1:nrow(test_data)) {\n",
    "  surv_probs[i, ] <- exp(-base_haz$hazard * exp(risk_scores[i]))\n",
    "}\n",
    "\n",
    "# Create a data frame for plotting\n",
    "surv_df <- data.frame(\n",
    "  Time = rep(base_haz$time, nrow(test_data)),\n",
    "  Survival = as.vector(t(surv_probs)),\n",
    "  Subject = rep(1:nrow(test_data), each = length(base_haz$time))\n",
    ")\n",
    "\n",
    "# Plot survival curves for a subset of subjects (e.g., first 5 for clarity)\n",
    "subset_subjects <- 1:min(5, nrow(test_data))\n",
    "surv_df_subset <- surv_df[surv_df$Subject %in% subset_subjects, ]\n",
    "\n",
    "ggplot(surv_df_subset, aes(x = Time, y = Survival, color = factor(Subject))) +\n",
    "  geom_line(linewidth = 1) +\n",
    "  labs(title = \"Survival Curves for Test Data (GBST GBM)\",\n",
    "       x = \"Time\", y = \"Survival Probability\",\n",
    "       color = \"Subject ID\") +\n",
    "  theme_minimal() +\n",
    "  scale_color_brewer(palette = \"Set1\") +\n",
    "  theme(legend.position = \"top\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dece715",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17caab33",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "# Optional: Plot average survival curve across all test subjects\n",
    "avg_surv <- colMeans(surv_probs)\n",
    "avg_surv_df <- data.frame(Time = base_haz$time, Survival = avg_surv)\n",
    "\n",
    "ggplot(avg_surv_df, aes(x = Time, y = Survival)) +\n",
    "  geom_line(linewidth = 1, color = \"blue\") +\n",
    "  labs(title = \"Average Survival Curve for Test Data (GBST-GBM)\",\n",
    "       x = \"Time\", y = \"Survival Probability\") +\n",
    "  theme_minimal()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8af0ca4e",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93eaff5c",
   "metadata": {},
   "source": [
    "### GBST with xgboost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "556214c2",
   "metadata": {},
   "source": [
    "\n",
    "The Gradient Boosting Survival Tree (GBST) model can also be implemented using the {xgboost} package in R. This approach allows for efficient handling of large datasets and provides flexibility in defining custom loss functions for survival analysis. Below is a step-by-step guide to fitting a GBST model using {xgboost}, including data preparation, model fitting, validation, and visualization.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff19195",
   "metadata": {},
   "source": [
    "#### Load and Prepare Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaacb253",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1edbba0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "# Set seed for reproducibility\n",
    "set.seed(123)\n",
    "\n",
    "# Load lung dataset\n",
    "data(lung, package = \"survival\")\n",
    "\n",
    "# Prepare data: remove missing values and select relevant features\n",
    "lung_clean <- lung %>%\n",
    "  na.omit() %>%\n",
    "  dplyr::select(time, status, age, sex, ph.ecog, ph.karno, pat.karno, meal.cal, wt.loss)\n",
    "\n",
    "# Convert status to 0/1 (xgboost expects 0 = censored, 1 = event)\n",
    "lung_clean$status <- lung_clean$status - 1\n",
    "\n",
    "# Split data into 70% training and 30% testing\n",
    "trainIndex <- caret::createDataPartition(lung_clean$status, p = 0.7, list = FALSE)\n",
    "train_data <- lung_clean[trainIndex, ]\n",
    "test_data <- lung_clean[-trainIndex, ]\n",
    "\n",
    "# Prepare data for xgboost\n",
    "features <- c(\"age\", \"sex\", \"ph.ecog\", \"ph.karno\", \"pat.karno\", \"meal.cal\", \"wt.loss\")\n",
    "X_train <- as.matrix(train_data[, features])\n",
    "y_train <- Surv(train_data$time, train_data$status)\n",
    "X_test <- as.matrix(test_data[, features])\n",
    "y_test <- Surv(test_data$time, test_data$status)\n",
    "\n",
    "# Create xgboost DMatrix\n",
    "dtrain <- xgb.DMatrix(data = X_train, label = train_data$time)\n",
    "#setinfo(dtrain, \"event\", train_data$status)\n",
    "dtest <- xgb.DMatrix(data = X_test, label = test_data$time)\n",
    "#setinfo(dtest, \"event\", test_data$status)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a686f017",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "713a7252",
   "metadata": {},
   "source": [
    "#### Train GBST Model with xgboost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37356f7a",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e053b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "# Set xgboost parameters for survival: Cox objective\n",
    "params <- list(\n",
    "  objective = \"survival:cox\",\n",
    "  eval_metric = \"cox-nloglik\",\n",
    "  max_depth = 4,\n",
    "  eta = 0.1,\n",
    "  nrounds = 100\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "xgb_model <- xgb.train(\n",
    "  params = params,\n",
    "  data = dtrain,\n",
    "  nrounds = params$nrounds,\n",
    "  verbose = 0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd5de7d1",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513b1feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "# Print model summary\n",
    "print(xgb_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "288c5ba9",
   "metadata": {},
   "source": [
    "#### Variable Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d493a6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "# Plot variable importance\n",
    "importance <- xgb.importance(model = xgb_model)\n",
    "xgb.plot.importance(importance, \n",
    "                    main = \"Variable Importance (GBST with xgboost)\",\n",
    "                    xlab = \"Relative Importance\", \n",
    "                    ylab = \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80098f80",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae007b42",
   "metadata": {},
   "source": [
    "#### Prediction and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f284a4d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "# Predict risk scores (for Cox model)\n",
    "pred_risk <- predict(xgb_model, dtest)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18af1248",
   "metadata": {},
   "source": [
    "#### Concordance index (C-Index) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b09ab9f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "# Calculate C-index (concordance index)\n",
    "cindex <- concordance(y_test ~ pred_risk)$concordance\n",
    "print(paste(\"C-index:\", round(cindex, 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7876c1b5",
   "metadata": {},
   "source": [
    "#### Integrated Brier Score (IBS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9770b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "# Define a prediction function for riskRegression\n",
    "predictRisk.xgb.Booster <- function(object, newdata, times, ...) {\n",
    "  X_new <- as.matrix(newdata[, features])\n",
    "  dnew <- xgb.DMatrix(data = X_new)\n",
    "  risk_scores <- predict(object, dnew)\n",
    "  # Convert risk scores to survival probabilities\n",
    "  surv_prob <- matrix(NA, nrow = nrow(newdata), ncol = length(times))\n",
    "  for (i in 1:nrow(newdata)) {\n",
    "    surv_prob[i, ] <- exp(-risk_scores[i] * times / mean(newdata$time))\n",
    "  }\n",
    "  return(surv_prob)\n",
    "}\n",
    "\n",
    "# Attempt to compute Brier Score and Integrated Brier Score with riskRegression\n",
    "brier_model <- tryCatch(\n",
    "  {\n",
    "    Score(\n",
    "      list(\"XGBoost\" = xgb_model),\n",
    "      formula = Surv(time, status) ~ 1,\n",
    "      data = test_data,\n",
    "      metrics = c(\"brier\"),\n",
    "      times = seq(0, max(test_data$time), length.out = 100),\n",
    "      summary = \"IBS\"\n",
    "    )\n",
    "  },\n",
    "  error = function(e) {\n",
    "    message(\"Error in riskRegression::Score: \", e$message)\n",
    "    return(NULL)\n",
    "  }\n",
    ")\n",
    "\n",
    "# Extract Integrated Brier Score\n",
    "ibs <- NA\n",
    "if (!is.null(brier_model) && !is.null(brier_model$Brier$XGBoost)) {\n",
    "  ibs <- brier_model$Brier$XGBoost$integrated[1]\n",
    "} else {\n",
    "  message(\"Falling back to manual IBS calculation due to riskRegression failure.\")\n",
    "  \n",
    "  # Manual Brier Score calculation\n",
    "  times <- seq(0, max(test_data$time), length.out = 100)\n",
    "  surv_prob <- predictRisk.xgb.Booster(xgb_model, test_data, times)\n",
    "  \n",
    "  # Kaplan-Meier estimator for IPCW weights\n",
    "  km_fit <- survfit(Surv(time, status) ~ 1, data = test_data)\n",
    "  km_summary <- summary(km_fit, times = times, extend = TRUE)\n",
    "  G_t <- km_summary$surv  # Survival function for censoring distribution\n",
    "  \n",
    "  # Compute Brier Score at each time point\n",
    "  brier_scores <- sapply(1:length(times), function(j) {\n",
    "    t <- times[j]\n",
    "    surv_t <- surv_prob[, j]\n",
    "    event <- test_data$status\n",
    "    time <- test_data$time\n",
    "    # Indicator for event or censoring before time t\n",
    "    ind <- (time <= t & event == 1) | (time > t)\n",
    "    # IPCW weights: 1/G(t) for events, 1/G(t) for censored after t\n",
    "    weights <- ifelse(time <= t & event == 1, 1 / G_t[j], ifelse(time > t, 1 / G_t[j], 0))\n",
    "    # Brier score: mean squared difference between observed and predicted\n",
    "    brier <- mean(weights * ((time <= t & event == 1) - surv_t)^2, na.rm = TRUE)\n",
    "    return(brier)\n",
    "  })\n",
    "  \n",
    "  # Integrate Brier Scores to get IBS\n",
    "  ibs <- pracma::trapz(times, brier_scores) / (max(times) - min(times))\n",
    "}\n",
    "\n",
    "# Print evaluation metrics\n",
    "cat(\"Integrated Brier Score:\", ibs, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d50d6363",
   "metadata": {},
   "source": [
    "#### Survival Curves "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6996029",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "# Plot survival curve\n",
    "times <- seq(0, max(test_data$time), length.out = 100)\n",
    "surv_prob <- predictRisk.xgb.Booster(xgb_model, test_data, times)\n",
    "surv_df <- data.frame(\n",
    "  time = times,\n",
    "  surv = colMeans(surv_prob)\n",
    ")\n",
    "\n",
    "# Create survival plot\n",
    "surv_plot <- ggsurvplot(\n",
    "  fit = survfit(Surv(time, status) ~ 1, data = test_data),\n",
    "  data = test_data,\n",
    "  title = \"Predicted Survival Curve- GBST (xgboost)\",\n",
    "  xlab = \"Time (days)\",\n",
    "  ylab = \"Survival Probability\"\n",
    ")\n",
    "surv_plot$plot <- surv_plot$plot +\n",
    "  geom_line(data = surv_df, aes(x = time, y = surv), color = \"red\", linetype = \"dashed\")\n",
    "surv_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b428fed",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "209b45cb",
   "metadata": {},
   "source": [
    "## Accelerated Failure Time (AFT) Gradient Boosting (AFTBoost)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df72fd53",
   "metadata": {},
   "source": [
    "\n",
    "Accelerated Failure Time (AFT) Gradient Boosting is a method that models survival data by assuming that the logarithm of the survival time follows a linear model. This approach allows for flexible modeling of survival times while accounting for censoring. The AFT model can be implemented using gradient boosting techniques, which iteratively fit weak learners to improve predictions. In R, packages like {gbm} and {xgboost} can be used to implement AFT models with custom loss functions. Below is a step-by-step guide to fitting an AFT Gradient Boosting model, including data preparation, model fitting, validation, and visualization.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afca2109",
   "metadata": {},
   "source": [
    "#### Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "252c2cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "# Prepare lung dataset\n",
    "set.seed(123)\n",
    "data(lung)\n",
    "# Remove missing values\n",
    "lung_complete <- lung %>% na.omit()\n",
    "\n",
    "# Split into 70% training and 30% testing\n",
    "n <- nrow(lung_complete)\n",
    "train_idx <- sample(1:n, size = round(0.7 * n))\n",
    "train_data <- lung_complete[train_idx, ]\n",
    "test_data <- lung_complete[-train_idx, ]\n",
    "\n",
    "# Prepare data for xgboost\n",
    "# Create feature matrix (exclude time and status)\n",
    "features <- c(\"age\", \"sex\", \"ph.ecog\", \"ph.karno\", \"pat.karno\", \"meal.cal\", \"wt.loss\")\n",
    "X_train <- as.matrix(train_data[, features])\n",
    "X_test <- as.matrix(test_data[, features])\n",
    "\n",
    "# For AFT, survival times are log-transformed, and status is used for censoring\n",
    "# xgboost expects y_lower_bound and y_upper_bound for survival data\n",
    "# For uncensored: y_lower = y_upper = log(time)\n",
    "# For censored: y_lower = log(time), y_upper = Inf\n",
    "y_lower_train <- log(train_data$time)\n",
    "y_upper_train <- ifelse(train_data$status == 2, log(train_data$time), Inf)  # status=2 is event\n",
    "y_lower_test <- log(test_data$time)\n",
    "y_upper_test <- ifelse(test_data$status == 2, log(test_data$time), Inf)\n",
    "\n",
    "# Create DMatrix for xgboost with labels in constructor\n",
    "dtrain <- xgb.DMatrix(data = X_train, label_lower_bound = y_lower_train, label_upper_bound = y_upper_train)\n",
    "dtest <- xgb.DMatrix(data = X_test, label_lower_bound = y_lower_test, label_upper_bound = y_upper_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb486aa",
   "metadata": {},
   "source": [
    "#### Fit AFT Gradient Boosting Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a99976b",
   "metadata": {},
   "source": [
    "\n",
    "AFT Gradient Boosting can be implemented using the `xgboost` package in R. The model is trained to minimize the negative log-likelihood of the AFT model, which can be specified through the `objective` parameter. The provided params list is used with `objective = \"survival:aft\"` and `eval_metric = \"aft-nloglik\"`. The model is trained with xgb.train for 100 boosting rounds (`nrounds = 100`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "025d4648",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "# Define parameters for AFT model\n",
    "params <- list(\n",
    "  objective = \"survival:aft\",\n",
    "  eval_metric = \"aft-nloglik\",\n",
    "  tree_method = \"hist\",          # efficient tree growth\n",
    "  aft_loss_distribution = \"normal\",  # normal distribution for AFT\n",
    "  aft_loss_distribution_scale = 1.0,\n",
    "  max_depth = 3,\n",
    "  eta = 0.1\n",
    ")\n",
    "\n",
    "# Fit AFT model using xgb.train\n",
    "set.seed(123)\n",
    "bst <- xgb.train(\n",
    "  params = params,\n",
    "  data = dtrain,\n",
    "  nrounds = 100,\n",
    "  verbose = 1\n",
    ")\n",
    "print(bst)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a57eed6b",
   "metadata": {},
   "source": [
    "### Variable Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c9983c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "importance_matrix <- xgb.importance(model = bst)\n",
    "xgb.plot.importance(importance_matrix, las = 2, main = \"Variable Importance (AFT XGBoost)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b92fd8b",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41fe2a33",
   "metadata": {},
   "source": [
    "### Prediction and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0babcbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "# Predict log-survival times on test set\n",
    "pred_log_time <- predict(bst, dtest)\n",
    "pred_time <- exp(pred_log_time)  # Exponentiate to get survival times"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "070a11eb",
   "metadata": {},
   "source": [
    "### Compute concordance index (C-index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12aace7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "# Compute concordance index (C-index)\n",
    "c_index <- concordance(Surv(time, status) ~ pred_time, data = test_data)$concordance\n",
    "cat(\"C-index:\", round(c_index, 3), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc67434e",
   "metadata": {},
   "source": [
    "### Integrated Brier Score (IBS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "201a926d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "eval_times <- seq(0, max(test_data$time), length.out = 100)\n",
    "brier_scores <- numeric(length(eval_times))\n",
    "\n",
    "# Assume Weibull distribution for survival probabilities\n",
    "aft_model <- survreg(Surv(time, status) ~ 1, data = train_data, dist = \"weibull\")\n",
    "shape <- 1 / aft_model$scale\n",
    "scale <- exp(coef(aft_model))\n",
    "\n",
    "for (i in seq_along(eval_times)) {\n",
    "  surv_probs <- exp(-(eval_times[i] / (pred_time * scale))^shape)\n",
    "  brier_scores[i] <- mean((as.numeric(test_data$time > eval_times[i]) - surv_probs)^2, na.rm = TRUE)\n",
    "}\n",
    "ibs <- mean(brier_scores, na.rm = TRUE)\n",
    "cat(\"Integrated Brier Score (IBS):\", round(ibs, 3), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b02eecc",
   "metadata": {},
   "source": [
    "### Survival Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d4e6a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "# Generate survival curves\n",
    "# Predict survival probabilities for test data\n",
    "surv_probs <- matrix(NA, nrow = nrow(test_data), ncol = length(eval_times))\n",
    "for (i in 1:nrow(test_data)) {\n",
    "  surv_probs[i, ] <- exp(-(eval_times / (pred_time[i] * scale))^shape)\n",
    "}\n",
    "\n",
    "# Create data frame for plotting\n",
    "surv_df <- data.frame(\n",
    "  Time = rep(eval_times, nrow(test_data)),\n",
    "  Survival = as.vector(t(surv_probs)),\n",
    "  Subject = rep(1:nrow(test_data), each = length(eval_times))\n",
    ")\n",
    "\n",
    "# Plot survival curves for a subset of subjects (first 5 for clarity)\n",
    "subset_subjects <- 1:min(5, nrow(test_data))\n",
    "surv_df_subset <- surv_df[surv_df$Subject %in% subset_subjects, ]\n",
    "\n",
    "ggplot(surv_df_subset, aes(x = Time, y = Survival, color = factor(Subject))) +\n",
    "  geom_line(linewidth = 1) +\n",
    "  labs(title = \"Survival Curves for Test Data (AFT XGBoost Model)\",\n",
    "       x = \"Time\", y = \"Survival Probability\",\n",
    "       color = \"Subject ID\") +\n",
    "  theme_minimal() +\n",
    "  scale_color_brewer(palette = \"Set1\") +\n",
    "  theme(legend.position = \"top\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca32f4e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "# Plot average survival curve\n",
    "avg_surv <- colMeans(surv_probs)\n",
    "avg_surv_df <- data.frame(Time = eval_times, Survival = avg_surv)\n",
    "\n",
    "ggplot(avg_surv_df, aes(x = Time, y = Survival)) +\n",
    "  geom_line(linewidth = 1, color = \"blue\") +\n",
    "  labs(title = \"Average Survival Curve for Test Data (AFT XGBoost Model)\",\n",
    "       x = \"Time\", y = \"Survival Probability\") +\n",
    "  theme_minimal()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba859328",
   "metadata": {},
   "source": [
    "## Survival Gradient Boosting with Custom Loss Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd74810",
   "metadata": {},
   "source": [
    "\n",
    "Survival Gradient Boosting with custom loss functions allows for flexible modeling of survival data by defining specific loss functions that capture the characteristics of the survival outcome. This approach can be particularly useful when standard loss functions do not adequately represent the underlying survival process. In R, packages like {xgboost} and {gbm} can be used to implement survival gradient boosting with custom loss functions. Below is a step-by-step guide to fitting a survival gradient boosting model with a custom loss function, including data preparation, model fitting, validation, and visualization. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a3459b",
   "metadata": {},
   "source": [
    "### Load and Prepare Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa46e244",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b08dea74",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "### Data and Data Preparation\n",
    "\n",
    "# Load the lung dataset from survival package\n",
    "data(lung, package = \"survival\")\n",
    "\n",
    "# Remove missing values\n",
    "lung_complete <- lung %>% na.omit()\n",
    "\n",
    "# Split into 70% training and 30% testing\n",
    "set.seed(123)\n",
    "train_indices <- sample(1:nrow(lung_complete), size = 0.7 * nrow(lung_complete))\n",
    "train_data <- lung_complete[train_indices, ]\n",
    "test_data <- lung_complete[-train_indices, ]\n",
    "\n",
    "# Training data\n",
    "time_train <- train_data$time\n",
    "status_train <- train_data$status - 1 # Convert to 0=censored, 1=event\n",
    "X_train <- train_data %>% dplyr::select(-time, -status)\n",
    "X_train_matrix <- as.matrix(X_train)\n",
    "\n",
    "# Test data\n",
    "time_test <- test_data$time\n",
    "status_test <- test_data$status - 1 # Convert to 0=censored, 1=event\n",
    "X_test <- test_data %>% dplyr::select(-time, -status)\n",
    "X_test_matrix <- as.matrix(X_test)\n",
    "\n",
    "# Prepare Data for XGBoost\n",
    "\n",
    "# Create XGBoost DMatrix\n",
    "dtrain <- xgb.DMatrix(X_train_matrix, label = time_train, weight = status_train)\n",
    "dtest <- xgb.DMatrix(X_test_matrix, label = time_test, weight = status_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d979180f",
   "metadata": {},
   "source": [
    "### Fit Survival Gradient Boosting Model with Custom Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90902d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "### Custom Objective Function for Cox Proportional Hazards\n",
    "\n",
    "cox_objective <- function(preds, dtrain) {\n",
    "  # Custom objective function for Cox proportional hazards\n",
    "  # Approximates the negative partial log-likelihood gradient and Hessian\n",
    "  y_true <- getinfo(dtrain, \"label\") # Time-to-event or censoring time\n",
    "  event_observed <- getinfo(dtrain, \"weight\") # Event indicator (1=event, 0=censored)\n",
    "  \n",
    "  # Sort by time in descending order for risk set\n",
    "  order <- order(-y_true)\n",
    "  preds_sorted <- preds[order]\n",
    "  y_true_sorted <- y_true[order]\n",
    "  event_observed_sorted <- event_observed[order]\n",
    "  \n",
    "  # Calculate risk set sum\n",
    "  exp_preds_sorted <- exp(preds_sorted)\n",
    "  risk_set_sum <- rev(cumsum(rev(exp_preds_sorted))) # Cumulative sum from the end\n",
    "  \n",
    "  # Gradient and Hessian\n",
    "  gradient <- -event_observed_sorted + (event_observed_sorted * exp_preds_sorted) / risk_set_sum\n",
    "  hessian <- (event_observed_sorted * exp_preds_sorted * risk_set_sum - \n",
    "              (event_observed_sorted * exp_preds_sorted)^2) / risk_set_sum^2\n",
    "  \n",
    "  # Reorder back to original\n",
    "  reorder <- order(order)\n",
    "  list(grad = gradient[reorder], hess = hessian[reorder])\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6459ba25",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc00bc44",
   "metadata": {},
   "source": [
    "### Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43dfd054",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "### Train the Model\n",
    "\n",
    "# Define XGBoost parameters\n",
    "params <- list(\n",
    "  eta = 0.05, # Learning rate\n",
    "  max_depth = 3,\n",
    "  subsample = 0.8,\n",
    "  colsample_bytree = 0.8,\n",
    "  seed = 123,\n",
    "  nthread = parallel::detectCores() - 1,\n",
    "  tree_method = \"hist\"\n",
    ")\n",
    "\n",
    "# Number of boosting rounds\n",
    "num_boost_round <- 100\n",
    "\n",
    "# Train the model\n",
    "cat(sprintf(\"Training XGBoost model with %d boosting rounds...\\n\", num_boost_round))\n",
    "initial_xgb_model <- xgb.train(\n",
    "  params = params,\n",
    "  data = dtrain,\n",
    "  nrounds = num_boost_round,\n",
    "  watchlist = list(train = dtrain, eval = dtest),\n",
    "  obj = cox_objective,\n",
    "  verbose = 10\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c4bc491",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cdb7aa1",
   "metadata": {},
   "source": [
    "### Variable Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "813c218c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "# figure width: 6.5\n",
    "# Get feature importance\n",
    "importance_xgb <- xgb.importance(model = initial_xgb_model, feature_names = colnames(X_train))\n",
    "\n",
    "# Plot variable importance\n",
    "ggplot(importance_xgb, aes(x = reorder(Feature, Gain), y = Gain)) +\n",
    "  geom_bar(stat = \"identity\") +\n",
    "  coord_flip() +\n",
    "  ggtitle(\"Feature Importance (XGBoost) - Type: 'Gain'\") +\n",
    "  xlab(\"Variable\") +\n",
    "  ylab(\"Importance (Gain)\") +\n",
    "  theme_minimal() +\n",
    "  theme(\n",
    "    panel.grid.major.x = element_line(color = \"gray80\"),\n",
    "    panel.grid.minor.x = element_line(color = \"gray90\")\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e991e114",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ed4c0b",
   "metadata": {},
   "source": [
    "### Prediction and Evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c213dc64",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "# Predict log partial hazards\n",
    "train_preds <- predict(initial_xgb_model, dtrain)\n",
    "test_preds <- predict(initial_xgb_model, dtest)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a2e01cf",
   "metadata": {},
   "source": [
    "### Concordance Index (C-index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92cbb3b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "# Calculate C-index using survival package\n",
    "train_cindex <- survcomp::concordance.index(\n",
    "  x = train_preds,\n",
    "  surv.time = time_train,\n",
    "  surv.event = status_train,\n",
    "  method = \"noether\"\n",
    ")$c.index\n",
    "test_cindex <- survcomp::concordance.index(\n",
    "  x = test_preds,\n",
    "  surv.time = time_test,\n",
    "  surv.event = status_test,\n",
    "  method = \"noether\"\n",
    ")$c.index\n",
    "\n",
    "cat(sprintf(\"Initial XGBoost model C-index (Train): %.3f\\n\", train_cindex))\n",
    "cat(sprintf(\"Initial XGBoost model C-index (Test): %.3f\\n\", test_cindex))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57bd1a82",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f56725b1",
   "metadata": {},
   "source": [
    "### Integrated Brier Score (IBS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0636ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "# Define evaluation times\n",
    "eval_times <- seq(0, max(time_test), length.out = 100)\n",
    "\n",
    "# Estimate baseline cumulative hazard using Breslow estimator\n",
    "train_data_baseline <- data.frame(\n",
    "  time = time_train,\n",
    "  status = status_train,\n",
    "  hazard_ratio = exp(predict(initial_xgb_model, dtrain))\n",
    ") %>% arrange(time)\n",
    "\n",
    "unique_event_times <- unique(train_data_baseline$time[train_data_baseline$status == 1])\n",
    "baseline_cumulative_hazard_values <- numeric(length(unique_event_times))\n",
    "cumulative_hazard <- 0\n",
    "\n",
    "for (i in seq_along(unique_event_times)) {\n",
    "  event_time <- unique_event_times[i]\n",
    "  at_risk_indices <- which(train_data_baseline$time >= event_time)\n",
    "  sum_hazard_ratios <- sum(train_data_baseline$hazard_ratio[at_risk_indices])\n",
    "  events_at_time <- sum(train_data_baseline$time == event_time & train_data_baseline$status == 1)\n",
    "  hazard_increment <- ifelse(sum_hazard_ratios < .Machine$double.eps, 0, events_at_time / sum_hazard_ratios)\n",
    "  cumulative_hazard <- cumulative_hazard + hazard_increment\n",
    "  baseline_cumulative_hazard_values[i] <- cumulative_hazard\n",
    "}\n",
    "\n",
    "# Add time 0 with hazard 0\n",
    "baseline_times <- c(0, unique_event_times)\n",
    "baseline_values <- c(0, baseline_cumulative_hazard_values)\n",
    "\n",
    "# Function to get baseline cumulative hazard at a given time\n",
    "get_baseline_cumulative_hazard <- function(query_time, baseline_times, baseline_values) {\n",
    "  if (query_time < baseline_times[1]) return(0)\n",
    "  if (query_time >= baseline_times[length(baseline_times)]) return(baseline_values[length(baseline_values)])\n",
    "  \n",
    "  idx <- findInterval(query_time, baseline_times)\n",
    "  if (baseline_times[idx] == query_time) return(baseline_values[idx])\n",
    "  \n",
    "  t_before <- baseline_times[idx]\n",
    "  t_after <- baseline_times[idx + 1]\n",
    "  lambda_before <- baseline_values[idx]\n",
    "  lambda_after <- baseline_values[idx + 1]\n",
    "  \n",
    "  if (t_after - t_before == 0) return(lambda_before)\n",
    "  lambda_before + (lambda_after - lambda_before) * (query_time - t_before) / (t_after - t_before)\n",
    "}\n",
    "\n",
    "# Calculate predicted survival probabilities\n",
    "test_hazard_ratios <- exp(predict(initial_xgb_model, dtest))\n",
    "predicted_survival_xgboost <- matrix(NA, nrow = nrow(X_test), ncol = length(eval_times))\n",
    "\n",
    "for (i in seq_along(eval_times)) {\n",
    "  lambda0_t <- get_baseline_cumulative_hazard(eval_times[i], baseline_times, baseline_values)\n",
    "  predicted_survival_xgboost[, i] <- exp(-lambda0_t * test_hazard_ratios)\n",
    "}\n",
    "\n",
    "# Kaplan-Meier for censoring distribution\n",
    "km_censor_fit <- survfit(Surv(time_test, 1 - status_test) ~ 1)\n",
    "\n",
    "# Function to get censoring survival probability\n",
    "get_survival_at_time <- function(query_time, km_fit) {\n",
    "  idx <- findInterval(query_time, km_fit$time)\n",
    "  if (idx == 0) return(1)\n",
    "  if (idx >= length(km_fit$surv)) return(km_fit$surv[length(km_fit$surv)])\n",
    "  km_fit$surv[idx]\n",
    "}\n",
    "\n",
    "# Calculate IBS\n",
    "xgboost_brier <- numeric(length(eval_times))\n",
    "for (i in seq_along(eval_times)) {\n",
    "  t <- eval_times[i]\n",
    "  predicted_surv <- predicted_survival_xgboost[, i]\n",
    "  brier_at_t <- numeric()\n",
    "  \n",
    "  for (j in 1:length(time_test)) {\n",
    "    G_min_Tt <- get_survival_at_time(min(time_test[j], t), km_censor_fit)\n",
    "    G_min_Tt <- max(G_min_Tt, .Machine$double.eps) # Avoid division by zero\n",
    "    \n",
    "    if (time_test[j] <= t && status_test[j] == 1) {\n",
    "      score_component <- (0 - predicted_surv[j])^2 / G_min_Tt\n",
    "    } else if (time_test[j] > t) {\n",
    "      score_component <- (1 - predicted_surv[j])^2 / G_min_Tt\n",
    "    } else {\n",
    "      score_component <- 0\n",
    "    }\n",
    "    brier_at_t <- c(brier_at_t, score_component)\n",
    "  }\n",
    "  \n",
    "  xgboost_brier[i] <- mean(brier_at_t, na.rm = TRUE)\n",
    "}\n",
    "\n",
    "# Integrated Brier Score (IBS) for XGBoost model\n",
    "xgboost_ibs <- mean(xgboost_brier, na.rm = TRUE)\n",
    "cat(sprintf(\"XGBoost model IBS: %.3f\\n\", xgboost_ibs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cbe3dc4",
   "metadata": {},
   "source": [
    "### Kaplan-Meier baseline IBS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1844d8d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "# Kaplan-Meier baseline IBS\n",
    "km_fit <- survfit(Surv(time_test, status_test) ~ 1)\n",
    "km_brier <- numeric(length(eval_times))\n",
    "for (i in seq_along(eval_times)) {\n",
    "  t <- eval_times[i]\n",
    "  km_surv <- get_survival_at_time(t, km_fit)\n",
    "  brier_at_t <- numeric()\n",
    "  \n",
    "  for (j in 1:length(time_test)) {\n",
    "    G_min_Tt <- get_survival_at_time(min(time_test[j], t), km_censor_fit)\n",
    "    G_min_Tt <- max(G_min_Tt, .Machine$double.eps)\n",
    "    \n",
    "    if (time_test[j] <= t && status_test[j] == 1) {\n",
    "      score_component <- (0 - km_surv)^2 / G_min_Tt\n",
    "    } else if (time_test[j] > t) {\n",
    "      score_component <- (1 - km_surv)^2 / G_min_Tt\n",
    "    } else {\n",
    "      score_component <- 0\n",
    "    }\n",
    "    brier_at_t <- c(brier_at_t, score_component)\n",
    "  }\n",
    "  \n",
    "  km_brier[i] <- mean(brier_at_t, na.rm = TRUE)\n",
    "}\n",
    "# Calculate Integrated Brier Score (IBS) for Kaplan-Meier baseline\n",
    "km_ibs <- mean(km_brier, na.rm = TRUE)\n",
    "cat(sprintf(\"Kaplan-Meier baseline IBS: %.3f\\n\", km_ibs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1297971",
   "metadata": {},
   "source": [
    "###  Survival Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb9dc353",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "# Calculate average survival curve\n",
    "average_surv_xgboost <- colMeans(predicted_survival_xgboost)\n",
    "surv_data_xgboost <- data.frame(\n",
    "  time = eval_times,\n",
    "  surv = average_surv_xgboost\n",
    ")\n",
    "\n",
    "# Plot survival curve\n",
    "ggplot(surv_data_xgboost, aes(x = time, y = surv)) +\n",
    "  geom_line(color = \"red\") +\n",
    "  ggtitle(\"Average Survival Curve (XGBoost) on Test Data\") +\n",
    "  xlab(\"Time\") +\n",
    "  ylab(\"Survival Probability\") +\n",
    "  theme_minimal() +\n",
    "  theme(\n",
    "    panel.grid.major = element_line(color = \"gray80\"),\n",
    "    panel.grid.minor = element_line(color = \"gray90\")\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ec47a2",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "581551ca",
   "metadata": {},
   "source": [
    "## Summary and Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8032d559",
   "metadata": {},
   "source": [
    "\n",
    "This notebook explores three types of Gradient Boosted Survival Models and their implementations in R: CoxBoost, GBST, and AFTBoost. \n",
    "\n",
    "**CoxBoost** model extends the Cox proportional hazards model using gradient boosting to model the log-hazard function. It assumes proportional hazards and optimizes the negative log-partial likelihood. The notebook demonstrates its implementation using the `{CoxBoost}` package, including data preparation, model fitting, cross-validation for penalty and steps, prediction, and evaluation using C-index and Integrated Brier Score (IBS).\n",
    "\n",
    "**GBST (Gradient Boosting Survival Trees)** is a generic framework that applies gradient boosting to survival data using tree-based models and survival-specific loss functions. CoxBoost is a special case of GBST. The notebook shows implementations using both the `{gbm}` package (with `distribution = \"coxph\"`) and the `{xgboost}` package (with `objective = \"survival:cox\"`), covering data preparation, model fitting, cross-validation (for `{gbm}`), variable importance, prediction, and evaluation with C-index and IBS.\n",
    "\n",
    "**AFTBoost (Accelerated Failure Time Gradient Boosting)**  models the logarithm of the survival time as a linear function of covariates, assuming predictors accelerate or decelerate the time to an event. The notebook demonstrates its implementation using the `{xgboost}` package (with `objective = \"survival:aft\"`), including data preparation (using `label_lower_bound` and `label_upper_bound`), model fitting, variable importance, prediction of log-survival times, and evaluation with C-index and IBS.\n",
    "\n",
    "\n",
    "The notebook successfully demonstrates the implementation of three different gradient boosted survival models in R using the `{CoxBoost}`, `{gbm}`, and `{xgboost}` packages. Each model offers a distinct approach to survival analysis, with CoxBoost and GBST focusing on hazard-based modeling (often using the Cox partial likelihood) and AFTBoost directly modeling survival times. The examples provided cover essential steps like data preparation, model fitting, hyperparameter tuning (through cross-validation), assessing variable importance, and evaluating model performance using metrics like C-index and IBS. The visualizations of survival curves offer insights into the models' predictions. The notebook serves as a valuable introduction to applying gradient boosting techniques to time-to-event data in R.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d7a3554",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e43e1246",
   "metadata": {},
   "source": [
    "\n",
    "1. Chen, T., & Guestrin, C. (2016). XGBoost: A Scalable Tree Boosting System. *KDD '16*, 785–794.  \n",
    "\n",
    "\n",
    "2. Hothorn, T., et al. (2006). Survival Ensembles. *Biostatistics, 7*(3), 355–373.  \n",
    "\n",
    "3. Li, K., et al. (2022). Efficient Gradient Boosting for Prognostic Biomarker Discovery. *Bioinformatics, 38*(6), 1631–1638.  \n",
    "\n",
    "4. Wang, Z., & Wang, C. Y. (2018). Gradient Boosting for Concordance Index. *Comput Math Methods Med, 2018*, 8734680.  \n",
    "  \n",
    "\n",
    "5. Zhang, H., et al. (2019). Gradient Boosting Survival Tree. *arXiv:1908.03385*.  "
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
