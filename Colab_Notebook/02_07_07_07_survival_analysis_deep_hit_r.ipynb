{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# DeepHit Tutorial: Deep Learning for Competing Risks Survival Analysis\n",
        "\n",
        "This tutorial provides a comprehensive guide to implementing DeepHit, a deep learning model for competing risks survival analysis developed by Changhee Lee et al. in 2018.\n",
        "\n",
        "**Reference:** [DeepHit GitHub Repository](https://github.com/chl8856/DeepHit)\n",
        "\n",
        "**Paper:** [DeepHit: A Deep Learning Approach to Survival Analysis with Competing Risks](http://medianetlab.ee.ucla.edu/papers/AAAI_2018_DeepHit)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Overview\n",
        "\n",
        "DeepHit is a deep learning model designed for survival analysis with competing risks. Unlike traditional survival models like Cox Proportional Hazards, DeepHit can handle:\n",
        "\n",
        "- **Multiple competing events**: Model different types of events that can occur\n",
        "- **Non-linear relationships**: Capture complex interactions between covariates and survival outcomes\n",
        "- **Time-dependent effects**: Model how the effect of covariates changes over time\n",
        "- **No proportional hazards assumption**: More flexible than Cox models\n",
        "\n",
        "### Key Advantages:\n",
        "1. **Flexibility**: Can model complex non-linear relationships\n",
        "2. **Competing Risks**: Handles multiple event types simultaneously\n",
        "3. **No assumptions**: Doesn't require proportional hazards assumption\n",
        "4. **Performance**: Often outperforms traditional methods on complex datasets\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. How It Works\n",
        "\n",
        "DeepHit uses a deep neural network to estimate the joint distribution of survival times and event types. The model architecture consists of:\n",
        "\n",
        "### Architecture:\n",
        "1. **Shared Sub-network**: Learns common representations from input features\n",
        "2. **Cause-Specific Sub-networks**: Separate networks for each competing event type\n",
        "3. **Output Layer**: Produces probability mass functions (PMF) over discrete time intervals for each event type\n",
        "\n",
        "### Loss Function:\n",
        "The model uses a combination of:\n",
        "- **Likelihood loss**: Maximizes the probability of observed events\n",
        "- **Ranking loss**: Ensures correct ordering of survival times\n",
        "- **Cause-specific loss**: Handles competing risks appropriately\n",
        "\n",
        "### Training Process:\n",
        "1. Input features are passed through shared layers\n",
        "2. Outputs are split into cause-specific branches\n",
        "3. Each branch produces PMF over time intervals\n",
        "4. Loss is computed based on observed events and times\n",
        "5. Model is optimized using backpropagation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Setup R Libraries\n",
        "\n",
        "First, let's install and load all necessary libraries.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "r"
        }
      },
      "outputs": [],
      "source": [
        "# Install required packages (run once if not installed)\n",
        "# install.packages(c(\"torch\", \"dplyr\", \"ggplot2\", \"tidyr\", \"caret\", \"scales\", \"viridis\", \"RColorBrewer\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "r"
        }
      },
      "outputs": [],
      "source": [
        "# Load required libraries\n",
        "library(torch)\n",
        "library(dplyr)\n",
        "library(ggplot2)\n",
        "library(tidyr)\n",
        "library(caret)\n",
        "library(scales)\n",
        "library(viridis)\n",
        "library(RColorBrewer)\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "set.seed(42)\n",
        "torch_manual_seed(42)\n",
        "\n",
        "# Check if CUDA is available\n",
        "cat(\"CUDA available:\", cuda_is_available(), \"\\n\")\n",
        "cat(\"Libraries loaded successfully!\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Create Synthetic Data\n",
        "\n",
        "We'll create synthetic data similar to the DeepHit repository's synthetic dataset. The data will include:\n",
        "- Multiple features (covariates)\n",
        "- Event times (time to event or censoring)\n",
        "- Event indicators (0: censored, 1: event type 1, 2: event type 2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "r"
        }
      },
      "outputs": [],
      "source": [
        "generate_synthetic_competing_risks_data <- function(n_samples = 1000, n_features = 12, n_risks = 2, seed = 42) {\n",
        "  # Generate synthetic competing risks survival data.\n",
        "  # \n",
        "  # Parameters:\n",
        "  # n_samples: Number of samples\n",
        "  # n_features: Number of features\n",
        "  # n_risks: Number of competing risks (event types)\n",
        "  # seed: Random seed\n",
        "  #\n",
        "  # Returns:\n",
        "  # data.frame with features, time, and event columns\n",
        "  set.seed(seed)\n",
        "  \n",
        "  # Generate features\n",
        "  X <- matrix(rnorm(n_samples * n_features), nrow = n_samples, ncol = n_features)\n",
        "  \n",
        "  # Create feature names\n",
        "  feature_names <- paste0(\"x\", 1:n_features)\n",
        "  colnames(X) <- feature_names\n",
        "  \n",
        "  # Generate event times and types based on features\n",
        "  times <- numeric(n_samples)\n",
        "  events <- integer(n_samples)\n",
        "  \n",
        "  for (i in 1:n_samples) {\n",
        "    # Create hazard functions that depend on features\n",
        "    # Risk 1: depends on first few features\n",
        "    hazard_1 <- exp(0.5 * X[i, 1] + 0.3 * X[i, 2] - 0.2 * X[i, 3])\n",
        "    \n",
        "    # Risk 2: depends on different features\n",
        "    hazard_2 <- exp(0.4 * X[i, 4] + 0.3 * X[i, 5] - 0.1 * X[i, 6])\n",
        "    \n",
        "    # Generate time to event for each risk\n",
        "    time_1 <- ifelse(hazard_1 > 0, rexp(1, rate = hazard_1), Inf)\n",
        "    time_2 <- ifelse(hazard_2 > 0, rexp(1, rate = hazard_2), Inf)\n",
        "    \n",
        "    # Determine which event occurs first\n",
        "    min_time <- min(time_1, time_2)\n",
        "    \n",
        "    # Add some censoring (30% censoring rate)\n",
        "    censor_time <- rexp(1, rate = 1/15)\n",
        "    \n",
        "    if (censor_time < min_time) {\n",
        "      times[i] <- censor_time\n",
        "      events[i] <- 0  # Censored\n",
        "    } else {\n",
        "      times[i] <- min_time\n",
        "      events[i] <- ifelse(time_1 < time_2, 1, 2)\n",
        "    }\n",
        "  }\n",
        "  \n",
        "  # Create data frame\n",
        "  data <- as.data.frame(X)\n",
        "  data$time <- times\n",
        "  data$event <- events\n",
        "  \n",
        "  return(data)\n",
        "}\n",
        "\n",
        "# Generate synthetic data\n",
        "data <- generate_synthetic_competing_risks_data(n_samples = 2000, n_features = 12, n_risks = 2)\n",
        "\n",
        "cat(\"Data shape:\", nrow(data), \"x\", ncol(data), \"\\n\")\n",
        "cat(\"\\nEvent distribution:\\n\")\n",
        "print(table(data$event))\n",
        "cat(\"\\nCensoring rate:\", mean(data$event == 0) * 100, \"%\\n\")\n",
        "cat(\"\\nFirst few rows:\\n\")\n",
        "print(head(data))\n",
        "\n",
        "# Save to CSV\n",
        "write.csv(data, 'synthetic_comprisk.csv', row.names = FALSE)\n",
        "cat(\"\\nData saved to 'synthetic_comprisk.csv'\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Split Data\n",
        "\n",
        "Split the data into training, validation, and testing sets.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "r"
        }
      },
      "outputs": [],
      "source": [
        "# Load data if not already in memory\n",
        "# data <- read.csv('synthetic_comprisk.csv')\n",
        "\n",
        "# Separate features and targets\n",
        "X <- data %>% select(-time, -event)\n",
        "y <- data %>% select(time, event)\n",
        "\n",
        "# Split into train and test (80/20)\n",
        "train_idx <- createDataPartition(y$event, p = 0.8, list = FALSE)\n",
        "X_train <- X[train_idx, ]\n",
        "X_test <- X[-train_idx, ]\n",
        "y_train <- y[train_idx, ]\n",
        "y_test <- y[-train_idx, ]\n",
        "\n",
        "# Further split training data into train and validation (80/20 of training)\n",
        "val_idx <- createDataPartition(y_train$event, p = 0.2, list = FALSE)\n",
        "X_val <- X_train[val_idx, ]\n",
        "X_train <- X_train[-val_idx, ]\n",
        "y_val <- y_train[val_idx, ]\n",
        "y_train <- y_train[-val_idx, ]\n",
        "\n",
        "cat(\"Training set:\", nrow(X_train), \"samples\\n\")\n",
        "cat(\"Validation set:\", nrow(X_val), \"samples\\n\")\n",
        "cat(\"Test set:\", nrow(X_test), \"samples\\n\")\n",
        "cat(\"\\nTraining event distribution:\\n\")\n",
        "print(table(y_train$event))\n",
        "cat(\"\\nTest event distribution:\\n\")\n",
        "print(table(y_test$event))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Data Preprocessing\n",
        "\n",
        "Preprocess the data by standardizing features and preparing time intervals for DeepHit.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "r"
        }
      },
      "outputs": [],
      "source": [
        "# Standardize features\n",
        "preProc <- preProcess(X_train, method = c(\"center\", \"scale\"))\n",
        "X_train_scaled <- predict(preProc, X_train)\n",
        "X_val_scaled <- predict(preProc, X_val)\n",
        "X_test_scaled <- predict(preProc, X_test)\n",
        "\n",
        "# Convert to matrices for torch\n",
        "X_train_scaled <- as.matrix(X_train_scaled)\n",
        "X_val_scaled <- as.matrix(X_val_scaled)\n",
        "X_test_scaled <- as.matrix(X_test_scaled)\n",
        "\n",
        "# Create discrete time intervals for DeepHit\n",
        "# DeepHit works with discrete time intervals\n",
        "max_time <- max(max(y_train$time), max(y_val$time), max(y_test$time))\n",
        "num_intervals <- 50  # Number of discrete time intervals\n",
        "time_intervals <- seq(0, max_time, length.out = num_intervals + 1)\n",
        "\n",
        "discretize_time <- function(times, intervals) {\n",
        "  # Convert continuous times to discrete interval indices\n",
        "  return(findInterval(times, intervals, rightmost.closed = TRUE) - 1)\n",
        "}\n",
        "\n",
        "y_train_discrete <- discretize_time(y_train$time, time_intervals)\n",
        "y_val_discrete <- discretize_time(y_val$time, time_intervals)\n",
        "y_test_discrete <- discretize_time(y_test$time, time_intervals)\n",
        "\n",
        "# Clip to valid range\n",
        "y_train_discrete <- pmax(0, pmin(y_train_discrete, num_intervals - 1))\n",
        "y_val_discrete <- pmax(0, pmin(y_val_discrete, num_intervals - 1))\n",
        "y_test_discrete <- pmax(0, pmin(y_test_discrete, num_intervals - 1))\n",
        "\n",
        "cat(\"Time intervals:\", num_intervals, \"intervals\\n\")\n",
        "cat(\"Time range: 0 to\", round(max_time, 2), \"\\n\")\n",
        "cat(\"\\nTraining discrete time distribution (first 10 intervals):\\n\")\n",
        "print(table(y_train_discrete)[1:min(10, length(table(y_train_discrete)))])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. DeepHit Implementation\n",
        "\n",
        "Now we'll implement the DeepHit model. This is a custom torch implementation based on the original paper.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "r"
        }
      },
      "outputs": [],
      "source": [
        "# DeepHit Dataset class\n",
        "DeepHitDataset <- dataset(\n",
        "  name = \"DeepHitDataset\",\n",
        "  initialize = function(X, times, events) {\n",
        "    self$X <- torch_tensor(X, dtype = torch_float32())\n",
        "    self$times <- torch_tensor(times, dtype = torch_long())\n",
        "    self$events <- torch_tensor(events, dtype = torch_long())\n",
        "  },\n",
        "  .getitem = function(i) {\n",
        "    list(self$X[i, ], self$times[i], self$events[i])\n",
        "  },\n",
        "  .length = function() {\n",
        "    self$X$shape[1]\n",
        "  }\n",
        ")\n",
        "\n",
        "# DeepHit Network architecture\n",
        "DeepHitNetwork <- nn_module(\n",
        "  \"DeepHitNetwork\",\n",
        "  initialize = function(input_dim, hidden_dims, num_risks, num_intervals, dropout = 0.1) {\n",
        "    self$num_risks <- num_risks\n",
        "    self$num_intervals <- num_intervals\n",
        "    \n",
        "    # Shared layers\n",
        "    shared_layers <- list()\n",
        "    prev_dim <- input_dim\n",
        "    \n",
        "    for (hidden_dim in hidden_dims) {\n",
        "      shared_layers <- append(shared_layers, nn_linear(prev_dim, hidden_dim))\n",
        "      shared_layers <- append(shared_layers, nn_batch_norm1d(hidden_dim))\n",
        "      shared_layers <- append(shared_layers, nn_relu())\n",
        "      shared_layers <- append(shared_layers, nn_dropout(dropout))\n",
        "      prev_dim <- hidden_dim\n",
        "    }\n",
        "    \n",
        "    self$shared_layers <- nn_sequential(!!!shared_layers)\n",
        "    \n",
        "    # Cause-specific layers\n",
        "    self$risk_layers <- nn_module_list()\n",
        "    for (i in 1:num_risks) {\n",
        "      self$risk_layers$append(\n",
        "        nn_sequential(\n",
        "          nn_linear(prev_dim, as.integer(prev_dim / 2)),\n",
        "          nn_batch_norm1d(as.integer(prev_dim / 2)),\n",
        "          nn_relu(),\n",
        "          nn_dropout(dropout),\n",
        "          nn_linear(as.integer(prev_dim / 2), num_intervals)\n",
        "        )\n",
        "      )\n",
        "    }\n",
        "  },\n",
        "  forward = function(x) {\n",
        "    # Shared representation\n",
        "    shared <- self$shared_layers(x)\n",
        "    \n",
        "    # Cause-specific outputs\n",
        "    outputs <- list()\n",
        "    for (i in 1:self$num_risks) {\n",
        "      output <- self$risk_layers[[i]](shared)\n",
        "      outputs[[i]] <- output\n",
        "    }\n",
        "    \n",
        "    # Stack outputs: [batch_size, num_risks, num_intervals]\n",
        "    return(torch_stack(outputs, dim = 2))\n",
        "  }\n",
        ")\n",
        "\n",
        "# DeepHit loss function\n",
        "deephit_loss <- function(pred, times, events, alpha = 0.5, sigma = 0.1) {\n",
        "  # DeepHit loss function combining likelihood and ranking losses.\n",
        "  # \n",
        "  # Parameters:\n",
        "  # pred: Model predictions [batch_size, num_risks, num_intervals]\n",
        "  # times: Discrete time indices [batch_size]\n",
        "  # events: Event indicators [batch_size] (0: censored, 1+: event types)\n",
        "  # alpha: Weight for ranking loss\n",
        "  # sigma: Parameter for ranking loss\n",
        "  batch_size <- pred$shape[1]\n",
        "  num_risks <- pred$shape[2]\n",
        "  num_intervals <- pred$shape[3]\n",
        "  \n",
        "  # Apply softmax to get probabilities\n",
        "  pred_probs <- nnf_softmax(pred, dim = 3)  # [batch_size, num_risks, num_intervals]\n",
        "  \n",
        "  # Likelihood loss\n",
        "  likelihood_loss <- 0.0\n",
        "  \n",
        "  for (i in 1:batch_size) {\n",
        "    time_idx <- as.integer(times[i]$item()) + 1  # R is 1-indexed\n",
        "    event <- as.integer(events[i]$item())\n",
        "    \n",
        "    if (event == 0) {  # Censored\n",
        "      # For censored, sum probabilities of all risks up to censoring time\n",
        "      surv_prob <- 1.0 - torch_sum(pred_probs[i, , 1:time_idx])\n",
        "      likelihood_loss <- likelihood_loss - torch_log(surv_prob + 1e-8)\n",
        "    } else {  # Event occurred\n",
        "      # Probability of specific event at specific time\n",
        "      event_prob <- pred_probs[i, event, time_idx]\n",
        "      likelihood_loss <- likelihood_loss - torch_log(event_prob + 1e-8)\n",
        "    }\n",
        "  }\n",
        "  \n",
        "  likelihood_loss <- likelihood_loss / batch_size\n",
        "  \n",
        "  # Ranking loss (simplified version)\n",
        "  ranking_loss <- 0.0\n",
        "  count <- 0\n",
        "  \n",
        "  for (i in 1:batch_size) {\n",
        "    if (events[i]$item() == 0) {  # Skip censored\n",
        "      next\n",
        "    }\n",
        "    \n",
        "    time_i <- as.integer(times[i]$item()) + 1\n",
        "    event_i <- as.integer(events[i]$item())\n",
        "    \n",
        "    # Cumulative incidence for event i\n",
        "    cif_i <- torch_cumsum(pred_probs[i, event_i, ], dim = 1)\n",
        "    \n",
        "    for (j in 1:batch_size) {\n",
        "      if (i == j || events[j]$item() == 0) {\n",
        "        next\n",
        "      }\n",
        "      \n",
        "      time_j <- as.integer(times[j]$item()) + 1\n",
        "      \n",
        "      if (time_i < time_j) {\n",
        "        # i should have higher risk than j\n",
        "        cif_j <- torch_cumsum(pred_probs[j, event_i, ], dim = 1)\n",
        "        diff <- cif_j[time_j] - cif_i[time_i]\n",
        "        ranking_loss <- ranking_loss + torch_exp(-diff / sigma)\n",
        "        count <- count + 1\n",
        "      } else if (time_j < time_i) {\n",
        "        # j should have higher risk than i\n",
        "        cif_j <- torch_cumsum(pred_probs[j, event_i, ], dim = 1)\n",
        "        diff <- cif_i[time_i] - cif_j[time_j]\n",
        "        ranking_loss <- ranking_loss + torch_exp(-diff / sigma)\n",
        "        count <- count + 1\n",
        "      }\n",
        "    }\n",
        "  }\n",
        "  \n",
        "  if (count > 0) {\n",
        "    ranking_loss <- ranking_loss / count\n",
        "  }\n",
        "  \n",
        "  total_loss <- likelihood_loss + alpha * ranking_loss\n",
        "  \n",
        "  return(list(total = total_loss, likelihood = likelihood_loss, ranking = ranking_loss))\n",
        "}\n",
        "\n",
        "# Model parameters\n",
        "input_dim <- ncol(X_train_scaled)\n",
        "hidden_dims <- c(64, 32)\n",
        "num_risks <- 2  # Two competing risks\n",
        "num_intervals <- num_intervals\n",
        "\n",
        "# Create model\n",
        "model <- DeepHitNetwork(input_dim, hidden_dims, num_risks, num_intervals, dropout = 0.1)\n",
        "\n",
        "# Move to GPU if available\n",
        "device <- if (cuda_is_available()) \"cuda\" else \"cpu\"\n",
        "model <- model$to(device = device)\n",
        "\n",
        "total_params <- sum(sapply(model$parameters, function(p) prod(p$shape)))\n",
        "cat(\"Model created with\", total_params, \"parameters\\n\")\n",
        "cat(\"Using device:\", device, \"\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "r"
        }
      },
      "outputs": [],
      "source": [
        "# Training setup\n",
        "learning_rate <- 0.001\n",
        "batch_size <- 64\n",
        "num_epochs <- 100\n",
        "\n",
        "optimizer <- optim_adam(model$parameters, lr = learning_rate)\n",
        "\n",
        "# Create datasets\n",
        "train_dataset <- DeepHitDataset(X_train_scaled, y_train_discrete, y_train$event)\n",
        "val_dataset <- DeepHitDataset(X_val_scaled, y_val_discrete, y_val$event)\n",
        "\n",
        "train_dataloader <- dataloader(train_dataset, batch_size = batch_size, shuffle = TRUE)\n",
        "val_dataloader <- dataloader(val_dataset, batch_size = batch_size, shuffle = FALSE)\n",
        "\n",
        "# Training loop\n",
        "train_losses <- numeric(num_epochs)\n",
        "val_losses <- numeric(num_epochs)\n",
        "best_val_loss <- Inf\n",
        "patience <- 10\n",
        "patience_counter <- 0\n",
        "\n",
        "for (epoch in 1:num_epochs) {\n",
        "  # Training\n",
        "  model$train()\n",
        "  epoch_train_loss <- 0.0\n",
        "  batch_count <- 0\n",
        "  \n",
        "  coro::loop(for (batch in train_dataloader) {\n",
        "    batch_X <- batch[[1]]$to(device = device)\n",
        "    batch_times <- batch[[2]]$to(device = device)\n",
        "    batch_events <- batch[[3]]$to(device = device)\n",
        "    \n",
        "    optimizer$zero_grad()\n",
        "    pred <- model(batch_X)\n",
        "    loss_result <- deephit_loss(pred, batch_times, batch_events)\n",
        "    loss_result$total$backward()\n",
        "    optimizer$step()\n",
        "    \n",
        "    epoch_train_loss <- epoch_train_loss + loss_result$total$item()\n",
        "    batch_count <- batch_count + 1\n",
        "  })\n",
        "  \n",
        "  train_losses[epoch] <- epoch_train_loss / batch_count\n",
        "  \n",
        "  # Validation\n",
        "  model$eval()\n",
        "  epoch_val_loss <- 0.0\n",
        "  batch_count <- 0\n",
        "  \n",
        "  with_no_grad({\n",
        "    coro::loop(for (batch in val_dataloader) {\n",
        "      batch_X <- batch[[1]]$to(device = device)\n",
        "      batch_times <- batch[[2]]$to(device = device)\n",
        "      batch_events <- batch[[3]]$to(device = device)\n",
        "      \n",
        "      pred <- model(batch_X)\n",
        "      loss_result <- deephit_loss(pred, batch_times, batch_events)\n",
        "      epoch_val_loss <- epoch_val_loss + loss_result$total$item()\n",
        "      batch_count <- batch_count + 1\n",
        "    })\n",
        "  })\n",
        "  \n",
        "  val_losses[epoch] <- epoch_val_loss / batch_count\n",
        "  \n",
        "  # Early stopping\n",
        "  if (epoch_val_loss < best_val_loss) {\n",
        "    best_val_loss <- epoch_val_loss\n",
        "    patience_counter <- 0\n",
        "    # Save best model\n",
        "    torch_save(model$state_dict(), 'best_deephit_model.pth')\n",
        "  } else {\n",
        "    patience_counter <- patience_counter + 1\n",
        "  }\n",
        "  \n",
        "  if (epoch %% 10 == 0) {\n",
        "    cat(sprintf(\"Epoch [%d/%d], Train Loss: %.4f, Val Loss: %.4f\\n\", \n",
        "                epoch, num_epochs, train_losses[epoch], val_losses[epoch]))\n",
        "  }\n",
        "  \n",
        "  if (patience_counter >= patience) {\n",
        "    cat(sprintf(\"Early stopping at epoch %d\\n\", epoch))\n",
        "    break\n",
        "  }\n",
        "}\n",
        "\n",
        "# Load best model\n",
        "model$load_state_dict(torch_load('best_deephit_model.pth'))\n",
        "\n",
        "cat(\"\\nTraining completed!\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "r"
        }
      },
      "outputs": [],
      "source": [
        "# Plot training curves\n",
        "loss_df <- data.frame(\n",
        "  epoch = 1:length(train_losses),\n",
        "  train_loss = train_losses,\n",
        "  val_loss = val_losses\n",
        ") %>%\n",
        "  pivot_longer(cols = c(train_loss, val_loss), names_to = \"type\", values_to = \"loss\")\n",
        "\n",
        "ggplot(loss_df, aes(x = epoch, y = loss, color = type)) +\n",
        "  geom_line(linewidth = 1) +\n",
        "  labs(x = \"Epoch\", y = \"Loss\", \n",
        "       title = \"DeepHit Training Curves\",\n",
        "       color = \"Type\") +\n",
        "  theme_minimal() +\n",
        "  theme(plot.title = element_text(size = 14, face = \"bold\"),\n",
        "        legend.position = \"bottom\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Prediction Model Evaluation\n",
        "\n",
        "Evaluate the model's predictions on the test set.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "r"
        }
      },
      "outputs": [],
      "source": [
        "# Make predictions on test set\n",
        "model$eval()\n",
        "test_dataset <- DeepHitDataset(X_test_scaled, y_test_discrete, y_test$event)\n",
        "test_dataloader <- dataloader(test_dataset, batch_size = batch_size, shuffle = FALSE)\n",
        "\n",
        "all_preds <- list()\n",
        "all_times <- list()\n",
        "all_events <- list()\n",
        "\n",
        "with_no_grad({\n",
        "  coro::loop(for (batch in test_dataloader) {\n",
        "    batch_X <- batch[[1]]$to(device = device)\n",
        "    pred <- model(batch_X)\n",
        "    \n",
        "    # Convert to probabilities\n",
        "    pred_probs <- nnf_softmax(pred, dim = 3)\n",
        "    \n",
        "    all_preds <- append(all_preds, list(pred_probs$cpu()))\n",
        "    all_times <- append(all_times, list(batch[[2]]$cpu()))\n",
        "    all_events <- append(all_events, list(batch[[3]]$cpu()))\n",
        "  })\n",
        "})\n",
        "\n",
        "# Concatenate all predictions\n",
        "test_preds <- torch_cat(all_preds, dim = 1)  # [n_samples, num_risks, num_intervals]\n",
        "test_times <- torch_cat(all_times, dim = 1)\n",
        "test_events <- torch_cat(all_events, dim = 1)\n",
        "\n",
        "# Convert to R arrays\n",
        "test_preds <- as.array(test_preds)\n",
        "test_times <- as.array(test_times)\n",
        "test_events <- as.array(test_events)\n",
        "\n",
        "cat(\"Test predictions shape:\", paste(dim(test_preds), collapse = \" x \"), \"\\n\")\n",
        "cat(\"Test times shape:\", length(test_times), \"\\n\")\n",
        "cat(\"Test events shape:\", length(test_events), \"\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "r"
        }
      },
      "outputs": [],
      "source": [
        "# Calculate Cumulative Incidence Functions (CIF)\n",
        "calculate_cif <- function(pred_probs) {\n",
        "  # Calculate Cumulative Incidence Function from predicted probabilities\n",
        "  # pred_probs: [n_samples, num_risks, num_intervals]\n",
        "  cif <- apply(pred_probs, c(1, 2), cumsum)  # Cumulative sum over time\n",
        "  # Reshape to [n_samples, num_risks, num_intervals]\n",
        "  dim(cif) <- dim(pred_probs)\n",
        "  return(cif)\n",
        "}\n",
        "\n",
        "test_cif <- calculate_cif(test_preds)\n",
        "\n",
        "# Plot CIF for a few samples\n",
        "plot_data <- data.frame()\n",
        "for (risk_idx in 1:num_risks) {\n",
        "  for (i in 1:min(10, dim(test_cif)[1])) {\n",
        "    plot_data <- rbind(plot_data, data.frame(\n",
        "      time = time_intervals[2:length(time_intervals)],\n",
        "      cif = test_cif[i, risk_idx, ],\n",
        "      sample = paste0(\"Sample \", i),\n",
        "      risk = paste0(\"Risk \", risk_idx)\n",
        "    ))\n",
        "  }\n",
        "}\n",
        "\n",
        "ggplot(plot_data, aes(x = time, y = cif, color = sample)) +\n",
        "  geom_line(alpha = 0.6, linewidth = 1) +\n",
        "  facet_wrap(~risk, scales = \"free\") +\n",
        "  labs(x = \"Time\", y = \"Cumulative Incidence\",\n",
        "       title = \"Cumulative Incidence Function - First 10 Samples\") +\n",
        "  theme_minimal() +\n",
        "  theme(plot.title = element_text(size = 12, face = \"bold\"),\n",
        "        legend.position = \"none\")\n",
        "\n",
        "# Calculate mean CIF for each risk\n",
        "mean_cif_risk1 <- apply(test_cif[, 1, ], 2, mean)\n",
        "mean_cif_risk2 <- apply(test_cif[, 2, ], 2, mean)\n",
        "\n",
        "mean_cif_df <- data.frame(\n",
        "  time = time_intervals[2:length(time_intervals)],\n",
        "  risk1 = mean_cif_risk1,\n",
        "  risk2 = mean_cif_risk2\n",
        ") %>%\n",
        "  pivot_longer(cols = c(risk1, risk2), names_to = \"risk\", values_to = \"cif\")\n",
        "\n",
        "ggplot(mean_cif_df, aes(x = time, y = cif, color = risk)) +\n",
        "  geom_line(linewidth = 1.5) +\n",
        "  labs(x = \"Time\", y = \"Cumulative Incidence\",\n",
        "       title = \"Mean Cumulative Incidence Functions\",\n",
        "       color = \"Risk\") +\n",
        "  theme_minimal() +\n",
        "  theme(plot.title = element_text(size = 14, face = \"bold\"),\n",
        "        legend.position = \"bottom\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Performance Metrics\n",
        "\n",
        "Calculate various performance metrics to evaluate the model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "r"
        }
      },
      "outputs": [],
      "source": [
        "calculate_concordance_index <- function(pred_cif, times, events, risk_idx) {\n",
        "  # Calculate concordance index (C-index) for a specific risk.\n",
        "  # Simplified version - compares predicted risk at event time.\n",
        "  # Get predicted risk scores (CIF at event time or max CIF)\n",
        "  risk_scores <- numeric()\n",
        "  valid_indices <- integer()\n",
        "  \n",
        "  for (i in 1:length(times)) {\n",
        "    if (events[i] > 0) {  # Only consider uncensored\n",
        "      time_idx <- min(as.integer(times[i]) + 1, dim(pred_cif)[3])\n",
        "      risk_score <- pred_cif[i, risk_idx, time_idx]\n",
        "      risk_scores <- c(risk_scores, risk_score)\n",
        "      valid_indices <- c(valid_indices, i)\n",
        "    }\n",
        "  }\n",
        "  \n",
        "  if (length(risk_scores) < 2) {\n",
        "    return(0.5)\n",
        "  }\n",
        "  \n",
        "  valid_times <- times[valid_indices]\n",
        "  valid_events <- events[valid_indices]\n",
        "  \n",
        "  # Calculate concordance\n",
        "  concordant <- 0\n",
        "  total <- 0\n",
        "  \n",
        "  for (i in 1:length(valid_times)) {\n",
        "    for (j in (i+1):length(valid_times)) {\n",
        "      if (valid_times[i] < valid_times[j]) {\n",
        "        if (risk_scores[i] > risk_scores[j]) {\n",
        "          concordant <- concordant + 1\n",
        "        }\n",
        "        total <- total + 1\n",
        "      } else if (valid_times[j] < valid_times[i]) {\n",
        "        if (risk_scores[j] > risk_scores[i]) {\n",
        "          concordant <- concordant + 1\n",
        "        }\n",
        "        total <- total + 1\n",
        "      }\n",
        "    }\n",
        "  }\n",
        "  \n",
        "  return(ifelse(total > 0, concordant / total, 0.5))\n",
        "}\n",
        "\n",
        "calculate_brier_score <- function(pred_cif, times, events, risk_idx, time_points) {\n",
        "  # Calculate Brier score at specific time points\n",
        "  brier_scores <- numeric()\n",
        "  \n",
        "  for (t in time_points) {\n",
        "    # Find closest interval\n",
        "    t_idx <- findInterval(t, time_intervals[2:length(time_intervals)], rightmost.closed = TRUE) + 1\n",
        "    t_idx <- min(t_idx, dim(pred_cif)[3])\n",
        "    \n",
        "    # Get predicted CIF at time t\n",
        "    pred_cif_t <- pred_cif[, risk_idx, t_idx]\n",
        "    \n",
        "    # Get true outcomes (1 if event occurred before t, 0 otherwise)\n",
        "    true_outcomes <- as.numeric((events == risk_idx) & (y_test$time <= time_intervals[t_idx + 1]))\n",
        "    \n",
        "    # Calculate Brier score\n",
        "    brier <- mean((pred_cif_t - true_outcomes)^2)\n",
        "    brier_scores <- c(brier_scores, brier)\n",
        "  }\n",
        "  \n",
        "  return(brier_scores)\n",
        "}\n",
        "\n",
        "# Calculate metrics for each risk\n",
        "cat(\"Performance Metrics:\\n\")\n",
        "cat(paste(rep(\"=\", 50), collapse = \"\"), \"\\n\")\n",
        "\n",
        "for (risk_idx in 1:num_risks) {\n",
        "  c_index <- calculate_concordance_index(test_cif, test_times, test_events, risk_idx)\n",
        "  cat(sprintf(\"\\nRisk %d:\\n\", risk_idx))\n",
        "  cat(sprintf(\"  Concordance Index (C-index): %.4f\\n\", c_index))\n",
        "  \n",
        "  # Brier score at different time points\n",
        "  time_points <- seq(0, max_time, length.out = 10)\n",
        "  brier_scores <- calculate_brier_score(test_cif, test_times, test_events, risk_idx, time_points)\n",
        "  cat(sprintf(\"  Mean Brier Score: %.4f\\n\", mean(brier_scores)))\n",
        "  cat(sprintf(\"  Integrated Brier Score: %.4f\\n\", \n",
        "              integrate(function(x) approx(time_points, brier_scores, x)$y, \n",
        "                       0, max_time)$value / max_time))\n",
        "}\n",
        "\n",
        "# Overall metrics\n",
        "cat(\"\\n\", paste(rep(\"=\", 50), collapse = \"\"), \"\\n\")\n",
        "cat(\"Overall Model Performance:\\n\")\n",
        "cat(\"Test samples:\", length(test_times), \"\\n\")\n",
        "cat(\"Event rate:\", mean(test_events > 0) * 100, \"%\\n\")\n",
        "cat(\"Censoring rate:\", mean(test_events == 0) * 100, \"%\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "r"
        }
      },
      "outputs": [],
      "source": [
        "# Visualize Brier scores over time\n",
        "time_points <- seq(0, max_time, length.out = 20)\n",
        "\n",
        "brier_df <- data.frame()\n",
        "for (risk_idx in 1:num_risks) {\n",
        "  brier_scores <- calculate_brier_score(test_cif, test_times, test_events, risk_idx, time_points)\n",
        "  brier_df <- rbind(brier_df, data.frame(\n",
        "    time = time_points,\n",
        "    brier_score = brier_scores,\n",
        "    risk = paste0(\"Risk \", risk_idx)\n",
        "  ))\n",
        "}\n",
        "\n",
        "ggplot(brier_df, aes(x = time, y = brier_score, color = risk)) +\n",
        "  geom_line(linewidth = 1.5) +\n",
        "  geom_point(size = 2) +\n",
        "  facet_wrap(~risk, scales = \"free\") +\n",
        "  labs(x = \"Time\", y = \"Brier Score\",\n",
        "       title = \"Brier Score Over Time\") +\n",
        "  theme_minimal() +\n",
        "  theme(plot.title = element_text(size = 12, face = \"bold\"),\n",
        "        legend.position = \"none\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Risk Stratification\n",
        "\n",
        "Stratify patients into risk groups based on predicted cumulative incidence.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "r"
        }
      },
      "outputs": [],
      "source": [
        "# Calculate risk scores for each patient\n",
        "# Use maximum CIF value as risk score\n",
        "risk_scores_risk1 <- apply(test_cif[, 1, ], 1, max)\n",
        "risk_scores_risk2 <- apply(test_cif[, 2, ], 1, max)\n",
        "\n",
        "# Combine risk scores (weighted sum or max)\n",
        "combined_risk_scores <- (risk_scores_risk1 + risk_scores_risk2) / 2\n",
        "\n",
        "# Stratify into risk groups (tertiles)\n",
        "risk_groups <- cut(combined_risk_scores, \n",
        "                   breaks = quantile(combined_risk_scores, probs = c(0, 1/3, 2/3, 1)),\n",
        "                   labels = c(\"Low Risk\", \"Medium Risk\", \"High Risk\"),\n",
        "                   include.lowest = TRUE)\n",
        "\n",
        "# Add to test data\n",
        "y_test_stratified <- y_test\n",
        "y_test_stratified$risk_group <- risk_groups\n",
        "y_test_stratified$risk_score <- combined_risk_scores\n",
        "\n",
        "cat(\"Risk Stratification:\\n\")\n",
        "cat(paste(rep(\"=\", 50), collapse = \"\"), \"\\n\")\n",
        "print(table(y_test_stratified$risk_group))\n",
        "cat(\"\\nRisk Score Statistics by Group:\\n\")\n",
        "print(y_test_stratified %>% \n",
        "      group_by(risk_group) %>% \n",
        "      summarise(\n",
        "        mean = mean(risk_score),\n",
        "        sd = sd(risk_score),\n",
        "        min = min(risk_score),\n",
        "        max = max(risk_score),\n",
        "        .groups = \"drop\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "r"
        }
      },
      "outputs": [],
      "source": [
        "# Visualize risk stratification\n",
        "# Plot 1: Risk score distribution\n",
        "p1 <- ggplot(y_test_stratified, aes(x = risk_score, fill = risk_group)) +\n",
        "  geom_histogram(alpha = 0.6, bins = 20, color = \"black\") +\n",
        "  labs(x = \"Risk Score\", y = \"Frequency\",\n",
        "       title = \"Risk Score Distribution by Group\",\n",
        "       fill = \"Risk Group\") +\n",
        "  theme_minimal() +\n",
        "  theme(plot.title = element_text(size = 12, face = \"bold\"),\n",
        "        legend.position = \"bottom\")\n",
        "\n",
        "# Plot 2: Kaplan-Meier-like curves for each risk group\n",
        "survival_data <- data.frame()\n",
        "for (group in levels(risk_groups)) {\n",
        "  group_indices <- which(y_test_stratified$risk_group == group)\n",
        "  \n",
        "  # Calculate survival probability (1 - CIF)\n",
        "  group_cif <- apply(test_cif[group_indices, , ], c(2, 3), mean)  # Average over samples\n",
        "  \n",
        "  # Overall survival (1 - sum of all CIFs)\n",
        "  overall_survival <- 1 - colSums(group_cif)\n",
        "  \n",
        "  survival_data <- rbind(survival_data, data.frame(\n",
        "    time = time_intervals[2:length(time_intervals)],\n",
        "    survival = overall_survival,\n",
        "    group = group\n",
        "  ))\n",
        "}\n",
        "\n",
        "p2 <- ggplot(survival_data, aes(x = time, y = survival, color = group)) +\n",
        "  geom_line(linewidth = 1.5) +\n",
        "  labs(x = \"Time\", y = \"Survival Probability\",\n",
        "       title = \"Survival Curves by Risk Group\",\n",
        "       color = \"Risk Group\") +\n",
        "  ylim(0, 1) +\n",
        "  theme_minimal() +\n",
        "  theme(plot.title = element_text(size = 12, face = \"bold\"),\n",
        "        legend.position = \"bottom\")\n",
        "\n",
        "# Combine plots\n",
        "library(gridExtra)\n",
        "grid.arrange(p1, p2, ncol = 2)\n",
        "\n",
        "# Event rates by risk group\n",
        "cat(\"\\nEvent Rates by Risk Group:\\n\")\n",
        "cat(paste(rep(\"=\", 50), collapse = \"\"), \"\\n\")\n",
        "print(table(y_test_stratified$risk_group, y_test_stratified$event))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Feature Importance Analysis and Visualization\n",
        "\n",
        "Analyze which features are most important for predictions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "r"
        }
      },
      "outputs": [],
      "source": [
        "# Permutation-based feature importance\n",
        "calculate_permutation_importance <- function(model, X, times, events, n_repeats = 10) {\n",
        "  # Calculate feature importance using permutation\n",
        "  model$eval()\n",
        "  \n",
        "  # Baseline prediction\n",
        "  with_no_grad({\n",
        "    X_tensor <- torch_tensor(X, dtype = torch_float32())$to(device = device)\n",
        "    baseline_pred <- model(X_tensor)\n",
        "    baseline_pred_probs <- nnf_softmax(baseline_pred, dim = 3)\n",
        "    baseline_cif <- calculate_cif(as.array(baseline_pred_probs$cpu()))\n",
        "    baseline_loss_result <- deephit_loss(\n",
        "      baseline_pred,\n",
        "      torch_tensor(times, dtype = torch_long())$to(device = device),\n",
        "      torch_tensor(events, dtype = torch_long())$to(device = device)\n",
        "    )\n",
        "  })\n",
        "  \n",
        "  baseline_loss <- baseline_loss_result$total$item()\n",
        "  \n",
        "  # Permute each feature\n",
        "  n_features <- ncol(X)\n",
        "  importances <- numeric(n_features)\n",
        "  \n",
        "  for (feat_idx in 1:n_features) {\n",
        "    permuted_losses <- numeric(n_repeats)\n",
        "    \n",
        "    for (rep in 1:n_repeats) {\n",
        "      X_permuted <- X\n",
        "      X_permuted[, feat_idx] <- sample(X_permuted[, feat_idx])\n",
        "      \n",
        "      with_no_grad({\n",
        "        X_tensor <- torch_tensor(X_permuted, dtype = torch_float32())$to(device = device)\n",
        "        permuted_pred <- model(X_tensor)\n",
        "        permuted_loss_result <- deephit_loss(\n",
        "          permuted_pred,\n",
        "          torch_tensor(times, dtype = torch_long())$to(device = device),\n",
        "          torch_tensor(events, dtype = torch_long())$to(device = device)\n",
        "        )\n",
        "        permuted_losses[rep] <- permuted_loss_result$total$item()\n",
        "      })\n",
        "    }\n",
        "    \n",
        "    # Importance is increase in loss\n",
        "    importances[feat_idx] <- mean(permuted_losses) - baseline_loss\n",
        "  }\n",
        "  \n",
        "  return(importances)\n",
        "}\n",
        "\n",
        "# Calculate feature importance (use subset for speed)\n",
        "cat(\"Calculating feature importance (this may take a while)...\\n\")\n",
        "n_subset <- min(100, nrow(X_test_scaled))\n",
        "feature_importance <- calculate_permutation_importance(\n",
        "  model, \n",
        "  X_test_scaled[1:n_subset, ],\n",
        "  y_test_discrete[1:n_subset],\n",
        "  y_test$event[1:n_subset],\n",
        "  n_repeats = 5\n",
        ")\n",
        "\n",
        "# Create importance data frame\n",
        "importance_df <- data.frame(\n",
        "  feature = colnames(X_test),\n",
        "  importance = feature_importance\n",
        ") %>%\n",
        "  arrange(desc(importance))\n",
        "\n",
        "cat(\"\\nFeature Importance (Top 10):\\n\")\n",
        "cat(paste(rep(\"=\", 50), collapse = \"\"), \"\\n\")\n",
        "print(head(importance_df, 10))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "r"
        }
      },
      "outputs": [],
      "source": [
        "# Visualize feature importance\n",
        "top_features <- head(importance_df, 12)\n",
        "\n",
        "ggplot(top_features, aes(x = reorder(feature, importance), y = importance)) +\n",
        "  geom_bar(stat = \"identity\", fill = viridis(nrow(top_features)), color = \"black\") +\n",
        "  coord_flip() +\n",
        "  labs(x = \"Feature\", y = \"Importance (Increase in Loss)\",\n",
        "       title = \"Feature Importance Analysis (Permutation-based)\") +\n",
        "  theme_minimal() +\n",
        "  theme(plot.title = element_text(size = 14, face = \"bold\"))\n",
        "\n",
        "# Correlation between features and risk scores\n",
        "feature_risk_corr <- sapply(colnames(X_test), function(feat) {\n",
        "  abs(cor(X_test[[feat]], combined_risk_scores))\n",
        "})\n",
        "\n",
        "corr_df <- data.frame(\n",
        "  feature = names(feature_risk_corr),\n",
        "  correlation = feature_risk_corr\n",
        ") %>%\n",
        "  arrange(desc(correlation))\n",
        "\n",
        "top_corr <- head(corr_df, 12)\n",
        "\n",
        "ggplot(top_corr, aes(x = reorder(feature, correlation), y = correlation)) +\n",
        "  geom_bar(stat = \"identity\", fill = plasma(nrow(top_corr)), color = \"black\") +\n",
        "  coord_flip() +\n",
        "  labs(x = \"Feature\", y = \"Absolute Correlation with Risk Score\",\n",
        "       title = \"Feature-Risk Score Correlation\") +\n",
        "  theme_minimal() +\n",
        "  theme(plot.title = element_text(size = 14, face = \"bold\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 12. Model Interpretation with SHAP\n",
        "\n",
        "Use SHAP (SHapley Additive exPlanations) to interpret model predictions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "r"
        }
      },
      "outputs": [],
      "source": [
        "# Install SHAP for R if not already installed\n",
        "# install.packages(\"fastshap\")\n",
        "\n",
        "if (requireNamespace(\"fastshap\", quietly = TRUE)) {\n",
        "  library(fastshap)\n",
        "  \n",
        "  # Create a wrapper function for model predictions\n",
        "  model_predict_wrapper <- function(X) {\n",
        "    # Wrapper function for model predictions\n",
        "    model$eval()\n",
        "    with_no_grad({\n",
        "      X_tensor <- torch_tensor(X, dtype = torch_float32())$to(device = device)\n",
        "      pred <- model(X_tensor)\n",
        "      pred_probs <- nnf_softmax(pred, dim = 3)\n",
        "      # Return max CIF as risk score\n",
        "      pred_probs_array <- as.array(pred_probs$cpu())\n",
        "      cif <- calculate_cif(pred_probs_array)\n",
        "      return(apply(cif[, 1, ], 1, max))  # Risk 1, max CIF\n",
        "    })\n",
        "  }\n",
        "  \n",
        "  # Use subset of data for SHAP (computationally expensive)\n",
        "  X_shap <- X_test_scaled[1:20, ]\n",
        "  X_shap_background <- X_train_scaled[1:50, ]\n",
        "  \n",
        "  # Create SHAP explainer\n",
        "  cat(\"Creating SHAP explainer (this may take a while)...\\n\")\n",
        "  explainer <- fastshap::explain(\n",
        "    object = model_predict_wrapper,\n",
        "    X = X_shap,\n",
        "    nsim = 100,\n",
        "    pred_wrapper = model_predict_wrapper,\n",
        "    feature_names = colnames(X_test)\n",
        "  )\n",
        "  \n",
        "  cat(\"\\nSHAP analysis completed!\\n\")\n",
        "  cat(\"Note: For detailed SHAP visualizations, consider using the 'shapr' package\\n\")\n",
        "  cat(\"or exporting to Python for full SHAP library support.\\n\")\n",
        "  \n",
        "} else {\n",
        "  cat(\"SHAP not installed. Install with: install.packages('fastshap')\\n\")\n",
        "  cat(\"\\nFor now, showing alternative interpretation using gradient-based methods...\\n\")\n",
        "  \n",
        "  # Alternative: Gradient-based feature importance\n",
        "  model$eval()\n",
        "  X_sample <- torch_tensor(X_test_scaled[1:10, ], dtype = torch_float32(), requires_grad = TRUE)$to(device = device)\n",
        "  \n",
        "  pred <- model(X_sample)\n",
        "  # Use max CIF as output\n",
        "  pred_probs <- nnf_softmax(pred, dim = 3)\n",
        "  cif <- torch_cumsum(pred_probs, dim = 3)\n",
        "  output <- torch_max(cif, dim = 3)[[1]][, 1]  # Risk 1, max CIF\n",
        "  \n",
        "  output$sum()$backward()\n",
        "  \n",
        "  gradients <- torch_mean(torch_abs(X_sample$grad), dim = 1)$cpu()\n",
        "  gradients_array <- as.array(gradients)\n",
        "  \n",
        "  grad_df <- data.frame(\n",
        "    feature = colnames(X_test),\n",
        "    gradient_importance = gradients_array\n",
        "  ) %>%\n",
        "    arrange(desc(gradient_importance))\n",
        "  \n",
        "  top_grad <- head(grad_df, 12)\n",
        "  \n",
        "  ggplot(top_grad, aes(x = reorder(feature, gradient_importance), y = gradient_importance)) +\n",
        "    geom_bar(stat = \"identity\", fill = \"steelblue\", color = \"black\") +\n",
        "    coord_flip() +\n",
        "    labs(x = \"Feature\", y = \"Gradient-based Importance\",\n",
        "         title = \"Gradient-based Feature Importance (Alternative to SHAP)\") +\n",
        "    theme_minimal() +\n",
        "    theme(plot.title = element_text(size = 14, face = \"bold\"))\n",
        "  \n",
        "  cat(\"\\nGradient-based Feature Importance:\\n\")\n",
        "  print(head(grad_df, 10))\n",
        "}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 13. Summary and Conclusion\n",
        "\n",
        "### Key Takeaways:\n",
        "\n",
        "1. **DeepHit Advantages**:\n",
        "   - Handles competing risks naturally\n",
        "   - No proportional hazards assumption\n",
        "   - Can capture non-linear relationships\n",
        "   - Flexible architecture for complex survival data\n",
        "\n",
        "2. **Model Performance**:\n",
        "   - The model learns to predict cumulative incidence functions for each competing risk\n",
        "   - Performance can be evaluated using C-index, Brier scores, and other metrics\n",
        "   - Risk stratification helps identify high-risk patients\n",
        "\n",
        "3. **Interpretability**:\n",
        "   - Feature importance analysis reveals which covariates drive predictions\n",
        "   - SHAP values provide local explanations for individual predictions\n",
        "   - Risk stratification enables clinical decision-making\n",
        "\n",
        "4. **Best Practices**:\n",
        "   - Proper data preprocessing (standardization) is crucial\n",
        "   - Hyperparameter tuning (learning rate, architecture, dropout) improves performance\n",
        "   - Early stopping prevents overfitting\n",
        "   - Cross-validation helps assess model generalizability\n",
        "\n",
        "### Limitations and Future Directions:\n",
        "\n",
        "- DeepHit requires sufficient data for training deep networks\n",
        "- Hyperparameter selection can be time-consuming\n",
        "- Model interpretation, while possible, is more complex than linear models\n",
        "- Future work could explore attention mechanisms or transformer architectures\n",
        "\n",
        "### Conclusion:\n",
        "\n",
        "DeepHit represents a significant advancement in survival analysis, particularly for competing risks scenarios. Its ability to model complex relationships without restrictive assumptions makes it valuable for modern healthcare applications. However, careful validation and interpretation remain essential for clinical deployment.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 14. Resources\n",
        "\n",
        "### Papers and Publications:\n",
        "\n",
        "1. **DeepHit Paper**:\n",
        "   - Lee, C., et al. (2018). \"DeepHit: A Deep Learning Approach to Survival Analysis with Competing Risks.\" AAAI 2018.\n",
        "   - Link: http://medianetlab.ee.ucla.edu/papers/AAAI_2018_DeepHit\n",
        "\n",
        "### Code Repositories:\n",
        "\n",
        "1. **Official DeepHit Repository**:\n",
        "   - GitHub: https://github.com/chl8856/DeepHit\n",
        "   - Contains original implementation and sample data\n",
        "\n",
        "2. **PyCox Library (Python)**:\n",
        "   - PyPI: https://pypi.org/project/pycox/\n",
        "   - Provides PyTorch implementations of various survival models including DeepHit\n",
        "\n",
        "3. **torch for R**:\n",
        "   - CRAN: https://cran.r-project.org/package=torch\n",
        "   - R interface to PyTorch for deep learning\n",
        "\n",
        "### Datasets:\n",
        "\n",
        "1. **Synthetic Competing Risks Data**:\n",
        "   - Sample data from DeepHit repository\n",
        "   - Link: https://raw.githubusercontent.com/chl8856/DeepHit/refs/heads/master/sample%20data/SYNTHETIC/synthetic_comprisk.csv\n",
        "\n",
        "### Related Tools and Libraries:\n",
        "\n",
        "1. **survival (R)**: Survival analysis in R\n",
        "   - https://cran.r-project.org/package=survival\n",
        "\n",
        "2. **fastshap (R)**: Fast SHAP values for R\n",
        "   - https://cran.r-project.org/package=fastshap\n",
        "\n",
        "3. **torch (R)**: Deep learning framework for R\n",
        "   - https://torch.mlverse.org/\n",
        "\n",
        "### Additional Reading:\n",
        "\n",
        "1. Competing Risks Survival Analysis: Theory and Application\n",
        "2. Deep Learning for Survival Analysis: A Review\n",
        "3. Neural Networks for Survival Analysis\n",
        "\n",
        "---\n",
        "\n",
        "**Tutorial Created**: 2024\n",
        "**Author**: Survival Analysis Tutorial Series\n",
        "**License**: Educational Use\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
