[
  {
    "objectID": "02-07-01-02-survival-analysis-nonparametric-nelson-aalen-r.html#overview",
    "href": "02-07-01-02-survival-analysis-nonparametric-nelson-aalen-r.html#overview",
    "title": "1.2 The Nelson-Aalen Estimator",
    "section": "Overview",
    "text": "Overview\nSurvival analysis studies the time until an event of interest occurs, such as patient death or disease recurrence. A challenge is handling censoring, where the event is not observed for all subjects (e.g., patients still alive at study end). Non-parametric methods like the Kaplan-Meier estimator for survival probability and the Nelson-Aalen estimator for the cumulative hazard function are foundational tools.\nIn this tutorial, we’ll focus on the Nelson-Aalen estimator, which estimates the cumulative hazard function \\(H(t)\\), representing the accumulated risk up to time \\(t\\). The survival probability \\(S(t)\\) relates to it via \\(S(t) = \\exp(-H(t))\\). We’ll apply it to the lung dataset:\n\nDataset Overview: 228 observations from a lung cancer trial (1960s). Key variables:\n\ntime: Survival time in days.\nstatus: Event indicator (1 = death, 0 = censored).\nsex: Patient sex (1 = male, 2 = female).\nOther covariates: age, ph.ecog (performance status), etc.\n\n\nWe’ll compute estimates, plot curves, find medians, and compare male vs. female survival using the log-rank test.\nThe Nelson-Aalen estimator is a non-parametric estimate of the cumulative hazard function \\(H(t) = \\int_0^t h(u) \\, du\\), where \\(h(u)\\) is the hazard rate (instantaneous risk of event at time \\(u\\), given survival to \\(u\\).\nUnder the assumption of independent censoring, the estimator is:\n\\[\n\\hat{H}(t) = \\sum_{t_i \\leq t} \\frac{d_i}{n_i}\n\\]\n\n\\(t_i\\): Distinct event times (ordered).\n\\(d_i\\): Number of events (deaths) at \\(t_i\\).\n\\(n_i\\): Number of individuals at risk just before \\(t_i\\) (those who haven’t experienced the event or been censored yet).\n\nThis is a step function that jumps at each event time by \\(d_i / n_i\\). The variance estimate is $^2(t) = _{t_i t} $, useful for confidence intervals.\nUnlike the Kaplan-Meier (which estimates \\((t)\\) directly and handles ties differently), Nelson-Aalen focuses on the hazard and is more straightforward for cumulative risk. It’s implemented in R’s survival package via survfit().",
    "crumbs": [
      "**Non-Parametric Methods**",
      "Nelson-Aalen Estimator"
    ]
  },
  {
    "objectID": "02-07-01-02-survival-analysis-nonparametric-nelson-aalen-r.html#nelson-aalen-estimator-from-scratch",
    "href": "02-07-01-02-survival-analysis-nonparametric-nelson-aalen-r.html#nelson-aalen-estimator-from-scratch",
    "title": "1.2 The Nelson-Aalen Estimator",
    "section": "Nelson-Aalen estimator from Scratch",
    "text": "Nelson-Aalen estimator from Scratch\nIn this tutorial, we’ll implement the Nelson-Aalen Estimator from scratch in R using the lung dataset from the survival package. We’ll avoid directly using survfit(..., type = \"aalen\") for the core computation, instead manually calculating the cumulative hazard function $ (t) = _{t_i t} $. We’ll then derive the survival function, compute confidence intervals, estimate x-year survival probabilities, plot the survival curve, calculate the median survival time, and compare survival between groups (male vs. female) using the log-rank test. The survival package will be used for data handling and the log-rank test, but the Nelson-Aalen computation will be coded manually.\n\nData\n\n\nCode\nlibrary(survival)\n# Load the lung dataset\ndata(lung)\n# Create a data frame with time, status, and sex\ndata &lt;- data.frame(\n  time = lung$time,\n  status = lung$status,  # 1 = event (death), 0 = censored\n  sex = lung$sex         # 1 = male, 2 = female\n)\n\n\n\n\nFunction to compute Nelson-Aalen estimator from scratch\n\n\nCode\n# Function to compute Nelson-Aalen estimator from scratch\nnelson_aalen &lt;- function(time, status) {\n  # Ensure data is sorted by time\n  ord &lt;- order(time)\n  time &lt;- time[ord]\n  status &lt;- status[ord]\n  \n  # Identify unique event times (where status == 1)\n  event_times &lt;- unique(time[status == 1])\n  event_times &lt;- sort(event_times)\n  \n  # Initialize vectors\n  n_risk &lt;- numeric(length(event_times))  # Number at risk\n  n_event &lt;- numeric(length(event_times)) # Number of events\n  cumhaz &lt;- numeric(length(event_times))  # Cumulative hazard\n  variance &lt;- numeric(length(event_times))# Variance of cumhaz\n  \n  # Compute n_i (at risk) and d_i (events) at each event time\n  for (i in seq_along(event_times)) {\n    t_i &lt;- event_times[i]\n    n_risk[i] &lt;- sum(time &gt;= t_i)  # Subjects still at risk\n    n_event[i] &lt;- sum(time == t_i & status == 1)  # Events at t_i\n    cumhaz[i] &lt;- ifelse(i == 1, 0, cumhaz[i-1]) + n_event[i] / n_risk[i]\n    variance[i] &lt;- ifelse(i == 1, 0, variance[i-1]) + n_event[i] / (n_risk[i] * (n_risk[i] - n_event[i]))\n  }\n  \n  return(list(\n    time = event_times,\n    n.risk = n_risk,\n    n.event = n_event,\n    cumhaz = cumhaz,\n    variance = variance\n  ))\n}\n\n\n\n\nCompute Nelson-Aalen for the entire dataset\n\n\nCode\n# Compute Nelson-Aalen for the entire dataset\nna_fit &lt;- nelson_aalen(data$time, data$status)\nsummary(na_fit)\n\n\n         Length Class  Mode   \ntime     60     -none- numeric\nn.risk   60     -none- numeric\nn.event  60     -none- numeric\ncumhaz   60     -none- numeric\nvariance 60     -none- numeric\n\n\n\n\nDerive survival function S(t) = exp(-H(t))\n\n\nCode\n# Derive survival function S(t) = exp(-H(t))\nsurvival &lt;- exp(-na_fit$cumhaz)\n\n# Confidence intervals (95%) for cumulative hazard\nz &lt;- qnorm(0.975)  # Z-score for 95% CI\nlower_haz &lt;- na_fit$cumhaz - z * sqrt(na_fit$variance)\nupper_haz &lt;- na_fit$cumhaz + z * sqrt(na_fit$variance)\nlower_surv &lt;- exp(-upper_haz)  # Survival CI: exp(-upper_haz) is lower bound\nupper_surv &lt;- exp(-lower_haz)  # exp(-lower_haz) is upper bound\n\n# Estimating x-Year Survival\n# For 1-year (365 days) and 5-year (1825 days) survival\nt_1yr &lt;- 365\nt_5yr &lt;- 1825\n\n# Find closest time points\nidx_1yr &lt;- max(which(na_fit$time &lt;= t_1yr))\nidx_5yr &lt;- max(which(na_fit$time &lt;= t_5yr))\n\ns_1yr &lt;- survival[idx_1yr]\ns_5yr &lt;- survival[idx_5yr]\n\ncat(\"1-Year Survival Probability:\", round(s_1yr, 3), \"\\n\")\n\n\n1-Year Survival Probability: 0.699 \n\n\nCode\ncat(\"5-Year Survival Probability:\", round(s_5yr, 3), \"\\n\")\n\n\n5-Year Survival Probability: 0.044 \n\n\n\n\nPlot Survival Curve\n\n\nCode\n# Plot Survival Curve\nplot(na_fit$time, survival, type = \"s\", col = \"red\", lwd = 2,\n     xlab = \"Time (days)\", ylab = \"Survival Probability S(t)\",\n     main = \"Nelson-Aalen Survival Curve (Lung Dataset)\")\nlines(na_fit$time, lower_surv, col = \"red\", lty = 2)\nlines(na_fit$time, upper_surv, col = \"red\", lty = 2)\nlegend(\"topright\", c(\"S(t)\", \"95% CI\"), col = \"red\", lty = c(1, 2), lwd = c(2, 1))\n\n\n\n\n\n\n\n\n\n\n\nMedian Survival Time\n\n\nCode\n# Median Survival Time\n# Find time where S(t) &lt;= 0.5\nmedian_idx &lt;- min(which(survival &lt;= 0.5))\nmedian_time &lt;- na_fit$time[median_idx]\ncat(\"Median Survival Time:\", median_time, \"days\\n\")\n\n\nMedian Survival Time: 588 days\n\n\n\n\nSurvival Times Between Groups (Male vs. Female)\n\n\nCode\n# Survival Times Between Groups (Male vs. Female)\n# Subset data by sex\ndata_male &lt;- subset(data, sex == 1)\ndata_female &lt;- subset(data, sex == 2)\n\n# Compute Nelson-Aalen for each group\nna_male &lt;- nelson_aalen(data_male$time, data_male$status)\nna_female &lt;- nelson_aalen(data_female$time, data_female$status)\n\n# Derive survival curves\nsurv_male &lt;- exp(-na_male$cumhaz)\nsurv_female &lt;- exp(-na_female$cumhaz)\n\n\n\n\nCode\n# Survival Times Between Groups (Male vs. Female)\n# Plot survival curves by group\nplot(na_male$time, surv_male, type = \"s\", col = \"blue\", lwd = 2,\n     xlab = \"Time (days)\", ylab = \"Survival Probability\",\n     main = \"Survival Curves by Sex (Male: Blue, Female: Red)\")\nlines(na_female$time, surv_female, col = \"red\", lwd = 2)\nlegend(\"topright\", c(\"Male\", \"Female\"), col = c(\"blue\", \"red\"), lwd = 2)\n\n\n\n\n\n\n\n\n\n\n\nLog-rank test for group comparison\n\n\nCode\n# Log-rank test for group comparison\nsurv_obj &lt;- Surv(time = data$time, event = data$status)\nlogrank_test &lt;- survdiff(surv_obj ~ data$sex)\nprint(logrank_test)\n\n\nCall:\nsurvdiff(formula = surv_obj ~ data$sex)\n\n             N Observed Expected (O-E)^2/E (O-E)^2/V\ndata$sex=1 138      112     91.6      4.55      10.3\ndata$sex=2  90       53     73.4      5.68      10.3\n\n Chisq= 10.3  on 1 degrees of freedom, p= 0.001 \n\n\n\n\nCode\n# remove all objects\nrm(list=ls())",
    "crumbs": [
      "**Non-Parametric Methods**",
      "Nelson-Aalen Estimator"
    ]
  },
  {
    "objectID": "02-07-01-02-survival-analysis-nonparametric-nelson-aalen-r.html#nelson-aalen-estimato-in-r",
    "href": "02-07-01-02-survival-analysis-nonparametric-nelson-aalen-r.html#nelson-aalen-estimato-in-r",
    "title": "1.2 The Nelson-Aalen Estimator",
    "section": "Nelson-Aalen estimato in R",
    "text": "Nelson-Aalen estimato in R\nIn this section, we will demonstrate how to compute the Nelson-Aalen estimator using R’s survival package. We’ll cover loading the data, calculating the estimator, estimating x-year survival probabilities, plotting survival curves, calculating median survival time, and comparing survival between groups using the log-rank test.\n\nInstall Required R Packages\nFollowing R packages are required to run this notebook. If any of these packages are not installed, you can install them using the code below:\n\n\nCode\npackages &lt;-c(\n         'tidyverse',\n         'gtsummary',\n         'survival',\n         'survminer',\n         'ggsurvfit',\n         'tidycmprsk',\n         'ggfortify',\n         'timereg',\n         'cmprsk',\n         'riskRegression'\n         )\n\n\n#| warning: false\n#| error: false\n\n# Install missing packages\nnew_packages &lt;- packages[!(packages %in% installed.packages()[,\"Package\"])]\nif(length(new_packages)) install.packages(new_packages)\n devtools::install_github(\"ItziarI/WeDiBaDis\")\n\n\nCode\n# Verify installation\ncat(\"Installed packages:\\n\")\n\n\nInstalled packages:\n\n\nCode\nprint(sapply(packages, requireNamespace, quietly = TRUE))\n\n\nRegistered S3 method overwritten by 'riskRegression':\n  method        from \n  nobs.multinom broom\n\n\n     tidyverse      gtsummary       survival      survminer      ggsurvfit \n          TRUE           TRUE           TRUE           TRUE           TRUE \n    tidycmprsk      ggfortify        timereg         cmprsk riskRegression \n          TRUE           TRUE           TRUE           TRUE           TRUE \n\n\n\n\nLoad Packages\n\n\nCode\n# Load packages with suppressed messages\ninvisible(lapply(packages, function(pkg) {\n  suppressPackageStartupMessages(library(pkg, character.only = TRUE))\n}))\n\n\n\n\nCode\n# Check loaded packages\ncat(\"Successfully loaded packages:\\n\")\n\n\nSuccessfully loaded packages:\n\n\nCode\nprint(search()[grepl(\"package:\", search())])\n\n\n [1] \"package:riskRegression\" \"package:cmprsk\"         \"package:timereg\"       \n [4] \"package:ggfortify\"      \"package:tidycmprsk\"     \"package:ggsurvfit\"     \n [7] \"package:survminer\"      \"package:ggpubr\"         \"package:gtsummary\"     \n[10] \"package:lubridate\"      \"package:forcats\"        \"package:stringr\"       \n[13] \"package:dplyr\"          \"package:purrr\"          \"package:readr\"         \n[16] \"package:tidyr\"          \"package:tibble\"         \"package:ggplot2\"       \n[19] \"package:tidyverse\"      \"package:survival\"       \"package:stats\"         \n[22] \"package:graphics\"       \"package:grDevices\"      \"package:utils\"         \n[25] \"package:datasets\"       \"package:methods\"        \"package:base\"          \n\n\n\n\nData\nFirst, load the necessary package and dataset. We’ll inspect the data and create a survival object.\n\n\nCode\n# Load the lung dataset\ndata(lung)\n# Create a data frame with relevant variables\ndata &lt;- data.frame(\n  time = lung$time,\n  status = lung$status,  # 1 = event (death), 0 = censored\n  sex = lung$sex         # 1 = male, 2 = female\n)\n\n\n\n\nCtrate a survival object\n\n\nCode\n# Create a survival object\nsurv_obj &lt;- Surv(time = data$time, event = data$status)\n\n\n\n\nCalculating the Nelson-Aalen Estimator\n\n\nCode\n# Calculating the Nelson-Aalen Estimator\n# Use survfit to compute the cumulative hazard (Nelson-Aalen estimator)\nfit_na &lt;- survfit(surv_obj ~ 1, type = \"fh\")  # 'fh' specifies Fleming-Harrington (Nelson-Aalen)\n\n\n\n\nExtract key components\n\n\nCode\nna_summary &lt;- summary(fit_na)\ntimes &lt;- na_summary$time\nn_risk &lt;- na_summary$n.risk\nn_event &lt;- na_summary$n.event\ncumhaz &lt;- na_summary$cumhaz\nstd_err &lt;- na_summary$std.chaz  # Standard error of cumhaz\n\n# Derive survival function S(t) = exp(-H(t))\nsurvival &lt;- exp(-cumhaz)\n\n# Compute 95% confidence intervals for cumulative hazard and survival\nz &lt;- qnorm(0.975)  # Z-score for 95% CI\nlower_haz &lt;- cumhaz - z * std_err\nupper_haz &lt;- cumhaz + z * std_err\nlower_surv &lt;- exp(-upper_haz)  # Lower survival bound\nupper_surv &lt;- exp(-lower_haz)  # Upper survival bound\n\n# Print a few rows of the results\ncat(\"Sample of Nelson-Aalen Estimates:\\n\")\n\n\nSample of Nelson-Aalen Estimates:\n\n\nCode\nprint(data.frame(Time = times[1:5], N_Risk = n_risk[1:5], N_Event = n_event[1:5], CumHaz = cumhaz[1:5]))\n\n\n  Time N_Risk N_Event      CumHaz\n1    5    228       1 0.004385965\n2   11    227       3 0.017660474\n3   12    224       1 0.022124760\n4   13    223       2 0.031113570\n5   15    221       1 0.035638456\n\n\n\n\nEstimating x-Year Survival\nThe x-year survival probability is ( (t) = (-(t)) ). For 1-year (365 days) and 5-year (1825 days) survival:\n\n\nCode\n# Extract cumhaz at specific times\nt_1yr &lt;- 365\nt_5yr &lt;- 1825\n\n# Find the closest time points and compute S(t)\ncumhaz_1yr &lt;- approx(fit_na$time, fit_na$cumhaz, xout = t_1yr, method = \"constant\", rule = 2)$y\ncumhaz_5yr &lt;- approx(fit_na$time, fit_na$cumhaz, xout = t_5yr, method = \"constant\", rule = 2)$y\n\ns_1yr &lt;- exp(-cumhaz_1yr)\ns_5yr &lt;- exp(-cumhaz_5yr)\n\ncat(\"1-Year Survival Probability:\", round(s_1yr, 3), \"\\n\")\n\n\n1-Year Survival Probability: 0.411 \n\n\nCode\ncat(\"5-Year Survival Probability:\", round(s_5yr, 3), \"\\n\")\n\n\n5-Year Survival Probability: 0.055 \n\n\n\n\nPlot Survival Curve\nPlot the cumulative hazard directly or derive the survival curve.\n\n\nCode\n# Plot cumulative hazard (Nelson-Aalen)\nplot(fit_na, fun = \"cumhaz\", xlab = \"Time (days)\", ylab = \"Cumulative Hazard H(t)\",\n     main = \"Nelson-Aalen Cumulative Hazard Estimate\", col = \"blue\", lwd = 2)\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Alternatively, plot survival curve (derived from Nelson-Aalen)\nplot(fit_na, xlab = \"Time (days)\", ylab = \"Survival Probability S(t)\",\n     main = \"Survival Curve (from Nelson-Aalen)\", col = \"red\", lwd = 2)\n\n\n\n\n\n\n\n\n\nThe cumulative hazard plot shows a rising step function, while the survival plot decreases toward 0.\n\n\nMedian Survival Time\nThe median is the time where \\(\\hat{S}(t) = 0.5\\), or \\(\\hat{H}(t) = \\ln(2) \\approx 0.693\\).\n\n\nCode\n# Median from survival curve\nmedian_time &lt;- median(fit_na)\ncat(\"Median Survival Time:\", median_time, \"days\\n\")\n\n\nMedian Survival Time: 310 days\n\n\nCode\n# Or manually: find time where cumhaz &gt;= log(2)\nlog2 &lt;- log(2)\nmedian_idx &lt;- min(which(fit_na$cumhaz &gt;= log2))\nmedian_manual &lt;- fit_na$time[median_idx]\ncat(\"Manual Median (days):\", median_manual, \"\\n\")\n\n\nManual Median (days): 310 \n\n\n\n\nSurvival Times Between Groups (Male/Female) Using the Log-Rank Test\nCompare groups by sex. Fit stratified models and use survdiff() for the log-rank test (tests if survival curves differ).\n\n\nCode\n# Fit Nelson-Aalen by sex\nfit_by_sex &lt;- survfit(surv_obj ~ sex, data = data)\n# Summary of\nsummary(fit_by_sex)\n\n\nCall: survfit(formula = surv_obj ~ sex, data = data)\n\n                sex=1 \n time n.risk n.event survival std.err lower 95% CI upper 95% CI\n   11    138       3   0.9783  0.0124       0.9542        1.000\n   12    135       1   0.9710  0.0143       0.9434        0.999\n   13    134       2   0.9565  0.0174       0.9231        0.991\n   15    132       1   0.9493  0.0187       0.9134        0.987\n   26    131       1   0.9420  0.0199       0.9038        0.982\n   30    130       1   0.9348  0.0210       0.8945        0.977\n   31    129       1   0.9275  0.0221       0.8853        0.972\n   53    128       2   0.9130  0.0240       0.8672        0.961\n   54    126       1   0.9058  0.0249       0.8583        0.956\n   59    125       1   0.8986  0.0257       0.8496        0.950\n   60    124       1   0.8913  0.0265       0.8409        0.945\n   65    123       2   0.8768  0.0280       0.8237        0.933\n   71    121       1   0.8696  0.0287       0.8152        0.928\n   81    120       1   0.8623  0.0293       0.8067        0.922\n   88    119       2   0.8478  0.0306       0.7900        0.910\n   92    117       1   0.8406  0.0312       0.7817        0.904\n   93    116       1   0.8333  0.0317       0.7734        0.898\n   95    115       1   0.8261  0.0323       0.7652        0.892\n  105    114       1   0.8188  0.0328       0.7570        0.886\n  107    113       1   0.8116  0.0333       0.7489        0.880\n  110    112       1   0.8043  0.0338       0.7408        0.873\n  116    111       1   0.7971  0.0342       0.7328        0.867\n  118    110       1   0.7899  0.0347       0.7247        0.861\n  131    109       1   0.7826  0.0351       0.7167        0.855\n  132    108       2   0.7681  0.0359       0.7008        0.842\n  135    106       1   0.7609  0.0363       0.6929        0.835\n  142    105       1   0.7536  0.0367       0.6851        0.829\n  144    104       1   0.7464  0.0370       0.6772        0.823\n  147    103       1   0.7391  0.0374       0.6694        0.816\n  156    102       2   0.7246  0.0380       0.6538        0.803\n  163    100       3   0.7029  0.0389       0.6306        0.783\n  166     97       1   0.6957  0.0392       0.6230        0.777\n  170     96       1   0.6884  0.0394       0.6153        0.770\n  175     94       1   0.6811  0.0397       0.6076        0.763\n  176     93       1   0.6738  0.0399       0.5999        0.757\n  177     92       1   0.6664  0.0402       0.5922        0.750\n  179     91       2   0.6518  0.0406       0.5769        0.736\n  180     89       1   0.6445  0.0408       0.5693        0.730\n  181     88       2   0.6298  0.0412       0.5541        0.716\n  183     86       1   0.6225  0.0413       0.5466        0.709\n  189     83       1   0.6150  0.0415       0.5388        0.702\n  197     80       1   0.6073  0.0417       0.5309        0.695\n  202     78       1   0.5995  0.0419       0.5228        0.687\n  207     77       1   0.5917  0.0420       0.5148        0.680\n  210     76       1   0.5839  0.0422       0.5068        0.673\n  212     75       1   0.5762  0.0424       0.4988        0.665\n  218     74       1   0.5684  0.0425       0.4909        0.658\n  222     72       1   0.5605  0.0426       0.4829        0.651\n  223     70       1   0.5525  0.0428       0.4747        0.643\n  229     67       1   0.5442  0.0429       0.4663        0.635\n  230     66       1   0.5360  0.0431       0.4579        0.627\n  239     64       1   0.5276  0.0432       0.4494        0.619\n  246     63       1   0.5192  0.0433       0.4409        0.611\n  267     61       1   0.5107  0.0434       0.4323        0.603\n  269     60       1   0.5022  0.0435       0.4238        0.595\n  270     59       1   0.4937  0.0436       0.4152        0.587\n  283     57       1   0.4850  0.0437       0.4065        0.579\n  284     56       1   0.4764  0.0438       0.3979        0.570\n  285     54       1   0.4676  0.0438       0.3891        0.562\n  286     53       1   0.4587  0.0439       0.3803        0.553\n  288     52       1   0.4499  0.0439       0.3716        0.545\n  291     51       1   0.4411  0.0439       0.3629        0.536\n  301     48       1   0.4319  0.0440       0.3538        0.527\n  303     46       1   0.4225  0.0440       0.3445        0.518\n  306     44       1   0.4129  0.0440       0.3350        0.509\n  310     43       1   0.4033  0.0441       0.3256        0.500\n  320     42       1   0.3937  0.0440       0.3162        0.490\n  329     41       1   0.3841  0.0440       0.3069        0.481\n  337     40       1   0.3745  0.0439       0.2976        0.471\n  353     39       2   0.3553  0.0437       0.2791        0.452\n  363     37       1   0.3457  0.0436       0.2700        0.443\n  364     36       1   0.3361  0.0434       0.2609        0.433\n  371     35       1   0.3265  0.0432       0.2519        0.423\n  387     34       1   0.3169  0.0430       0.2429        0.413\n  390     33       1   0.3073  0.0428       0.2339        0.404\n  394     32       1   0.2977  0.0425       0.2250        0.394\n  428     29       1   0.2874  0.0423       0.2155        0.383\n  429     28       1   0.2771  0.0420       0.2060        0.373\n  442     27       1   0.2669  0.0417       0.1965        0.362\n  455     25       1   0.2562  0.0413       0.1868        0.351\n  457     24       1   0.2455  0.0410       0.1770        0.341\n  460     22       1   0.2344  0.0406       0.1669        0.329\n  477     21       1   0.2232  0.0402       0.1569        0.318\n  519     20       1   0.2121  0.0397       0.1469        0.306\n  524     19       1   0.2009  0.0391       0.1371        0.294\n  533     18       1   0.1897  0.0385       0.1275        0.282\n  558     17       1   0.1786  0.0378       0.1179        0.270\n  567     16       1   0.1674  0.0371       0.1085        0.258\n  574     15       1   0.1562  0.0362       0.0992        0.246\n  583     14       1   0.1451  0.0353       0.0900        0.234\n  613     13       1   0.1339  0.0343       0.0810        0.221\n  624     12       1   0.1228  0.0332       0.0722        0.209\n  643     11       1   0.1116  0.0320       0.0636        0.196\n  655     10       1   0.1004  0.0307       0.0552        0.183\n  689      9       1   0.0893  0.0293       0.0470        0.170\n  707      8       1   0.0781  0.0276       0.0390        0.156\n  791      7       1   0.0670  0.0259       0.0314        0.143\n  814      5       1   0.0536  0.0239       0.0223        0.128\n  883      3       1   0.0357  0.0216       0.0109        0.117\n\n                sex=2 \n time n.risk n.event survival std.err lower 95% CI upper 95% CI\n    5     90       1   0.9889  0.0110       0.9675        1.000\n   60     89       1   0.9778  0.0155       0.9478        1.000\n   61     88       1   0.9667  0.0189       0.9303        1.000\n   62     87       1   0.9556  0.0217       0.9139        0.999\n   79     86       1   0.9444  0.0241       0.8983        0.993\n   81     85       1   0.9333  0.0263       0.8832        0.986\n   95     83       1   0.9221  0.0283       0.8683        0.979\n  107     81       1   0.9107  0.0301       0.8535        0.972\n  122     80       1   0.8993  0.0318       0.8390        0.964\n  145     79       2   0.8766  0.0349       0.8108        0.948\n  153     77       1   0.8652  0.0362       0.7970        0.939\n  166     76       1   0.8538  0.0375       0.7834        0.931\n  167     75       1   0.8424  0.0387       0.7699        0.922\n  182     71       1   0.8305  0.0399       0.7559        0.913\n  186     70       1   0.8187  0.0411       0.7420        0.903\n  194     68       1   0.8066  0.0422       0.7280        0.894\n  199     67       1   0.7946  0.0432       0.7142        0.884\n  201     66       2   0.7705  0.0452       0.6869        0.864\n  208     62       1   0.7581  0.0461       0.6729        0.854\n  226     59       1   0.7452  0.0471       0.6584        0.843\n  239     57       1   0.7322  0.0480       0.6438        0.833\n  245     54       1   0.7186  0.0490       0.6287        0.821\n  268     51       1   0.7045  0.0501       0.6129        0.810\n  285     47       1   0.6895  0.0512       0.5962        0.798\n  293     45       1   0.6742  0.0523       0.5791        0.785\n  305     43       1   0.6585  0.0534       0.5618        0.772\n  310     42       1   0.6428  0.0544       0.5447        0.759\n  340     39       1   0.6264  0.0554       0.5267        0.745\n  345     38       1   0.6099  0.0563       0.5089        0.731\n  348     37       1   0.5934  0.0572       0.4913        0.717\n  350     36       1   0.5769  0.0579       0.4739        0.702\n  351     35       1   0.5604  0.0586       0.4566        0.688\n  361     33       1   0.5434  0.0592       0.4390        0.673\n  363     32       1   0.5265  0.0597       0.4215        0.658\n  371     30       1   0.5089  0.0603       0.4035        0.642\n  426     26       1   0.4893  0.0610       0.3832        0.625\n  433     25       1   0.4698  0.0617       0.3632        0.608\n  444     24       1   0.4502  0.0621       0.3435        0.590\n  450     23       1   0.4306  0.0624       0.3241        0.572\n  473     22       1   0.4110  0.0626       0.3050        0.554\n  520     19       1   0.3894  0.0629       0.2837        0.534\n  524     18       1   0.3678  0.0630       0.2628        0.515\n  550     15       1   0.3433  0.0634       0.2390        0.493\n  641     11       1   0.3121  0.0649       0.2076        0.469\n  654     10       1   0.2808  0.0655       0.1778        0.443\n  687      9       1   0.2496  0.0652       0.1496        0.417\n  705      8       1   0.2184  0.0641       0.1229        0.388\n  728      7       1   0.1872  0.0621       0.0978        0.359\n  731      6       1   0.1560  0.0590       0.0743        0.328\n  735      5       1   0.1248  0.0549       0.0527        0.295\n  765      3       1   0.0832  0.0499       0.0257        0.270\n\n\n\n\nCode\n# Plot survival curves by group\nplot(fit_by_sex, col = c(\"blue\", \"red\"), lwd = 2, \n     xlab = \"Time (days)\", ylab = \"Survival Probability\",\n     main = \"Survival Curves by Sex (Male: Blue, Female: Red)\")\n\nlegend(\"topright\", c(\"Male\", \"Female\"), col = c(\"blue\", \"red\"), lwd = 2)\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Log-rank test\nlogrank_test &lt;- survdiff(surv_obj ~ sex, data = lung)\nprint(logrank_test)\n\n\nCall:\nsurvdiff(formula = surv_obj ~ sex, data = lung)\n\n        N Observed Expected (O-E)^2/E (O-E)^2/V\nsex=1 138      112     91.6      4.55      10.3\nsex=2  90       53     73.4      5.68      10.3\n\n Chisq= 10.3  on 1 degrees of freedom, p= 0.001 \n\n\nCode\n# Medians by group\nsummary(fit_by_sex)$table  # Includes medians\n\n\n      records n.max n.start events    rmean se(rmean) median 0.95LCL 0.95UCL\nsex=1     138   138     138    112 326.0841  22.91156    270     212     310\nsex=2      90    90      90     53 460.6473  34.68985    426     348     550\n\n\nFemales show better survival (higher p-value indicates difference). Medians: Males ~252 days, Females ~426 days.",
    "crumbs": [
      "**Non-Parametric Methods**",
      "Nelson-Aalen Estimator"
    ]
  },
  {
    "objectID": "02-07-01-02-survival-analysis-nonparametric-nelson-aalen-r.html#summary-and-conclusion",
    "href": "02-07-01-02-survival-analysis-nonparametric-nelson-aalen-r.html#summary-and-conclusion",
    "title": "1.2 The Nelson-Aalen Estimator",
    "section": "Summary and Conclusion",
    "text": "Summary and Conclusion\nIn this tutorial, we explored the Nelson-Aalen estimator using the lung dataset. We covered its theory as a cumulative hazard estimator, data preparation with Surv(), computation via survfit(), x-year survival estimates, plotting, median calculation, and group comparisons with the log-rank test. Key takeaways:\n\nNelson-Aalen is robust and non-parametric, ideal for exploratory analysis.\nIt complements Kaplan-Meier by focusing on hazard accumulation.\nIn the lung data, overall median survival is ~310 days, with females faring better (log-rank p=0.0065).\n\nThis method is foundational for more advanced models like Cox regression. Practice with your own data to build intuition.",
    "crumbs": [
      "**Non-Parametric Methods**",
      "Nelson-Aalen Estimator"
    ]
  },
  {
    "objectID": "02-07-01-02-survival-analysis-nonparametric-nelson-aalen-r.html#resources",
    "href": "02-07-01-02-survival-analysis-nonparametric-nelson-aalen-r.html#resources",
    "title": "1.2 The Nelson-Aalen Estimator",
    "section": "Resources",
    "text": "Resources\n\nBooks:\n\nKlein, J. P., & Moeschberger, M. L. (2003). Survival Analysis: Techniques for Censored and Truncated Data. Springer.\nTherneau, T. M., & Grambsch, P. M. (2000). Modeling Survival Data: Extending the Cox Model. Springer.\n\nOnline:\n\nR survival package vignette: vignette(\"survival\").\nUCLA IDRE Tutorial: https://stats.idre.ucla.edu/r/dae/non-parametric-survival-analysis-using-r/.\nDatacamp: “Survival Analysis in R” course.",
    "crumbs": [
      "**Non-Parametric Methods**",
      "Nelson-Aalen Estimator"
    ]
  },
  {
    "objectID": "02-07-01-00-survival-analysis-non-parametric-introduction-r.html#overview",
    "href": "02-07-01-00-survival-analysis-non-parametric-introduction-r.html#overview",
    "title": "1. Nonparametric Survival Analysis",
    "section": "Overview",
    "text": "Overview\nNonparametric survival analysis refers to techniques used to analyze and estimate survival data without making assumptions about the underlying distribution of survival times. These methods are especially useful when you don’t know the form of the hazard function or the distribution of survival times. The main non-parametric methods include the Kaplan-Meier estimator and the log-rank test.\nNon-parametric Methods of Survival Analysis\nSurvival analysis is a branch of statistics used to analyze the expected duration of time until one or more events happen — such as death in biological organisms or failure in mechanical systems. Non-parametric methods in survival analysis make no assumptions about the underlying probability distribution of survival times. They are flexible, robust, and widely used for exploratory data analysis and when the shape of the survival distribution is unknown.\nThese methods estimate the survival function (S(t) = P(T &gt; t), the probability of surviving beyond time t) directly from the observed data, without assuming a specific parametric form (e.g., exponential, Weibull).",
    "crumbs": [
      "**Non-Parametric Methods**",
      "Introduction to Non-Parametric Survival Models"
    ]
  },
  {
    "objectID": "02-07-01-00-survival-analysis-non-parametric-introduction-r.html#key-features-of-non-parametric-methods",
    "href": "02-07-01-00-survival-analysis-non-parametric-introduction-r.html#key-features-of-non-parametric-methods",
    "title": "1. Nonparametric Survival Analysis",
    "section": "Key Features of Non-parametric Methods:",
    "text": "Key Features of Non-parametric Methods:\n\nNo assumption about the functional form of the survival or hazard function.\nUse actual observed event and censoring times.\nProvide step-function estimates of survival probabilities.\nIdeal for small to moderate sample sizes or when distributional assumptions are questionable.",
    "crumbs": [
      "**Non-Parametric Methods**",
      "Introduction to Non-Parametric Survival Models"
    ]
  },
  {
    "objectID": "02-07-01-00-survival-analysis-non-parametric-introduction-r.html#major-non-parametric-methods-in-survival-analysis",
    "href": "02-07-01-00-survival-analysis-non-parametric-introduction-r.html#major-non-parametric-methods-in-survival-analysis",
    "title": "1. Nonparametric Survival Analysis",
    "section": "Major Non-parametric Methods in Survival Analysis:",
    "text": "Major Non-parametric Methods in Survival Analysis:\n\nKaplan-Meier Estimator (Product-Limit Estimator)\n\nMost widely used non-parametric method.\nEstimates the survival function from lifetime data, handling right-censored observations.\nProduces a step function that changes value only at the time of each event.\nFormula: \\[\n\\hat{S}(t) = \\prod_{t_i \\leq t} \\left(1 - \\frac{d_i}{n_i}\\right)\n\\] where:\n\n\\(t_i\\) = distinct event time\n\\(d_i\\) = number of events at time \\(t_i\\)\n\\(n_i\\) = number at risk just before time \\(t_i\\)\n\nAdvantages:\n\nHandles censored data naturally.\nEasy to compute and interpret.\nCan be plotted for visual comparison between groups.\n\nLimitations:\n\nBecomes unstable with small sample sizes at later time points.\nNot smooth — stepwise function.\n\n\n\n\nNelson-Aalen Estimator\n\nEstimates the cumulative hazard function \\(H(t) = \\int_0^t h(u) du\\).\nAlso handles censored data.\nFormula: \\[\n\\hat{H}(t) = \\sum_{t_i \\leq t} \\frac{d_i}{n_i}\n\\] (same notation as Kaplan-Meier)\nThe survival function can be derived from it: \\[\n\\hat{S}(t) = \\exp(-\\hat{H}(t))\n\\] — this is called the Breslow estimator or exponential of Nelson-Aalen.\nUse case: When interest is in hazard rates or when comparing hazard functions.\nAdvantages:\n\nMore stable than Kaplan-Meier for estimating hazard.\nUseful for model diagnostics (e.g., checking proportional hazards).\n\n\n\n\nLife Table (Actuarial) Method\n\nOlder method, used when data are grouped into intervals (e.g., yearly, monthly).\nCommon in demography, insurance, and actuarial science.\nFor each interval, it calculates:\n\nNumber entering the interval\nNumber censored in the interval\nEffective number at risk\nConditional probability of surviving the interval\nCumulative survival probability\n\nAssumes censoring occurs uniformly within intervals (or at midpoint).\nFor interval \\(t_i, t_{i+1})\\): \\[\n\\hat{S}(t_{i+1}) = \\hat{S}(t_i) \\times \\left(1 - \\frac{d_i}{n'_i}\\right)\n\\] where \\(n'_i\\) = adjusted number at risk (accounting for censoring).\nAdvantages:\n\nGood for large datasets with grouped times.\nEasy to communicate to non-statisticians.\n\nDisadvantages:\n\nLoss of information due to grouping.\nLess precise than Kaplan-Meier when exact event times are known.",
    "crumbs": [
      "**Non-Parametric Methods**",
      "Introduction to Non-Parametric Survival Models"
    ]
  },
  {
    "objectID": "02-07-01-00-survival-analysis-non-parametric-introduction-r.html#comparison-summary",
    "href": "02-07-01-00-survival-analysis-non-parametric-introduction-r.html#comparison-summary",
    "title": "1. Nonparametric Survival Analysis",
    "section": "Comparison Summary:",
    "text": "Comparison Summary:\n\n\n\n\n\n\n\n\n\n\nMethod\nEstimates\nHandles Censoring?\nData Format\nUse Case\n\n\n\n\nKaplan-Meier\nSurvival Function\nYes\nExact event times\nMost common; group comparisons\n\n\nNelson-Aalen\nCumulative Hazard\nYes\nExact event times\nHazard analysis, model checking\n\n\nLife Table\nSurvival Function\nYes (approximate)\nGrouped intervals\nActuarial, demographic studies",
    "crumbs": [
      "**Non-Parametric Methods**",
      "Introduction to Non-Parametric Survival Models"
    ]
  },
  {
    "objectID": "02-07-01-00-survival-analysis-non-parametric-introduction-r.html#visualization-inference",
    "href": "02-07-01-00-survival-analysis-non-parametric-introduction-r.html#visualization-inference",
    "title": "1. Nonparametric Survival Analysis",
    "section": "Visualization & Inference",
    "text": "Visualization & Inference\n\nKaplan-Meier curves are typically plotted to visualize survival over time.\nLog-rank test (non-parametric) is often used to compare survival distributions between two or more groups using Kaplan-Meier estimates.\nConfidence intervals for survival estimates can be calculated using Greenwood’s formula (for KM) or other variance estimators.",
    "crumbs": [
      "**Non-Parametric Methods**",
      "Introduction to Non-Parametric Survival Models"
    ]
  },
  {
    "objectID": "02-07-01-00-survival-analysis-non-parametric-introduction-r.html#when-to-use-non-parametric-methods",
    "href": "02-07-01-00-survival-analysis-non-parametric-introduction-r.html#when-to-use-non-parametric-methods",
    "title": "1. Nonparametric Survival Analysis",
    "section": "When to Use Non-parametric Methods?",
    "text": "When to Use Non-parametric Methods?\n\nInitial exploratory analysis.\nWhen parametric assumptions are not met or unknown.\nSmall sample sizes.\nComparing survival between groups without modeling covariates.\nPresenting results visually to non-technical audiences.",
    "crumbs": [
      "**Non-Parametric Methods**",
      "Introduction to Non-Parametric Survival Models"
    ]
  },
  {
    "objectID": "02-07-01-00-survival-analysis-non-parametric-introduction-r.html#limitations",
    "href": "02-07-01-00-survival-analysis-non-parametric-introduction-r.html#limitations",
    "title": "1. Nonparametric Survival Analysis",
    "section": "️ Limitations",
    "text": "️ Limitations\n\nCannot adjust for covariates (unlike Cox regression).\nDo not provide smooth functions — may be jagged or unstable with sparse data.\nNot suitable for extrapolation beyond observed time range.",
    "crumbs": [
      "**Non-Parametric Methods**",
      "Introduction to Non-Parametric Survival Models"
    ]
  },
  {
    "objectID": "02-07-01-00-survival-analysis-non-parametric-introduction-r.html#summary-and-conclusion",
    "href": "02-07-01-00-survival-analysis-non-parametric-introduction-r.html#summary-and-conclusion",
    "title": "1. Nonparametric Survival Analysis",
    "section": "Summary and Conclusion",
    "text": "Summary and Conclusion\nNon-parametric survival methods — especially Kaplan-Meier — are foundational tools in survival analysis. They offer intuitive, assumption-free estimates of survival probabilities and form the basis for hypothesis testing (e.g., log-rank test) and more advanced modeling (e.g., semi-parametric Cox models). Understanding these methods is essential for any survival data analysis. The next sections will demonstrate how to implement these methods in R, both manually and using the {survival} package.",
    "crumbs": [
      "**Non-Parametric Methods**",
      "Introduction to Non-Parametric Survival Models"
    ]
  },
  {
    "objectID": "02-07-01-01-survival-analysis-nonparametric-kaplan-meier-r.html#overview",
    "href": "02-07-01-01-survival-analysis-nonparametric-kaplan-meier-r.html#overview",
    "title": "1.1 Kaplan-Meier Estimator",
    "section": "Overview",
    "text": "Overview\nThe Kaplan-Meier estimator is a non-parametric statistic used to estimate the survival function from time-to-event data. It is particularly valuable when some data points are censored, meaning the event of interest (e.g., death, equipment failure) has not occurred for some subjects at the end of the study period.\nThe survival function \\(S(t)\\) estimates the probability that an individual survives beyond time \\(t\\). The Kaplan-Meier estimator provides a step function that drops at each event time (e.g., death, failure). The survival probability at a given time is calculated as:\n\\[ \\hat{S}(t) = \\prod_{t_i \\leq t} \\left( \\frac{n_i - d_i}{n_i} \\right) \\]\nWhere: - \\(t_i\\) is the time of the \\(i\\)-th event (e.g., death). - \\(n_i\\) is the number of subjects still at risk just before time \\(t_i\\). - \\(d_i\\) is the number of events (e.g., deaths) at time \\(t_i\\). - The product of the ratios \\(\\frac{n_i - d_i}{n_i}\\) across all event times up to \\(t\\) gives the estimated survival probability at time ( t ).\n\nFeatures of Kaplan-Meier Estimation\n\nNon-parametric: The Kaplan-Meier estimator does not assume any specific distribution of the survival times.\nStepwise Survival Curve: The survival curve is a step function that drops at each event time and stays flat between events.\nCensoring: The Kaplan-Meier method can handle right-censored data, where we do not observe the event of interest for some subjects by the time the study ends. Censoring is denoted by ticks on the survival curve, showing the points where individuals were lost to follow-up or their event had not occurred by the study’s end.\n\n\n\nKaplan-Meier Survival Curve Interpretation\n\nThe curve starts at 1 (100% survival probability) and decreases over time as events occur. At each event time (e.g., death or failure), the curve steps down.\nThe Y-axis shows the estimated survival probability, and the X-axis represents time.\nThe rate of decline in the curve reflects the frequency of events (e.g., if there is a rapid decline, the event is occurring more frequently).\nCensored data points are marked with tick marks or other indicators along the curve. These represent individuals who did not experience the event during the study but were lost to follow-up or had not yet experienced the event at the end of the study.",
    "crumbs": [
      "**Non-Parametric Methods**",
      "Kaplan-Meier Estimator"
    ]
  },
  {
    "objectID": "02-07-01-01-survival-analysis-nonparametric-kaplan-meier-r.html#kaplan-meier-esimator-from-scratch",
    "href": "02-07-01-01-survival-analysis-nonparametric-kaplan-meier-r.html#kaplan-meier-esimator-from-scratch",
    "title": "1.1 Kaplan-Meier Estimator",
    "section": "Kaplan-Meier Esimator from Scratch",
    "text": "Kaplan-Meier Esimator from Scratch\nThis is a manual calculation of the Kaplan-Meier estimator, without using any external R packages. The steps involve create a dataset, determining the number of individuals at risk, calculating the survival probability at each event time, and then plotting the results. You can compare these manually calculated results with those obtained from {estimation} analysis packages to verify accuracy. This exercise help you to understand theory behind Kaplan-Meier estimation.\nSteps:\n\nCreate a dataset: We’ll created a simulated dataset similar to the lung dataset in the {survival} package.\nSort Data by Time: Organize the dataset based on event times.\nCalculate Risk Set and Survival Probabilities: For each unique event time, manually calculate the Kaplan-Meier survival probabilities.\nPlot the Kaplan-Meier Survival Curve.\n\n\nCreate a dataset\nTo create a dataset similar to the lung dataset in the {survival} package, we’ll simulate survival data with random values for survival time, event status, and other variables such as age, sex, and treatment. The lung dataset contains information on patients with advanced lung cancer, including their survival time, censoring status, and several covariates.\nVariables typically included in a survival dataset like lung:\n\ntime: Survival time (numeric).\nstatus: Censoring indicator (0 = censored, 1 = event).\nage: Age of the patient (numeric).\nsex: Gender of the patient (1 = male, 2 = female).\nph.ecog: ECOG performance score (0 to 5).\ntreatment: Treatment group (1 = standard, 2 = experimental).\n\n\n\nCode\n# Set the seed for reproducibility\nset.seed(123)\n\n# Number of observations\nn &lt;- 228  # same as the original lung dataset\n\n# Simulate survival time (in days)\ntime &lt;- round(runif(n, min = 1, max = 1000))  # random survival time between 1 and 1000 days\n\n# Simulate censoring indicator (0 = censored, 1 = event)\nstatus &lt;- rbinom(n, size = 1, prob = 0.7)  # 70% of events, 30% censored\n\n# Simulate age (between 40 and 80 years)\nage &lt;- round(runif(n, min = 40, max = 80))\n\n# Simulate sex (1 = male, 2 = female)\nsex &lt;- sample(c(1, 2), size = n, replace = TRUE)\n\n# Simulate ECOG performance score (0 to 4)\nph_ecog &lt;- sample(0:4, size = n, replace = TRUE)\n\n# Simulate treatment group (1 = standard, 2 = experimental)\ntreatment &lt;- sample(c(1, 2), size = n, replace = TRUE)\n\n# Create the data frame\nsimulated_lung&lt;- data.frame(time = time,\n                             status = status,\n                             age = age,\n                             sex = sex,\n                             ph_ecog = ph_ecog,\n                             treatment = treatment)\n\n# Inspect the first few rows of the simulated dataset\nhead(simulated_lung)\n\n\n  time status age sex ph_ecog treatment\n1  288      1  72   1       4         2\n2  789      0  70   2       0         2\n3  410      1  46   2       2         2\n4  883      1  45   1       1         2\n5  941      1  79   2       2         2\n6   47      1  57   1       3         2\n\n\n\n\nSort data by time and event status\nWe need to sort the dataset by survival time\n\n\nCode\n# Sort data by 'time'\nsimulated_lung &lt;- simulated_lung[order(simulated_lung$time), ]\nhead(simulated_lung)\n\n\n    time status age sex ph_ecog treatment\n74     2      1  52   1       1         1\n143   11      1  76   2       0         1\n35    26      1  79   1       2         1\n18    43      0  70   1       4         1\n6     47      1  57   1       3         2\n51    47      1  61   1       1         1\n\n\n\n\nCalculate Kaplan-Meier estimates\nTo calculate the Kaplan-Meier estimates manually, you can follow these steps:\n\nRisk Set: At each unique event time, count how many individuals are still at risk.\nEvent Occurrence: At each unique event time, count how many events (deaths) occur.\nSurvival Probability: Compute the survival probability at each step using the formula:\n\n\\[  S(t_i) = S(t_{i-1}) \\times \\left(1 - \\frac{d_i}{n_i}\\right) \\]\nwhere: - \\(d\\) is the number of events at time \\(t_i\\), - \\(n_i\\) is the number of individuals at risk at time \\(i\\).\n\n\nCode\n# Initialize survival probability\nn &lt;- nrow(simulated_lung)  # total number of individuals\nS_t &lt;- 1              # start with a survival probability of 1 (i.e., 100% survival at t = 0)\nkm_table &lt;- data.frame(time = numeric(0), n_risk = numeric(0), events = numeric(0), survival = numeric(0))\n\n# Loop through each unique event time\nunique_times &lt;- unique(simulated_lung$time[simulated_lung$status == 1])  # only times where events occurred\n\nfor (t in unique_times) {\n  # Calculate the number of individuals at risk at this time point\n  n_risk &lt;- sum(simulated_lung$time &gt;= t)\n  \n  # Calculate the number of events at this time point\n  d_i &lt;- sum(simulated_lung$time == t & simulated_lung$status == 1)\n  \n  # Update the survival probability\n  S_t &lt;- S_t * (1 - d_i / n_risk)\n  \n  # Add this to the Kaplan-Meier table\n  km_table &lt;- rbind(km_table, data.frame(time = t, n_risk = n_risk, events = d_i, survival = S_t))\n}\n\n# Print the resulting Kaplan-Meier estimates\nhead(km_table)\n\n\n  time n_risk events  survival\n1    2    228      1 0.9956140\n2   11    227      1 0.9912281\n3   26    226      1 0.9868421\n4   47    224      2 0.9780310\n5   54    221      1 0.9736055\n6   62    220      1 0.9691801\n\n\n\n\nPlot the Kaplan-Meier survival curve\nOnce you have manually computed the Kaplan-Meier estimates, you can plot the survival curve using base R plotting functions:\n\n\nCode\n# Plot the survival curve\nplot(km_table$time, km_table$survival, type = \"s\", col = \"blue\", lwd = 2,\n     xlab = \"Time (days)\", ylab = \"Survival Probability\",\n     main = \"Manual Kaplan-Meier Estimate - Lung Dataset\")",
    "crumbs": [
      "**Non-Parametric Methods**",
      "Kaplan-Meier Estimator"
    ]
  },
  {
    "objectID": "02-07-01-01-survival-analysis-nonparametric-kaplan-meier-r.html#kaplan-meier-estimator-in-r",
    "href": "02-07-01-01-survival-analysis-nonparametric-kaplan-meier-r.html#kaplan-meier-estimator-in-r",
    "title": "1.1 Kaplan-Meier Estimator",
    "section": "Kaplan-Meier Estimator in R",
    "text": "Kaplan-Meier Estimator in R\nThis tutorial introduces survival analysis and how to conduct it in R. It primarily follows the Survival Analysis in R tutorials by Emily C. Zabor and Joseph Rickert’s Survival Analysis in R tutorials.\nThis tutorial is mostly used two R packages {survival} and {ggsurvfit}. The survival package in R is a powerful and widely used tool for survival analysis, which deals with the analysis of time-to-event data. It provides a suite of functions to handle various types of survival data, fit survival models, and visualize survival curves. This package is widely applied in medical research, reliability engineering, and many other fields where time-to-event data is of interest.\n\nThe {ggsurvfit} package eases the creation of time-to-event (aka survival) summary figures with {ggplot2}. The concise and modular code creates images that are ready for publication or sharing. Competing risks cumulative incidence is also supported via ggcuminc().\n\nAdditionally we will use {ggfortify} which offers fortify and autoplot functions to allow automatic ggplot2 to visualize Kaplan-Meier plots.\n\nInstall Required R Packages\nFollowing R packages are required to run this notebook. If any of these packages are not installed, you can install them using the code below:\n\n\nCode\npackages &lt;-c(\n         'tidyverse',\n         'report',\n         'performance',\n         'gtsummary',\n         'MASS',\n         'epiDisplay',\n         'survival',\n         'survminer',\n         'ggsurvfit',\n         'tidycmprsk',\n         'ggfortify',\n         'timereg',\n         'cmprsk',\n         'riskRegression'\n         )\n\n\n#| warning: false\n#| error: false\n\n# Install missing packages\nnew_packages &lt;- packages[!(packages %in% installed.packages()[,\"Package\"])]\nif(length(new_packages)) install.packages(new_packages)\n devtools::install_github(\"ItziarI/WeDiBaDis\")\n\n# Verify installation\ncat(\"Installed packages:\\n\")\nprint(sapply(packages, requireNamespace, quietly = TRUE))\n\n\nLoad Packages\n\n\nCode\n# Load packages with suppressed messages\ninvisible(lapply(packages, function(pkg) {\n  suppressPackageStartupMessages(library(pkg, character.only = TRUE))\n}))\n\n\n\n\nCode\n# Check loaded packages\ncat(\"Successfully loaded packages:\\n\")\n\n\nSuccessfully loaded packages:\n\n\nCode\nprint(search()[grepl(\"package:\", search())])\n\n\n [1] \"package:riskRegression\" \"package:cmprsk\"         \"package:timereg\"       \n [4] \"package:ggfortify\"      \"package:tidycmprsk\"     \"package:ggsurvfit\"     \n [7] \"package:survminer\"      \"package:ggpubr\"         \"package:epiDisplay\"    \n[10] \"package:nnet\"           \"package:survival\"       \"package:foreign\"       \n[13] \"package:MASS\"           \"package:gtsummary\"      \"package:performance\"   \n[16] \"package:report\"         \"package:lubridate\"      \"package:forcats\"       \n[19] \"package:stringr\"        \"package:dplyr\"          \"package:purrr\"         \n[22] \"package:readr\"          \"package:tidyr\"          \"package:tibble\"        \n[25] \"package:ggplot2\"        \"package:tidyverse\"      \"package:stats\"         \n[28] \"package:graphics\"       \"package:grDevices\"      \"package:utils\"         \n[31] \"package:datasets\"       \"package:methods\"        \"package:base\"          \n\n\n\n\nData\nWe will be utilizing the lung dataset from the {survival} package, which serves as a valuable resource for analyzing survival data. This dataset comprises information from subjects diagnosed with advanced lung cancer, specifically gathered from the North Central Cancer Treatment Group, a prominent clinical trial network dedicated to cancer research.\nThroughout this tutorial, we will concentrate on the following key variables that provide insight into the patients’ demographics and clinical outcomes:\n\nTime: This variable denotes the observed survival time in days, measuring the duration from the start of treatment until the event of interest occurs, whether that be death or censoring.\nStatus: This variable represents the censoring status of each patient. A value of 1 indicates that the patient is censored, meaning they either withdrew from the study or were still alive at the end of the observation period. A value of 2 signifies that the patient has died, marking the occurrence of the event being studied.\nSex: This variable captures the gender of the subjects, providing crucial demographic information. A value of 1 corresponds to Male and a value of 2 corresponds to Female. This distinction may be important for understanding potential differences in survival rates between genders.\n\n\n\nCode\n# Load lung cancer dataset\ndata(lung)\nglimpse(lung)\n\n\nRows: 228\nColumns: 10\n$ inst      &lt;dbl&gt; 3, 3, 3, 5, 1, 12, 7, 11, 1, 7, 6, 16, 11, 21, 12, 1, 22, 16…\n$ time      &lt;dbl&gt; 306, 455, 1010, 210, 883, 1022, 310, 361, 218, 166, 170, 654…\n$ status    &lt;dbl&gt; 2, 2, 1, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, …\n$ age       &lt;dbl&gt; 74, 68, 56, 57, 60, 74, 68, 71, 53, 61, 57, 68, 68, 60, 57, …\n$ sex       &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 2, 1, …\n$ ph.ecog   &lt;dbl&gt; 1, 0, 0, 1, 0, 1, 2, 2, 1, 2, 1, 2, 1, NA, 1, 1, 1, 2, 2, 1,…\n$ ph.karno  &lt;dbl&gt; 90, 90, 90, 90, 100, 50, 70, 60, 70, 70, 80, 70, 90, 60, 80,…\n$ pat.karno &lt;dbl&gt; 100, 90, 90, 60, 90, 80, 60, 80, 80, 70, 80, 70, 90, 70, 70,…\n$ meal.cal  &lt;dbl&gt; 1175, 1225, NA, 1150, NA, 513, 384, 538, 825, 271, 1025, NA,…\n$ wt.loss   &lt;dbl&gt; NA, 15, 15, 11, 0, 0, 10, 1, 16, 34, 27, 23, 5, 32, 60, 15, …\n\n\n\n\nData processing\nNow we will re-code the data as 1=event, 0=censored:\n\n\nCode\nlung &lt;- \n  lung |&gt; \n  mutate(\n    status = recode(status, `1` = 0, `2` = 1)\n  )\n\n\nNow we have:\n\ntime: Observed survival time in days\nstatus: censoring status 0=censored, 1=dead\nsex: 1=Male, 2=Female\n\n\n\nCode\nhead(lung[, c(\"time\", \"status\", \"sex\")])\n\n\n  time status sex\n1  306      1   1\n2  455      1   1\n3 1010      0   1\n4  210      1   1\n5  883      1   1\n6 1022      0   1\n\n\n\n\nKaplan Meier Analysis\nThe Kaplan-Meier method is the most common technique for estimating survival times and probabilities. It is a non-parametric approach that produces a step function, with a drop occurring each time an event takes place.\nThe {survival} package is fundamental to the entire R survival analysis framework. It is not only feature-rich but also essential for creating the object produced by the Surv() function, which includes data on failure times and censoring. Each subject will have a single entry representing their survival time, followed by a “+” sign if the subject was censored. Let’s examine the first 15 observations:\n\n\nCode\nSurv(lung$time, lung$status)[1:15]\n\n\n [1]  306   455  1010+  210   883  1022+  310   361   218   166   170   654 \n[13]  728    71   567 \n\n\nSubject 1 experienced a notable event at 306 days of observation, indicating a significant outcome in their progression. Subject 2 had a similar event occur later, at 455 days, suggesting varying timelines in their responses. Meanwhile, subject 3 was censored at 1010 days, meaning that their outcome could not be fully assessed due to the study’s conclusion or loss to follow-up. This variation among subjects highlights the differences in individual experiences and response times within the study.\n\n\n\n\n\n\nNote\n\n\n\nthe Surv() function in the {survival} package accepts by default TRUE/FALSE, where TRUE is event and FALSE is censored; 1/0 where 1 is event and 0 is censored; or 2/1 where 2 is event and 1 is censored.\n\n\nThe survfit() function plays a crucial role in survival analysis by generating survival curves based on the Kaplan-Meier method. This method is particularly useful for estimating the probability of survival over a specified time period, especially when dealing with censored data, which is common in medical research.\nTo initiate our analysis, we start with the formula Surv(futime, status) ~ 1, where time represents the follow-up time until the event of interest or censoring occurs, and status indicates whether the event happened (usually coded as 1) or if the data is censored (coded as 0). The survfit() function utilizes this formula to calculate the Kaplan-Meier estimates, providing a stepwise approximation of the survival function. Additionally, the summary() function complements this analysis by offering a detailed summary of the survival estimates. Within this function, the times parameter allows researchers to specify particular time points at which they wish to view the survival probabilities. This feature is beneficial for focusing on time intervals of interest, enhancing the interpretability of the survival curves. Overall, these tools are essential for understanding patient survival trajectories and can inform clinical decision-making and research developments.\n\n\nCode\nkm.fit&lt;- survfit(Surv(time, status) ~ 1, data = lung)\nstr(km.fit)\n\n\nList of 17\n $ n        : int 228\n $ time     : num [1:186] 5 11 12 13 15 26 30 31 53 54 ...\n $ n.risk   : num [1:186] 228 227 224 223 221 220 219 218 217 215 ...\n $ n.event  : num [1:186] 1 3 1 2 1 1 1 1 2 1 ...\n $ n.censor : num [1:186] 0 0 0 0 0 0 0 0 0 0 ...\n $ surv     : num [1:186] 0.996 0.982 0.978 0.969 0.965 ...\n $ std.err  : num [1:186] 0.0044 0.00885 0.00992 0.01179 0.01263 ...\n $ cumhaz   : num [1:186] 0.00439 0.0176 0.02207 0.03103 0.03556 ...\n $ std.chaz : num [1:186] 0.00439 0.0088 0.00987 0.01173 0.01257 ...\n $ type     : chr \"right\"\n $ logse    : logi TRUE\n $ conf.int : num 0.95\n $ conf.type: chr \"log\"\n $ lower    : num [1:186] 0.987 0.966 0.959 0.947 0.941 ...\n $ upper    : num [1:186] 1 1 0.997 0.992 0.989 ...\n $ t0       : num 0\n $ call     : language survfit(formula = Surv(time, status) ~ 1, data = lung)\n - attr(*, \"class\")= chr \"survfit\"\n\n\nThis model is designed to generate estimates for 1, 30, 60, and 90 days, and then every 90 days thereafter. It is the simplest possible model, requiring just three lines of R code to fit and produce both numerical and graphical summaries.\n\n\nCode\nsummary(km.fit, times = c(1,30,60,90*(1:10)))\n\n\nCall: survfit(formula = Surv(time, status) ~ 1, data = lung)\n\n time n.risk n.event survival std.err lower 95% CI upper 95% CI\n    1    228       0   1.0000  0.0000       1.0000        1.000\n   30    219      10   0.9561  0.0136       0.9299        0.983\n   60    213       7   0.9254  0.0174       0.8920        0.960\n   90    201      10   0.8816  0.0214       0.8406        0.925\n  180    160      36   0.7217  0.0298       0.6655        0.783\n  270    108      30   0.5753  0.0338       0.5128        0.645\n  360     70      24   0.4340  0.0358       0.3693        0.510\n  450     48      16   0.3287  0.0356       0.2659        0.406\n  540     33      10   0.2554  0.0344       0.1962        0.333\n  630     22       7   0.1958  0.0330       0.1407        0.272\n  720     14       8   0.1246  0.0290       0.0789        0.197\n  810      7       5   0.0783  0.0246       0.0423        0.145\n  900      3       2   0.0503  0.0228       0.0207        0.123\n\n\n\n\nKaplan-Meier plots\nWe will use the {ggsurvfit} package to generate Kaplan-Meier plots. This package simplifies the plotting of time-to-event endpoints by leveraging the capabilities of the {ggplot2} package. The {ggsurvfit} package works best when you create the survival object using the included ggsurvfit::survfit2() function. This function uses the same syntax as survival::survfit(), but ggsurvfit::survfit2() tracks the environment from the function call, which helps the plot have better default values for labeling and p-value reporting.\n\n\nCode\nsurvfit2(Surv(time, status) ~ 1, data = lung) |&gt; \n  ggsurvfit() +\n  labs(\n    x = \"Days\",\n    y = \"Overall survival probability\"\n  )\n\n\n\n\n\n\n\n\n\nThe default plot in ggsurvfit() shows the step function only. We can add the confidence interval using add_confidence_interval():\n\n\nCode\nsurvfit2(Surv(time, status) ~ 1, data = lung) |&gt; \n  ggsurvfit() +\n  labs(\n    x = \"Days\",\n    y = \"Overall survival probability\"\n  ) +\n add_confidence_interval()\n\n\n\n\n\n\n\n\n\nIf we want to display the numbers at risk below the x-axis, we can achieve this by using add_risktable() :\n\n\nCode\nsurvfit2(Surv(time, status) ~ 1, data = lung)  |&gt;  \n  ggsurvfit() +\n  labs(\n    x = \"Days\",\n    y = \"Overall survival probability\"\n  ) +\n add_confidence_interval() +\n    add_risktable() \n\n\n\n\n\n\n\n\n\nAlternatively, the ggfortify package allows you to create a simple survival plot with CI using the autoplot() function.\n\n\nCode\nautoplot(km.fit)\n\n\nWarning: `aes_string()` was deprecated in ggplot2 3.0.0.\nℹ Please use tidy evaluation idioms with `aes()`.\nℹ See also `vignette(\"ggplot2-in-packages\")` for more information.\nℹ The deprecated feature was likely used in the ggfortify package.\n  Please report the issue at &lt;https://github.com/sinhrks/ggfortify/issues&gt;.\n\n\n\n\n\n\n\n\n\n\n\nEstimating \\(x\\)-year survival\nIn survival analysis, one key point of interest is the probability of surviving beyond a specific number of years. For instance, to estimate the probability of surviving up to one year, you can use the summary function with the times argument. Note that the time variable in the lung dataset is measured in days, so you should set times to 365.25.\n\n\nCode\nsummary(survfit(Surv(time, status) ~ 1, data = lung), times = 365.25)\n\n\nCall: survfit(formula = Surv(time, status) ~ 1, data = lung)\n\n time n.risk n.event survival std.err lower 95% CI upper 95% CI\n  365     65     121    0.409  0.0358        0.345        0.486\n\n\n\n\nCode\nsurvfit2(Surv(time, status) ~ 1, data = lung)  |&gt;  \n  ggsurvfit() +\n  labs(\n    x = \"Days\",\n    y = \"Overall survival probability\"\n  ) +\n add_confidence_interval() + \n   add_risktable() +\n  geom_segment(x = 365.25, xend = 365.25, y = -0.05, yend = 0.4092416, \n               linewidth = 1.0) +\n  geom_segment(x = 365.25, xend = -40, y = 0.4092416, yend = 0.4092416,\n               linewidth = 1.0, \n               arrow = arrow(length = unit(0.2, \"inches\"))) \n\n\n\n\n\n\n\n\n\nBy one year, 121 of the 228 lung patients had died, and if we ignore censoring, the one year probability of survival:\n\n\nCode\n(1-121/228)*100\n\n\n[1] 46.92982\n\n\nIgnoring the fact that 42 patients were censored before the one-year mark can lead to an inaccurate estimate of the one-year probability of survival (47%). The correct estimate, which takes censoring into account using the Kaplan-Meier method, is 41%. Ignoring censoring results in an overestimation of the overall survival probability. Consider two studies, each with 228 subjects, where both have 165 deaths. In one study (represented by the blue line), censoring is ignored, while in the other (shown by the yellow line), censoring is taken into account. Censored subjects only provide information for part of the follow-up period and then drop out of the risk set, which leads to a lower cumulative probability of survival. Ignoring censoring incorrectly assumes that censored patients remain part of the risk set for the entire follow-up period.\n\n\nCode\nlung2 &lt;- \n  lung |&gt; \n  mutate(\n    time = ifelse(status == 1, time, 1022), \n    group = \"Ignoring censoring\") |&gt;  \n  full_join(mutate(lung, group = \"With censoring\"))\n\nsurvfit2(Surv(time, status) ~ group, data = lung2) |&gt; \n  ggsurvfit() +\n  labs(\n    x = \"Days\",\n    y = \"Overall survival probability\"\n    ) + \n  add_confidence_interval() +\n  add_risktable(risktable_stats = \"n.risk\")\n\n\n\n\n\n\n\n\n\nWe can produce nice tables of \\(x\\)-time survival probability estimates using the tbl_survfit() function from the {gtsummary} package:\n\n\nCode\ntbl_survfit(\n  survfit(Surv(time, status) ~ 1, data = lung),\n  times = 365.25,\n  label_header = \"**1-year survival (95% CI)**\"\n)\n\n\n\n\n\n\n\n\nCharacteristic\n1-year survival (95% CI)\n\n\n\n\nOverall\n41% (34%, 49%)\n\n\n\n\n\n\n\n\n\nMedian survival time\nIn survival analysis, one important point to consider is the average survival time, which we typically measure using the median. Since survival times are usually not normally distributed, using the mean is not an appropriate way to summarize this data.\nThe median survival time can be directly obtained from the survfit() object.\n\n\nCode\nsurvfit(Surv(time, status) ~ 1, data = lung)\n\n\nCall: survfit(formula = Surv(time, status) ~ 1, data = lung)\n\n       n events median 0.95LCL 0.95UCL\n[1,] 228    165    310     285     363\n\n\nWe can create appealing tables displaying the median survival time estimates using the tbl_survfit() function from the {gtsummary} package.\n\n\nCode\nsurvfit(Surv(time, status) ~ 1, data = lung) %&gt;% \n  tbl_survfit(\n    probs = 0.5,\n    label_header = \"**Median survival (95% CI)**\"\n  )\n\n\n\n\n\n\n\n\nCharacteristic\nMedian survival (95% CI)\n\n\n\n\nOverall\n310 (285, 363)\n\n\n\n\n\n\n\n\n\nSurvival times between groups (male/female) using log-rank test\nThe log-rank test is a non-parametric statistical method used to compare the survival distributions of two or more groups. It assesses whether there is a significant difference between the survival curves by giving equal weight to observations throughout the entire follow-up period.\nTo calculate the log-rank p-value, we can use the survdiff() function. For example, we can test for differences in survival time based on sex using the lung dataset.\n\n\nCode\nsurvdiff(Surv(time, status) ~ sex, data = lung)\n\n\nCall:\nsurvdiff(formula = Surv(time, status) ~ sex, data = lung)\n\n        N Observed Expected (O-E)^2/E (O-E)^2/V\nsex=1 138      112     91.6      4.55      10.3\nsex=2  90       53     73.4      5.68      10.3\n\n Chisq= 10.3  on 1 degrees of freedom, p= 0.001 \n\n\nThere is a significant difference in overall survival by sex in the lung data, with a p-value of 0.001.\nNext, we look at survival curves by sex.\n\n\nCode\nsurvfit2(Surv(time, status) ~ sex, data = lung) |&gt; \n  ggsurvfit() +\n  labs(\n    x = \"Days\",\n    y = \"Overall survival probability\"\n  ) +\n  add_confidence_interval() +\n  add_risktable()",
    "crumbs": [
      "**Non-Parametric Methods**",
      "Kaplan-Meier Estimator"
    ]
  },
  {
    "objectID": "02-07-01-01-survival-analysis-nonparametric-kaplan-meier-r.html#summary-and-conclusion",
    "href": "02-07-01-01-survival-analysis-nonparametric-kaplan-meier-r.html#summary-and-conclusion",
    "title": "1.1 Kaplan-Meier Estimator",
    "section": "Summary and Conclusion",
    "text": "Summary and Conclusion\nNonparametric survival analysis, especially using the Kaplan-Meier estimator and log-rank test, is a powerful tool for analyzing time-to-event data, particularly in situations where the distribution of survival times is unknown and when censoring is present. These techniques allow researchers to estimate and compare survival probabilities between groups, providing a foundation for more advanced survival analysis methods.\nThese methods are fundamental in fields like clinical research, reliability engineering, and economics, where time-to-event outcomes and censored data are prevalent.",
    "crumbs": [
      "**Non-Parametric Methods**",
      "Kaplan-Meier Estimator"
    ]
  },
  {
    "objectID": "02-07-01-01-survival-analysis-nonparametric-kaplan-meier-r.html#references",
    "href": "02-07-01-01-survival-analysis-nonparametric-kaplan-meier-r.html#references",
    "title": "1.1 Kaplan-Meier Estimator",
    "section": "References",
    "text": "References\n\nSurvival Analysis with R\nSurvival data analysis\nSurvival Analysis in R\nSurvival Analysis with R\nSurvival Analysis in R Companion\nSurvival Analysis in R",
    "crumbs": [
      "**Non-Parametric Methods**",
      "Kaplan-Meier Estimator"
    ]
  },
  {
    "objectID": "02-07-02-02-survival-analysis-time-dependent-covariates-r.html#what-are-time-dependent-covariates",
    "href": "02-07-02-02-survival-analysis-time-dependent-covariates-r.html#what-are-time-dependent-covariates",
    "title": "2.2 Survival Analysis with Time-Dependent Covariates",
    "section": "What Are Time-Dependent Covariates?",
    "text": "What Are Time-Dependent Covariates?\nTime-dependent covariates (also called time-varying covariates) are predictor variables whose values change over time during the follow-up period of a survival study.\n\nExamples in Practice:\n\nMedical: Blood pressure, CD4 count, treatment status (started/stopped during study)\nEngineering: Temperature, stress levels, maintenance status of equipment\nEconomics: Employment status, income level, policy changes\nSocial Sciences: Marital status, education level, behavioral changes\n\n\n\nStandard vs. Time-Dependent Approach:\n\n\n\n\n\n\n\nStandard Survival Analysis\nTime-Dependent Survival Analysis\n\n\n\n\nUses baseline values only\nUses current values at time t\n\n\nAssumes covariates are fixed\nAllows covariates to evolve\n\n\nMay introduce bias if values change\nProvides more accurate risk assessment\n\n\n\n\n\nWhy Do We Need Time-Dependent Covariates?\n\nProblem with Fixed Covariates: Imagine studying the effect of blood pressure on heart attack risk:\nPatient A has BP = 120/80 at baseline but develops hypertension (160/100) after 6 months\nStandard analysis would still classify them as “low risk” based on baseline\nReality: Their risk increased when BP rose\nSolution: Time-dependent covariates allow the hazard at time t to depend on the current value of the covariate at that exact time.\n\n\n\nMathematical Framework\nStandard Cox Model:\n\\[\nh(t \\mid X) = h_0(t) \\exp(\\beta X)\n\\] Where \\(X\\) is fixed for each subject.\nCox Model with Time-Dependent Covariates:\n\\[\nh(t \\mid X(t)) = h_0(t) \\exp(\\beta X(t))\n\\] Where \\(X(t)\\) is the value of the covariate at time t.\nThe hazard ratio compares two individuals at the same time point t, using their current covariate values at that time.\n\n\nData Structure: Start-Stop Format\nTo implement time-dependent covariates, data must be structured in “start-stop” (or counting process) format:\n\n\n\nID\ntstart\ntstop\nevent\ncovariate\n\n\n\n\n1\n0\n30\n0\n0\n\n\n1\n30\n75\n1\n1\n\n\n2\n0\n45\n0\n0\n\n\n2\n45\n120\n0\n1\n\n\n\n\n\nRules:\n\nEach row represents a time interval [tstart, tstop)\nCovariate values are constant within each interval\nEvent occurs at the end of the interval where event = 1\nMultiple rows per subject are allowed\n\n\n\nImplementation Approaches\n\n\n1. True Time-Dependent Covariates\n\nCovariate values actually change during follow-up\nExample: Lab measurements, treatment initiation\nRequires data restructuring into start-stop format\n\n\n\n2. Time-Transformed Covariates (for PH violation)\n\nCreate interaction terms like covariate × log(time)\nUsed when proportional hazards assumption is violated\nDoesn’t require new data structure\n\n\n\n3. Cumulative Covariates\n\nUse running averages or cumulative exposure\nExample: Total drug dose received up to time t\n\n\n\nImportant Assumptions and Considerations\n\n\nValid Assumptions:\n\nNo future knowledge: Covariate at time t depends only on information available at or before t\nAccurate timing: Change points are recorded precisely\nPH assumption: Still applies to the time-dependent coefficients\n\n\n\nCommon Pitfalls:\n\n1. Immortal Time Bias\n\nWrong: Assign treatment status based on future events\nRight: Treatment status can only change based on past/current information\n\n\n\n2. Informative Censoring\n\nIf covariate measurement stops when patient deteriorates, this creates bias\nMissing data should be handled appropriately (e.g., last observation carried forward)\n\n\n\n3. Over-splitting\n\nToo many intervals can lead to computational issues\nBalance between accuracy and practicality"
  },
  {
    "objectID": "02-07-02-02-survival-analysis-time-dependent-covariates-r.html#practical-example",
    "href": "02-07-02-02-survival-analysis-time-dependent-covariates-r.html#practical-example",
    "title": "2.2 Survival Analysis with Time-Dependent Covariates",
    "section": "Practical Example",
    "text": "Practical Example\n\nScenario: Cancer Treatment Study\n\nPatients may start chemotherapy during follow-up\nWe want to estimate the effect of being on chemo at time t\n\n\n\nData Structure:\n\n\n\nPatient\ntstart\ntstop\nDeath\nChemo\n\n\n\n\nA\n0\n60\n0\n0\n\n\nA\n60\n120\n1\n1\n\n\nB\n0\n90\n0\n0\n\n\nB\n90\n180\n0\n1\n\n\n\n\n\nInterpretation:\n\nHazard ratio for chemo represents the instantaneous risk of death for someone currently on chemo vs. currently not on chemo, at the same time point\n\n\n\nWhen NOT to Use Time-Dependent Covariates\n\nWhen covariates are truly fixed (e.g., genetic markers, sex, baseline age)\nWhen change points are unknown or poorly recorded\nWhen the primary interest is baseline risk prediction rather than dynamic risk assessment\n\n\n\nAdvantages and Benefits\n\n\nAdvantages:\n\nMore realistic modeling of real-world processes\nReduced bias from misclassifying time-varying exposures\nBetter risk prediction for clinical decision-making\nHandles treatment switching in clinical trials\n\n\n\nApplications:\n\nClinical research: Treatment effects, biomarker dynamics\nReliability engineering: Maintenance effects, wear-and-tear\nEconomics: Policy impact assessment\nEpidemiology: Behavioral risk factor changes",
    "crumbs": [
      "**Semi-Parametric Methods**",
      "Survival Analysis with Time-Dependent Covariates"
    ]
  },
  {
    "objectID": "02-07-02-02-survival-analysis-time-dependent-covariates-r.html#survival-analysis-with-time-dependent-covariates-1",
    "href": "02-07-02-02-survival-analysis-time-dependent-covariates-r.html#survival-analysis-with-time-dependent-covariates-1",
    "title": "2.2 Survival Analysis with Time-Dependent Covariates",
    "section": "Survival Analysis with Time-Dependent Covariates",
    "text": "Survival Analysis with Time-Dependent Covariates\nThis tutorial uses real data from the Primary Biliary Cirrhosis (PBC) study and demonstrates how to handle time-dependent covariates using the tmerge() function in R.\n\nInstall Required R Packages\nFollowing R packages are required to run this notebook. If any of these packages are not installed, you can install them using the code below:\n\n\nCode\npackages &lt;-c(\n         'tidyverse',\n         'performance',\n         'gtsummary',\n         'survival',\n         'survminer',\n         'ggsurvfit',\n         'tidycmprsk',\n         'ggfortify',\n         'timereg',\n         'cmprsk',\n         'condSURV',\n         'riskRegression',\n         'joineR'\n         )\n\n\n#| warning: false\n#| error: false\n\n# Install missing packages\nnew_packages &lt;- packages[!(packages %in% installed.packages()[,\"Package\"])]\nif(length(new_packages)) install.packages(new_packages)\n\ndevtools::install_github(\"ItziarI/WeDiBaDis\")\n\n\nVerify Installation\n\n\nCode\n# Verify installation\ncat(\"Installed packages:\\n\")\n\n\nInstalled packages:\n\n\nCode\nprint(sapply(packages, requireNamespace, quietly = TRUE))\n\n\nRegistered S3 method overwritten by 'riskRegression':\n  method        from \n  nobs.multinom broom\n\n\n     tidyverse    performance      gtsummary       survival      survminer \n          TRUE           TRUE           TRUE           TRUE           TRUE \n     ggsurvfit     tidycmprsk      ggfortify        timereg         cmprsk \n          TRUE           TRUE           TRUE           TRUE           TRUE \n      condSURV riskRegression         joineR \n          TRUE           TRUE           TRUE \n\n\n\n\nLoad Packages\n\n\nCode\n# Load packages with suppressed messages\ninvisible(lapply(packages, function(pkg) {\n  suppressPackageStartupMessages(library(pkg, character.only = TRUE))\n}))\n\n\n\n\nCode\n# Check loaded packages\ncat(\"Successfully loaded packages:\\n\")\n\n\nSuccessfully loaded packages:\n\n\nCode\nprint(search()[grepl(\"package:\", search())])\n\n\n [1] \"package:joineR\"         \"package:riskRegression\" \"package:condSURV\"      \n [4] \"package:cmprsk\"         \"package:timereg\"        \"package:ggfortify\"     \n [7] \"package:tidycmprsk\"     \"package:ggsurvfit\"      \"package:survminer\"     \n[10] \"package:ggpubr\"         \"package:survival\"       \"package:gtsummary\"     \n[13] \"package:performance\"    \"package:lubridate\"      \"package:forcats\"       \n[16] \"package:stringr\"        \"package:dplyr\"          \"package:purrr\"         \n[19] \"package:readr\"          \"package:tidyr\"          \"package:tibble\"        \n[22] \"package:ggplot2\"        \"package:tidyverse\"      \"package:stats\"         \n[25] \"package:graphics\"       \"package:grDevices\"      \"package:utils\"         \n[28] \"package:datasets\"       \"package:methods\"        \"package:base\"          \n\n\n\n\nData\nWe’ll use the Stanford Heart Transplant dataset (built into the survival package as heart). This dataset tracks patients waiting for heart transplants. Key features:\n\nIt includes time-dependent covariates like transplant (0 = no transplant, 1 = transplant received, which changes over time for some patients).\nThe data is already in counting process format: each row represents an interval with start (start time of interval), stop (end time of interval), and event (1 if event occurred at end of interval, 0 otherwise).\nOther covariates: age (age at entry), year (year of acceptance), surgery (prior surgery: 0/1).\nEvent: Death or censoring.\nThere are 103 observations from 69 patients (some have multiple rows due to time-dependent changes).\n\nThis dataset is ideal because the transplant status is time-dependent—patients start without a transplant, and some receive one later, splitting their survival time into pre- and post-transplant intervals.\n\n\nCode\ndata(heart, package = \"survival\")\nhead(heart)\n\n\n  start stop event        age      year surgery transplant id\n1     0   50     1 -17.155373 0.1232033       0          0  1\n2     0    6     1   3.835729 0.2546201       0          0  2\n3     0    1     0   6.297057 0.2655715       0          0  3\n4     1   16     1   6.297057 0.2655715       0          1  3\n5     0   36     0  -7.737166 0.4900753       0          0  4\n6    36   39     1  -7.737166 0.4900753       0          1  4\n\n\n\n\nCode\n# Load the pbcseq dataset\ndata(pbcseq)\n\n\nWarning in data(pbcseq): data set 'pbcseq' not found\n\n\nCode\n# Inspect pbcseq\nhead(pbcseq)\n\n\n  id futime status trt      age sex day ascites hepato spiders edema bili chol\n1  1    400      2   1 58.76523   f   0       1      1       1     1 14.5  261\n2  1    400      2   1 58.76523   f 192       1      1       1     1 21.3   NA\n3  2   5169      0   1 56.44627   f   0       0      1       1     0  1.1  302\n4  2   5169      0   1 56.44627   f 182       0      1       1     0  0.8   NA\n5  2   5169      0   1 56.44627   f 365       0      1       1     0  1.0   NA\n6  2   5169      0   1 56.44627   f 768       0      1       1     0  1.9   NA\n  albumin alk.phos   ast platelet protime stage\n1    2.60     1718 138.0      190    12.2     4\n2    2.94     1612   6.2      183    11.2     4\n3    4.14     7395 113.5      221    10.6     3\n4    3.60     2107 139.5      188    11.0     3\n5    3.55     1711 144.2      161    11.6     3\n6    3.92     1365 144.2      122    10.6     3\n\n\n\n\nPrepare Data for Time-Dependent Analysis\nThe heart data is already in the required format for time-dependent Cox models:\n\nUse Surv(start, stop, event) in the model formula to handle intervals.\nThis format allows the model to account for covariate changes at specific times.\n\nIf your own data isn’t in this format, you’d need to reshape it using functions like tmerge() from the survival package to create start-stop intervals based on time-varying covariates.\nThe tmerge() function is used to create a dataset suitable for time-dependent Cox models by merging baseline data with time-varying covariate data. For example, if you had a long-format dataset with time points where covariates change, tmerge() can merge baseline data with time-dependent data:\n# Hypothetical example (not needed for 'heart')\ntdc_data &lt;- tmerge(data1 = baseline, data2 = time_dep, id = id, tstart = 0, tstop = time)\n\n\nFit Cox Model with Time-Dependent Covariates\nThe Cox proportional hazards model extends to time-dependent covariates naturally in the counting process format. Fit a model including fixed covariates (age, surgery) and time-dependent (transplant):\n\n\nCode\n# Fit the Cox model\ncox_model &lt;- coxph(Surv(start, stop, event) ~ age + surgery + transplant, data = heart)\n# Print summary\nsummary(cox_model)\n\n\nCall:\ncoxph(formula = Surv(start, stop, event) ~ age + surgery + transplant, \n    data = heart)\n\n  n= 172, number of events= 75 \n\n                coef exp(coef) se(coef)      z Pr(&gt;|z|)  \nage          0.03054   1.03101  0.01389  2.198   0.0279 *\nsurgery     -0.77333   0.46147  0.35967 -2.150   0.0315 *\ntransplant1  0.01610   1.01623  0.30859  0.052   0.9584  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n            exp(coef) exp(-coef) lower .95 upper .95\nage            1.0310     0.9699     1.003    1.0595\nsurgery        0.4615     2.1670     0.228    0.9339\ntransplant1    1.0162     0.9840     0.555    1.8606\n\nConcordance= 0.6  (se = 0.036 )\nLikelihood ratio test= 10.72  on 3 df,   p=0.01\nWald test            = 9.68  on 3 df,   p=0.02\nScore (logrank) test = 10  on 3 df,   p=0.02\n\n\nInterpretation\nage: Positive coefficient means older age increases hazard (risk of death) by about 3% per year (exp(coef) ≈ 1.03). surgery: Negative coefficient indicates prior surgery reduces hazard by about 66% (exp(coef) ≈ 0.34), significant (p=0.012). transplant: Not significant but negative coef suggests transplant might slightly reduce hazard.\nThe model accounts for transplant changing over time.\n\n\nProportional Hazards Assumption\nCheck proportional hazards assumption (should be non-significant for validity):\n\n\nCode\ncox.zph(cox_model)\n\n\n            chisq df    p\nage        0.8954  1 0.34\nsurgery    0.0968  1 0.76\ntransplant 0.1283  1 0.72\nGLOBAL     1.3793  3 0.71\n\n\nIf p-values are low, the assumption may be violated—consider stratifying or extending the model.\n\n\nModel Diagnostics and Interpretation\n\nHazard Ratios: From exp(coef), interpret as multiplicative effect on hazard rate.\nConfidence Intervals: Use confint(cox_model).\nPredict Survival: For new data, use survfit(cox_model, newdata = ...).\n\nFor time-dependent models, predictions are trickier because survival curves depend on when the covariate changes. You can simulate scenarios:\n\n\nCode\n# Hypothetical new patient: age 0 (mean), no surgery, transplant at day 50\nnew_data &lt;- data.frame(start = c(0, 50), \n                       stop = c(50, Inf),\n                       event = 0, \n                       age = 0, \n                       surgery = 0, \n                       transplant = c(0, 1))\n# But for prediction, use survfit with type=\"surv\"\n\n\n\n\nEstimate Cumulative Baseline Hazard\nThe basehaz() function extracts the cumulative baseline hazard function $ H_0(t $. By default, it assumes covariates are centered (mean = 0 for continuous covariates like age).\n\n\nCode\n# Estimate cumulative baseline hazard\nbaseline_hazard &lt;- basehaz(cox_model, centered = TRUE)\n# View the first few rows\nhead(baseline_hazard)\n\n\n      hazard time\n1 0.01028146  1.0\n2 0.04162004  2.0\n3 0.07395244  3.0\n4 0.07395244  4.0\n5 0.07395244  4.5\n6 0.09610416  5.0\n\n\n\nInterpretation: $ H_0(t) $ is the expected number of events (e.g., deaths) by time $ t $ for a reference individual (covariates = 0). For example, at time = 15, if hazard = 0.040615, the cumulative risk of the event is low for the baseline group.\nTime-Dependent Covariates: The transplant covariate’s time-dependent nature is handled in the model’s likelihood, so the baseline hazard assumes transplant = 0 (no transplant) unless specified otherwise.\nRelation to Survival: The baseline survival function is \\(S_0(t) = \\exp(-H_0(t))\\). We can compute this if needed.\n\nTo visualize \\(H_0(t)\\), we can plot it against time using ggplot2 for clarity. The cumulative hazard typically increases over time, with steps at each event.\n\n\nCode\n# Plot cumulative baseline hazard\nggplot(baseline_hazard, aes(x = time, y = hazard)) +\n  geom_step(color = \"#1F77B4\", size = 1) +\n  labs(title = \"Cumulative Baseline Hazard Function\",\n       x = \"Time (Days)\",\n       y = \"Cumulative Hazard H_0(t)\") +\n  theme_minimal()\n\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\n\n\n\n\n\n\nCompute Baseline Survival\nIf you want the baseline survival function $S_0(t) = (-H_0(t)) $:\n\n\nCode\n# Add survival probability\nbaseline_hazard$survival &lt;- exp(-baseline_hazard$hazard)\n\n# Plot survival\nggplot(baseline_hazard, aes(x = time, y = survival)) +\n  geom_step(color = \"#FF7F0E\", size = 1) +\n  labs(title = \"Baseline Survival Function\",\n       x = \"Time (Days)\",\n       y = \"Survival Probability S_0(t)\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\nCumulative hazard for a specific patient\nTo estimate the cumulative hazard for a specific patient (e.g., age = mean, surgery = 1, transplant = 1), use the model’s coefficients:\n\n\nCode\n# Extract coefficients\ncoef &lt;- coef(cox_model)\n\n# Hazard for specific covariates: H(t) = H_0(t) * exp(beta_age * age + beta_surgery * surgery + beta_transplant * transplant)\nbaseline_hazard$hazard_specific &lt;- baseline_hazard$hazard * exp(coef[\"age\"] * 0 + coef[\"surgery\"] * 2 + coef[\"transplant\"] * 2)\n\n# Plot\nggplot(baseline_hazard, aes(x = time)) +\n  geom_step(aes(y = hazard, color = \"Baseline\"), size = 1) +\n  geom_step(aes(y = hazard_specific, color = \"Surgery & Transplant\"), size = 1) +\n  labs(title = \"Cumulative Hazard: Baseline vs. Specific\",\n       x = \"Time (Days)\",\n       y = \"Cumulative Hazard\") +\n  scale_color_manual(values = c(\"Baseline\" = \"#1F77B4\", \"Surgery & Transplant\" = \"#FF7F0E\")) +\n  theme_minimal()\n\n\nWarning: Removed 111 rows containing missing values or values outside the scale range\n(`geom_step()`).\n\n\n\n\n\n\n\n\n\n\n\nPredict Survival for a New Patient\nTo predict survival for a new patient with specific covariates (e.g., age = mean, surgery = 1, transplant = 1), use the survfit() function:\n\n\nCode\n# New patient data\nnew_patient &lt;- data.frame(age = 0, surgery = 1, transplant = 1)\n\n# Predict survival curve for the new patient\nsurv_pred &lt;- survfit(cox_model, newdata = new_patient)\n\n\nWarning in model.frame.default(data = structure(list(age = 0, surgery = 1, :\nvariable 'transplant' is not a factor\n\n\nCode\n# Plot survival curve using ggsurvplot\nggsurvplot(\n  surv_pred,\n  data = new_patient,\n  conf.int = TRUE,           # Include confidence intervals\n  palette = \"#1F77B4\",      # Blue color for the curve\n  xlab = \"Time (Days)\",\n  ylab = \"Survival Probability\",\n  title = \"Predicted Survival for New Patient (Age = Mean, Surgery = 1, Transplant = 1)\",\n  ggtheme = theme_minimal(), # Clean theme\n  risk.table = TRUE,        # Add risk table\n  risk.table.height = 0.25, # Adjust table height\n  linetype = \"solid\",       # Solid line for the curve\n  size = 1.2                # Line thickness\n)\n\n\nIgnoring unknown labels:\n• fill : \"Strata\"\nIgnoring unknown labels:\n• fill : \"Strata\"\nIgnoring unknown labels:\n• fill : \"Strata\"\nIgnoring unknown labels:\n• fill : \"Strata\"\nIgnoring unknown labels:\n• colour : \"Strata\"",
    "crumbs": [
      "**Semi-Parametric Methods**",
      "Survival Analysis with Time-Dependent Covariates"
    ]
  },
  {
    "objectID": "02-07-02-02-survival-analysis-time-dependent-covariates-r.html#summary-and-conclusion",
    "href": "02-07-02-02-survival-analysis-time-dependent-covariates-r.html#summary-and-conclusion",
    "title": "2.2 Survival Analysis with Time-Dependent Covariates",
    "section": "Summary and Conclusion",
    "text": "Summary and Conclusion\nThe cumulative baseline hazard $ H_0(t) $ is estimated using basehaz() from the fitted Cox model. It’s a step function that increases at event times and forms the basis for predicting hazards for any covariate combination. The heart dataset’s time-dependent structure is handled seamlessly in the Cox model. Use the visualizations to interpret how risk accumulates, and adjust for specific covariates as needed. Time-dependent covariates allow for more realistic modeling of changing risk factors over time, improving the validity and applicability of survival analysis in many fields.",
    "crumbs": [
      "**Semi-Parametric Methods**",
      "Survival Analysis with Time-Dependent Covariates"
    ]
  },
  {
    "objectID": "02-07-02-02-survival-analysis-time-dependent-covariates-r.html#resources",
    "href": "02-07-02-02-survival-analysis-time-dependent-covariates-r.html#resources",
    "title": "2.2 Survival Analysis with Time-Dependent Covariates",
    "section": "Resources",
    "text": "Resources\n\nSurvival Package Vignette: Time-Dependent Covariates\n\nOfficial guide on Cox models with time-dependent covariates using the heart dataset.\n\nLink: vignette(\"timedep\", package = \"survival\") or CRAN.\n\nDataCamp: Survival Analysis in R\n\nInteractive course covering time-dependent covariates with survival package.\n\nLink: DataCamp.\n\nPaul Lambert’s Tutorial on Flexible Parametric Models\n\nAdvanced guide with R code for time-dependent covariate modeling.\n\nLink: Paul Lambert’s Website.\n\nApplied Survival Analysis Using R (Dirk F. Moore)\n\nBook with practical R code for time-dependent Cox models.\n\nLink: Springer.\n\nR for Health Data Science (Survival Chapter)\n\nFree book chapter on survival analysis with time-varying covariates.\n\nLink: R for Health Data Science.",
    "crumbs": [
      "**Semi-Parametric Methods**",
      "Survival Analysis with Time-Dependent Covariates"
    ]
  },
  {
    "objectID": "02-07-02-03-survival-analysis-stratified-cox-model-r.html#overview",
    "href": "02-07-02-03-survival-analysis-stratified-cox-model-r.html#overview",
    "title": "2.3 Stratified Cox Model",
    "section": "Overview",
    "text": "Overview\nThe Stratified Cox Proportional Hazards Model is a modification of the standard Cox Proportional Hazards (PH) model, which is a semi-parametric method used in survival analysis to assess the impact of covariates on the hazard rate of an event (e.g., death, failure) over time. The standard Cox model assumes that the hazard ratios are constant over time (the PH assumption), meaning the effect of covariates on the hazard is proportional and does not change.\nHowever, this assumption can be violated for certain covariates, such as categorical factors like treatment group or tumor type, where the hazard functions cross or diverge non-proportionally. The stratified version addresses this by dividing the data into strata based on the levels of the violating covariate (e.g., different categories of a variable). Within each stratum, a separate baseline hazard function ( h_{0j}(t) ) is estimated, allowing it to vary across strata. The model takes the form:\n\\[\nh_{ij}(t) = h_{0j}(t) \\exp(\\mathbf{x}_i^T \\boldsymbol{\\beta})\n\\]\nwhere: - \\(i\\) indexes individuals, - \\(j\\) indexes strata, - \\(h_{0j}(t)\\) is the stratum-specific baseline hazard, - \\(\\mathbf{x}_i\\) are the covariates (excluding the stratification variable), - \\(\\boldsymbol{\\beta}\\) are the regression coefficients, assumed to be the same across all strata.\nThis approach controls for the stratifying variable without estimating a coefficient for it, meaning you cannot directly test or quantify its effect on the hazard. The partial likelihood is the product of stratum-specific likelihoods, and estimation proceeds by summing contributions from each stratum (e.g., via Newton-Raphson optimization).\n\nKey advantages\n\nHandles non-PH for the stratifying variable without needing time-dependent covariates.\nUseful for sensitivity analyses or when the stratifying factor is a confounder (e.g., study site in multi-center trials).",
    "crumbs": [
      "**Semi-Parametric Methods**",
      "Stratified Cox Model"
    ]
  },
  {
    "objectID": "02-07-02-03-survival-analysis-stratified-cox-model-r.html#limitations",
    "href": "02-07-02-03-survival-analysis-stratified-cox-model-r.html#limitations",
    "title": "2.3 Stratified Cox Model",
    "section": "Limitations",
    "text": "Limitations\n\nReduces statistical efficiency slightly if stratification is unnecessary.\nNo inference (e.g., p-values) for the stratification variable.\nBest for categorical variables with few levels; challenging for continuous or many-level variables.\n\nThe PH assumption must still hold within each stratum for the remaining covariates. Diagnostics like Schoenfeld residuals (via cox.zph() in R) or plots of log cumulative hazards can check for violations before deciding to stratify.\n\nStratified Cox Model in R\nWe uses the survival package in R to fit and interpret a stratified Cox model. We’ll use the built-in lung dataset from the package, which contains survival data for 228 patients with advanced lung cancer. Key variables include: - time: Survival time in days. - status: Censoring indicator (1 = censored, 2 = dead). - age: Age in years. - sex: Sex (1 = male, 2 = female). - wt.loss: Weight loss in the last six months (in pounds).\n(Note: In practice, recode status to 0/1 for censored/event if needed, but here it’s already suitable after adjustment.)\n\n\nInstall Required R Packages\nFollowing R packages are required to run this notebook. If any of these packages are not installed, you can install them using the code below:\n\n\nCode\npackages &lt;-c(\n         'tidyverse',\n         'performance',\n         'gtsummary',\n         'survival',\n         'survminer',\n         'ggsurvfit',\n         'tidycmprsk',\n         'ggfortify',\n         'timereg',\n         'cmprsk',\n         'condSURV',\n         'riskRegression',\n         'joineR'\n         )\n\n\n#| warning: false\n#| error: false\n\n# Install missing packages\nnew_packages &lt;- packages[!(packages %in% installed.packages()[,\"Package\"])]\nif(length(new_packages)) install.packages(new_packages)\n\ndevtools::install_github(\"ItziarI/WeDiBaDis\")\n\n\nVerify installation\n\n\nCode\n# Verify installation\ncat(\"Installed packages:\\n\")\n\n\nInstalled packages:\n\n\nCode\nprint(sapply(packages, requireNamespace, quietly = TRUE))\n\n\nRegistered S3 method overwritten by 'riskRegression':\n  method        from \n  nobs.multinom broom\n\n\n     tidyverse    performance      gtsummary       survival      survminer \n          TRUE           TRUE           TRUE           TRUE           TRUE \n     ggsurvfit     tidycmprsk      ggfortify        timereg         cmprsk \n          TRUE           TRUE           TRUE           TRUE           TRUE \n      condSURV riskRegression         joineR \n          TRUE           TRUE           TRUE \n\n\n\n\nLoad Packages\n\n\nCode\n# Load packages with suppressed messages\ninvisible(lapply(packages, function(pkg) {\n  suppressPackageStartupMessages(library(pkg, character.only = TRUE))\n}))\n\n\n\n\nCode\n# Check loaded packages\ncat(\"Successfully loaded packages:\\n\")\n\n\nSuccessfully loaded packages:\n\n\nCode\nprint(search()[grepl(\"package:\", search())])\n\n\n [1] \"package:joineR\"         \"package:riskRegression\" \"package:condSURV\"      \n [4] \"package:cmprsk\"         \"package:timereg\"        \"package:ggfortify\"     \n [7] \"package:tidycmprsk\"     \"package:ggsurvfit\"      \"package:survminer\"     \n[10] \"package:ggpubr\"         \"package:survival\"       \"package:gtsummary\"     \n[13] \"package:performance\"    \"package:lubridate\"      \"package:forcats\"       \n[16] \"package:stringr\"        \"package:dplyr\"          \"package:purrr\"         \n[19] \"package:readr\"          \"package:tidyr\"          \"package:tibble\"        \n[22] \"package:ggplot2\"        \"package:tidyverse\"      \"package:stats\"         \n[25] \"package:graphics\"       \"package:grDevices\"      \"package:utils\"         \n[28] \"package:datasets\"       \"package:methods\"        \"package:base\"          \n\n\n\n\nData\nWe will be utilizing the lung dataset from the {survival} package, which serves as a valuable resource for analyzing survival data. This dataset comprises information from subjects diagnosed with advanced lung cancer, specifically gathered from the North Central Cancer Treatment Group, a prominent clinical trial network dedicated to cancer research. This dataset has 228 observations and 10 variables. For analysis, we’ll focus on time, status, age, sex, and wt.loss.\n\n\nCode\n# Load veteran  dataset\ndata(lung)\nglimpse(lung)\n\n\nRows: 228\nColumns: 10\n$ inst      &lt;dbl&gt; 3, 3, 3, 5, 1, 12, 7, 11, 1, 7, 6, 16, 11, 21, 12, 1, 22, 16…\n$ time      &lt;dbl&gt; 306, 455, 1010, 210, 883, 1022, 310, 361, 218, 166, 170, 654…\n$ status    &lt;dbl&gt; 2, 2, 1, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, …\n$ age       &lt;dbl&gt; 74, 68, 56, 57, 60, 74, 68, 71, 53, 61, 57, 68, 68, 60, 57, …\n$ sex       &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 2, 1, …\n$ ph.ecog   &lt;dbl&gt; 1, 0, 0, 1, 0, 1, 2, 2, 1, 2, 1, 2, 1, NA, 1, 1, 1, 2, 2, 1,…\n$ ph.karno  &lt;dbl&gt; 90, 90, 90, 90, 100, 50, 70, 60, 70, 70, 80, 70, 90, 60, 80,…\n$ pat.karno &lt;dbl&gt; 100, 90, 90, 60, 90, 80, 60, 80, 80, 70, 80, 70, 90, 70, 70,…\n$ meal.cal  &lt;dbl&gt; 1175, 1225, NA, 1150, NA, 513, 384, 538, 825, 271, 1025, NA,…\n$ wt.loss   &lt;dbl&gt; NA, 15, 15, 11, 0, 0, 10, 1, 16, 34, 27, 23, 5, 32, 60, 15, …\n\n\n\n\nFit a Standard (Unstratified) Cox Model\nFirst, fit a standard Cox model to assess covariates and check the PH assumption.\n\n\nCode\n# Fit the model\nlung_cox &lt;- coxph(Surv(time, status == 2) ~ age + sex + wt.loss, data = lung)\n# Summary of results\nsummary(lung_cox)\n\n\nCall:\ncoxph(formula = Surv(time, status == 2) ~ age + sex + wt.loss, \n    data = lung)\n\n  n= 214, number of events= 152 \n   (14 observations deleted due to missingness)\n\n              coef  exp(coef)   se(coef)      z Pr(&gt;|z|)   \nage      0.0200882  1.0202913  0.0096644  2.079   0.0377 * \nsex     -0.5210319  0.5939074  0.1743541 -2.988   0.0028 **\nwt.loss  0.0007596  1.0007599  0.0061934  0.123   0.9024   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n        exp(coef) exp(-coef) lower .95 upper .95\nage        1.0203     0.9801    1.0011    1.0398\nsex        0.5939     1.6838    0.4220    0.8359\nwt.loss    1.0008     0.9992    0.9887    1.0130\n\nConcordance= 0.612  (se = 0.027 )\nLikelihood ratio test= 14.67  on 3 df,   p=0.002\nWald test            = 13.98  on 3 df,   p=0.003\nScore (logrank) test = 14.24  on 3 df,   p=0.003\n\n\nInterpretation: Age and sex are significant predictors (higher age increases hazard; females have lower hazard). Weight loss is not significant.\n\n\nCheck the Proportional Hazards Assumption\nUse Schoenfeld residuals to test PH.\n\n\nCode\n# Test PH assumption\nph_test &lt;- cox.zph(lung_cox)\nph_test\n\n\n         chisq df    p\nage     0.5077  1 0.48\nsex     2.5489  1 0.11\nwt.loss 0.0144  1 0.90\nGLOBAL  3.0051  3 0.39\n\n\nCode\nplot(ph_test)  # Visual check for each covariate\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIf the p-values in ph_test are low (e.g., &lt;0.05 for sex), it indicates a violation. Plots should show flat lines around zero for residuals vs. time if PH holds. Suppose sex violates PH; this motivates stratification.\n\n\nFit the Stratified Cox Model\nStratify by the violating variable (e.g., sex). This allows different baseline hazards for males and females.\n\n\nCode\n# Fit stratified model\nlung_strat_sex &lt;- coxph(Surv(time, status == 2) ~ age + wt.loss + strata(sex), data = lung)\n# Summary of results\nsummary(lung_strat_sex)\n\n\nCall:\ncoxph(formula = Surv(time, status == 2) ~ age + wt.loss + strata(sex), \n    data = lung)\n\n  n= 214, number of events= 152 \n   (14 observations deleted due to missingness)\n\n             coef exp(coef)  se(coef)     z Pr(&gt;|z|)  \nage     0.0192190 1.0194049 0.0096226 1.997   0.0458 *\nwt.loss 0.0001412 1.0001412 0.0062509 0.023   0.9820  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n        exp(coef) exp(-coef) lower .95 upper .95\nage         1.019     0.9810     1.000     1.039\nwt.loss     1.000     0.9999     0.988     1.012\n\nConcordance= 0.561  (se = 0.027 )\nLikelihood ratio test= 4.09  on 2 df,   p=0.1\nWald test            = 3.99  on 2 df,   p=0.1\nScore (logrank) test = 4  on 2 df,   p=0.1\n\n\nInterpretation: No coefficient for sex (the stratifying variable). Estimates for age and wt.loss are similar to the unstratified model, with age borderline significant. This serves as a sensitivity check; if results differ substantially, the PH violation may bias the original model.\n\n\nPredict and Plot Survival Curves\nEstimate stratum-specific survival curves.\n\n\nCode\n# Survival curves by stratum\nstrat_surv &lt;- survfit(lung_strat_sex)\n# Plot\nplot(strat_surv, col = 1:2, xlab = \"Time (days)\", ylab = \"Survival Probability\")\nlegend(\"topright\", legend = c(\"Male\", \"Female\"), col = 1:2, lty = 1)\n\n\n\n\n\n\n\n\n\nThis plots separate curves for each sex stratum, adjusted for covariates. For predictions with new data:\n\n\nCode\nnew_data &lt;- data.frame(age = 60, wt.loss = 10, sex = c(1, 2))  # Example for male and female\npredict_surv &lt;- survfit(lung_strat_sex, newdata = new_data)\nplot(predict_surv)\n\n\n\n\n\n\n\n\n\n\n\nAdditional Considerations\n\nIf multiple variables violate PH, stratify on one and use time-dependent terms (e.g., tt()) for others.\nCompare models using ANOVA: anova(lung_cox, lung_strat_sex).\nFor large datasets or more diagnostics, explore packages like survminer for enhanced plotting (e.g., ggcoxzph() for PH checks).\nAlways validate with domain knowledge; stratification is ideal when the variable is a nuisance factor rather than of primary interest.\n\nThis tutorial provides a basic workflow. Adapt it to your data, and run diagnostics thoroughly. For more examples, see the survival package vignette: vignette(\"survival\").",
    "crumbs": [
      "**Semi-Parametric Methods**",
      "Stratified Cox Model"
    ]
  },
  {
    "objectID": "02-07-02-03-survival-analysis-stratified-cox-model-r.html#summary-and-conclusion",
    "href": "02-07-02-03-survival-analysis-stratified-cox-model-r.html#summary-and-conclusion",
    "title": "2.3 Stratified Cox Model",
    "section": "Summary and Conclusion",
    "text": "Summary and Conclusion\nThe Stratified Cox Proportional Hazards Model is a powerful extension of the standard Cox model that allows for the accommodation of non-proportional hazards by stratifying the analysis based on categorical variables that violate the proportional hazards assumption. This approach enables researchers to control for confounding factors without estimating their effects directly, thus providing more accurate estimates for other covariates of interest. This tutorial demonstrated how to implement a stratified Cox model in R, including fitting the model, checking assumptions, and interpreting results. By following these steps, researchers can effectively analyze survival data while addressing potential violations of model assumptions.",
    "crumbs": [
      "**Semi-Parametric Methods**",
      "Stratified Cox Model"
    ]
  },
  {
    "objectID": "02-07-02-03-survival-analysis-stratified-cox-model-r.html#resources",
    "href": "02-07-02-03-survival-analysis-stratified-cox-model-r.html#resources",
    "title": "2.3 Stratified Cox Model",
    "section": "Resources",
    "text": "Resources\n\n“Modeling Survival Data: Extending the Cox Model” by Therneau and Grambsch** Covers stratification in depth with R examples; Springer, ISBN: 978-0387987842.\nR survival Package Vignette Official docs with stratified Cox code; access via vignette(“survival”) or CRAN.\n\n3 UCLA Tutorial: Survival Analysis in R Practical guide including stratified models; UCLA IDRE.\n\nsurvminer Package Visualization tools for stratified Cox (e.g., diagnostics); CRAN vignette at survminer.\nYouTube: MarinStatsLectures Survival Series Video tutorials on Cox models and stratification in R; Playlist.\nCoursera: Survival Analysis in R Course with stratified Cox labs; by Imperial College Coursera.",
    "crumbs": [
      "**Semi-Parametric Methods**",
      "Stratified Cox Model"
    ]
  },
  {
    "objectID": "02-07-03-00-survival-analysis-recurrent-event-models-r.html#key-differences-between-models",
    "href": "02-07-03-00-survival-analysis-recurrent-event-models-r.html#key-differences-between-models",
    "title": "Recurrent Event Models",
    "section": "Key Differences Between Models",
    "text": "Key Differences Between Models\n\n\n\n\n\n\n\n\n\n\nFeature\nAndersen-Gill (AG)\nPrentice-Williams-Peterson (PWP)\nFrailty Models\nMarginal Models (WLW)\n\n\n\n\nBaseline Hazard\nSingle \\(h_0(t)\\)\nStratified \\(h_{0k}(t)\\) or gap time \\(h_{0k}(u)\\)\nSingle \\(h_0(t)\\) with frailty term\nSingle \\(h_0(t)\\)\n\n\nCovariates\nFixed or time-dependent\nFixed or time-dependent\nFixed or time-dependent\nFixed or time-dependent\n\n\nEvent Dependence\nAssumes independence given covariates\nModels event order or gap times explicitly\nAccounts for unobserved heterogeneity\nTreats events as separate processes\n\n\nData Structure\nCounting process format\nCounting process format with stratum\nStandard survival format with clustering\nCounting process format\n\n\nPrimary Purpose\nHandle recurrent events\nHandle ordered recurrent events\nAccount for unobserved heterogeneity\nAnalyze overall effects of recurrent events\n\n\nEffect Estimation\nSingle \\(\\boldsymbol{\\beta}\\)\nSingle \\(\\ boldsymbol{\\beta}\\) per stratum\nSingle \\(\\boldsymbol{\\beta}\\) with frailty variance\nSingle \\(\\boldsymbol{\\beta}\\)\n\n\nCorrelation Handling\nRobust variance (cluster by subject)\nStratification by event number\nRandom effect (frailty term)\nRobust variance (cluster by subject)"
  },
  {
    "objectID": "02-07-03-00-survival-analysis-recurrent-event-models-r.html#summary-and-key-takeaways",
    "href": "02-07-03-00-survival-analysis-recurrent-event-models-r.html#summary-and-key-takeaways",
    "title": "Recurrent Event Models",
    "section": "Summary and Key Takeaways",
    "text": "Summary and Key Takeaways\n\nRecurrent event models extend traditional survival analysis to handle multiple occurrences of the same event per subject, accounting for dependencies between events.\nCommon recurrent event models include Andersen-Gill (AG), Prentice-Williams-Pet erson (PWP), frailty models, and marginal models (e.g., Wei-Lin-Weissfeld).\nEach model has unique assumptions, data structures, and applications, making them suitable for different research contexts.\nRecurrent event models can incorporate time-dependent covariates, but their primary focus is on event recurrence rather than solely on changing predictors.\nChoosing the appropriate model depends on the research question, data characteristics, and assumptions about event dependence\nUnderstanding the distinctions between recurrent event models and survival analysis with time-dependent covariates is crucial for accurate modeling and interpretation in survival analysis studies."
  },
  {
    "objectID": "02-07-03-00-survival-analysis-recurrent-event-models-r.html#resources",
    "href": "02-07-03-00-survival-analysis-recurrent-event-models-r.html#resources",
    "title": "Recurrent Event Models",
    "section": "Resources",
    "text": "Resources\n\nBook: “The Statistical Analysis of Recurrent Events” by Cook & Lawless (2007)\n\nCovers PWP models with theory and R examples.\n\nAccess: Springer or libraries (ISBN: 978-0-387-69809-0).\n\nR “survival” Package Vignette (CRAN)\n\nGuides PWP model fitting with coxph() and datasets like bladder.\n\nAccess: vignette(\"survival\") or CRAN website.\n\nArticle: Cook & Lawless (2002)\n\nReviews PWP-TT/GT models and applications.\n\nAccess: PubMed or Sage Journals (DOI: 10.1191/0962280202sm295ra).\n\nUCLA Tutorial: Recurrent Event Analysis in R\n\nStep-by-step R code for PWP models using survival.\n\nAccess: https://stats.idre.ucla.edu/r/seminars/recurrent-events/.\n\nBook: “Survival Analysis” by Klein & Moeschberger (2003, Ch. 12)\n\nExplains PWP models and counting processes.\n\nAccess: Springer or libraries (ISBN: 978-0-387-95399-1)."
  },
  {
    "objectID": "02-07-04-00-survival-analysis-recurrent-event-models-r.html#key-differences-between-models",
    "href": "02-07-04-00-survival-analysis-recurrent-event-models-r.html#key-differences-between-models",
    "title": "Recurrent Event Models",
    "section": "Key Differences Between Models",
    "text": "Key Differences Between Models\n\n\n\n\n\n\n\n\n\n\nFeature\nAndersen-Gill (AG)\nPrentice-Williams-Peterson (PWP)\nFrailty Models\nMarginal Models (WLW)\n\n\n\n\nBaseline Hazard\nSingle \\(h_0(t)\\)\nStratified \\(h_{0k}(t)\\) or gap time \\(h_{0k}(u)\\)\nSingle \\(h_0(t)\\) with frailty term\nSingle \\(h_0(t)\\)\n\n\nCovariates\nFixed or time-dependent\nFixed or time-dependent\nFixed or time-dependent\nFixed or time-dependent\n\n\nEvent Dependence\nAssumes independence given covariates\nModels event order or gap times explicitly\nAccounts for unobserved heterogeneity\nTreats events as separate processes\n\n\nData Structure\nCounting process format\nCounting process format with stratum\nStandard survival format with clustering\nCounting process format\n\n\nPrimary Purpose\nHandle recurrent events\nHandle ordered recurrent events\nAccount for unobserved heterogeneity\nAnalyze overall effects of recurrent events\n\n\nEffect Estimation\nSingle \\(\\boldsymbol{\\beta}\\)\nSingle \\(\\ boldsymbol{\\beta}\\) per stratum\nSingle \\(\\boldsymbol{\\beta}\\) with frailty variance\nSingle \\(\\boldsymbol{\\beta}\\)\n\n\nCorrelation Handling\nRobust variance (cluster by subject)\nStratification by event number\nRandom effect (frailty term)\nRobust variance (cluster by subject)"
  },
  {
    "objectID": "02-07-04-00-survival-analysis-recurrent-event-models-r.html#summary-and-key-takeaways",
    "href": "02-07-04-00-survival-analysis-recurrent-event-models-r.html#summary-and-key-takeaways",
    "title": "Recurrent Event Models",
    "section": "Summary and Key Takeaways",
    "text": "Summary and Key Takeaways\n\nRecurrent event models extend traditional survival analysis to handle multiple occurrences of the same event per subject, accounting for dependencies between events.\nCommon recurrent event models include Andersen-Gill (AG), Prentice-Williams-Pet erson (PWP), frailty models, and marginal models (e.g., Wei-Lin-Weissfeld).\nEach model has unique assumptions, data structures, and applications, making them suitable for different research contexts.\nRecurrent event models can incorporate time-dependent covariates, but their primary focus is on event recurrence rather than solely on changing predictors.\nChoosing the appropriate model depends on the research question, data characteristics, and assumptions about event dependence\nUnderstanding the distinctions between recurrent event models and survival analysis with time-dependent covariates is crucial for accurate modeling and interpretation in survival analysis studies."
  },
  {
    "objectID": "02-07-04-00-survival-analysis-recurrent-event-models-r.html#resources",
    "href": "02-07-04-00-survival-analysis-recurrent-event-models-r.html#resources",
    "title": "Recurrent Event Models",
    "section": "Resources",
    "text": "Resources\n\nBook: “The Statistical Analysis of Recurrent Events” by Cook & Lawless (2007)\n\nCovers PWP models with theory and R examples.\n\nAccess: Springer or libraries (ISBN: 978-0-387-69809-0).\n\nR “survival” Package Vignette (CRAN)\n\nGuides PWP model fitting with coxph() and datasets like bladder.\n\nAccess: vignette(\"survival\") or CRAN website.\n\nArticle: Cook & Lawless (2002)\n\nReviews PWP-TT/GT models and applications.\n\nAccess: PubMed or Sage Journals (DOI: 10.1191/0962280202sm295ra).\n\nUCLA Tutorial: Recurrent Event Analysis in R\n\nStep-by-step R code for PWP models using survival.\n\nAccess: https://stats.idre.ucla.edu/r/seminars/recurrent-events/.\n\nBook: “Survival Analysis” by Klein & Moeschberger (2003, Ch. 12)\n\nExplains PWP models and counting processes.\n\nAccess: Springer or libraries (ISBN: 978-0-387-95399-1)."
  },
  {
    "objectID": "02-07-04-01-survival-analysis-andersen-gill-model-r.html#overview",
    "href": "02-07-04-01-survival-analysis-andersen-gill-model-r.html#overview",
    "title": "4.1 Andersen-Gill (AG) Model",
    "section": "Overview",
    "text": "Overview\nThe AG model assumes that:\n\nEvents follow a non-homogeneous Poisson process.\nThe hazard for the k-th event depends on calendar time (not time since last event).\nAll events from the same subject are conditionally independent given covariates (though robust standard errors account for within-subject correlation).\n\n\nHazard Function\nHazard for the i-th subject at time t:\n\\[\nh_i(t) = h_0(t) \\exp(\\beta^T X_i(t))\n\\]\nWhere:\n\n\\(h_0(t)\\): baseline hazard (common to all events)\n\\(X_i(t)\\): possibly time-varying covariates\nEach subject contributes multiple rows to the dataset (one per event or risk interval)\n\n\n\nKey Assumptions\n\nProportional hazards over calendar time\nEvents are independent conditional on covariates (robust SEs relax this)\nNo terminal event that stops the process (e.g., death may need special handling)\n\nNote: If a terminal event (like death) prevents further recurrences, consider joint modeling or competing risks approaches instead.",
    "crumbs": [
      "**Recurrent Event Models**",
      "Andersen-Gill (AG) Model"
    ]
  },
  {
    "objectID": "02-07-04-01-survival-analysis-andersen-gill-model-r.html#implementation-in-r",
    "href": "02-07-04-01-survival-analysis-andersen-gill-model-r.html#implementation-in-r",
    "title": "4.1 Andersen-Gill (AG) Model",
    "section": "Implementation in R",
    "text": "Implementation in R\nWe’ll use the built-in readmission-like data. Since R doesn’t include a standard recurrent event dataset, we’ll simulate one or use the tcut example from the survival package.\n\nInstall Required R Packages\nFollowing R packages are required to run this notebook. If any of these packages are not installed, you can install them using the code below:\n\n\nCode\npackages &lt;-c(\n         'tidyverse',\n         'survival',\n         'survminer',\n         'ggsurvfit',\n         'tidycmprsk',\n         'ggfortify',\n         'timereg',\n         'cmprsk',\n         'riskRegression',\n         'reda'\n         )\n\n\n{r  #| warning: false #| error: false # Install missing packages new_packages &lt;- packages[!(packages %in% installed.packages()[,\"Package\"])] if(length(new_packages)) install.packages(new_packages) devtools::install_github(\"ItziarI/WeDiBaDis\")\n\n\nCode\n# Verify installation\ncat(\"Installed packages:\\n\")\n\n\nInstalled packages:\n\n\nCode\nprint(sapply(packages, requireNamespace, quietly = TRUE))\n\n\nRegistered S3 method overwritten by 'riskRegression':\n  method        from \n  nobs.multinom broom\n\n\n     tidyverse       survival      survminer      ggsurvfit     tidycmprsk \n          TRUE           TRUE           TRUE           TRUE           TRUE \n     ggfortify        timereg         cmprsk riskRegression           reda \n          TRUE           TRUE           TRUE           TRUE           TRUE \n\n\n\n\nLoad Packages\n\n\nCode\n# Load packages with suppressed messages\ninvisible(lapply(packages, function(pkg) {\n  suppressPackageStartupMessages(library(pkg, character.only = TRUE))\n}))\n\n\n\n\nCode\n# Check loaded packages\ncat(\"Successfully loaded packages:\\n\")\n\n\nSuccessfully loaded packages:\n\n\nCode\nprint(search()[grepl(\"package:\", search())])\n\n\n [1] \"package:reda\"           \"package:riskRegression\" \"package:cmprsk\"        \n [4] \"package:timereg\"        \"package:ggfortify\"      \"package:tidycmprsk\"    \n [7] \"package:ggsurvfit\"      \"package:survminer\"      \"package:ggpubr\"        \n[10] \"package:survival\"       \"package:lubridate\"      \"package:forcats\"       \n[13] \"package:stringr\"        \"package:dplyr\"          \"package:purrr\"         \n[16] \"package:readr\"          \"package:tidyr\"          \"package:tibble\"        \n[19] \"package:ggplot2\"        \"package:tidyverse\"      \"package:stats\"         \n[22] \"package:graphics\"       \"package:grDevices\"      \"package:utils\"         \n[25] \"package:datasets\"       \"package:methods\"        \"package:base\"          \n\n\n\n\nData\nWe use the survival package and bladder1 dataset, which contains recurrent bladder tumor data. The data set contains multiple rows per patient, with start and stop times for each interval, event indicators, and covariates.\nid: Patient id treatment: Placebo, pyridoxine (vitamin B6), or thiotepa number: Initial number of tumours (8=8 or more) size: Size (cm) of largest initial tumour recur: Number of recurrences start,stop: The start and end time of each time interval status: End of interval code, 0=censored, 1=recurrence, 2=death from bladder disease, 3=death other/unknown cause rtumor: Number of tumors found at the time of a recurrence rsize: Size of largest tumor at a recurrence enum: Event number (observation number within patient)\n\n\nCode\n# Load bladder1 (long format)\ndata(bladder1)\n\n\nWarning in data(bladder1): data set 'bladder1' not found\n\n\nCode\nstr(bladder1)\n\n\n'data.frame':   294 obs. of  11 variables:\n $ id       : int  1 2 3 4 5 6 6 7 8 9 ...\n $ treatment: Factor w/ 3 levels \"placebo\",\"pyridoxine\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ number   : int  1 1 2 1 5 4 4 1 1 1 ...\n $ size     : int  1 3 1 1 1 1 1 1 1 3 ...\n $ recur    : int  0 0 0 0 0 1 1 0 0 1 ...\n $ start    : int  0 0 0 0 0 0 6 0 0 0 ...\n $ stop     : int  0 1 4 7 10 6 10 14 18 5 ...\n $ status   : num  3 3 0 0 3 1 3 0 0 1 ...\n $ rtumor   : chr  \".\" \".\" \".\" \".\" ...\n $ rsize    : chr  \".\" \".\" \".\" \".\" ...\n $ enum     : num  1 1 1 1 1 1 2 1 1 1 ...\n\n\n\n\nData Preparation\nWe will create gaptime for PWP-GT and truncate to the first 4 events for stable risk sets:\n\n\nCode\n# Identify problematic rows\ninvalid_intervals &lt;- bladder1[bladder1$stop &lt;= bladder1$start, ]\ninvalid_status &lt;- bladder1[!bladder1$status %in% c(0, 1), ]\n\n# Remove invalid rows\nbladder1_clean &lt;- bladder1 %&gt;%\n  filter(stop &gt; start, status %in% c(0, 1))\n\n# Create gaptime for PWP-GT\nbladder1_clean &lt;- bladder1_clean %&gt;%\n  group_by(id) %&gt;%\n  mutate(gaptime = stop - start) %&gt;%\n  ungroup()\n\n# Truncate to first 4 events for PWP models\nbladder_trunc &lt;- bladder1_clean[bladder1_clean$enum &lt;= 4, ]\n\n# Verify\nhead(bladder_trunc)\n\n\n# A tibble: 6 × 12\n     id treatment number  size recur start  stop status rtumor rsize  enum\n  &lt;int&gt; &lt;fct&gt;      &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;  &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt; &lt;dbl&gt;\n1     3 placebo        2     1     0     0     4      0 .      .         1\n2     4 placebo        1     1     0     0     7      0 .      .         1\n3     6 placebo        4     1     1     0     6      1 1      1         1\n4     7 placebo        1     1     0     0    14      0 .      .         1\n5     8 placebo        1     1     0     0    18      0 .      .         1\n6     9 placebo        1     3     1     0     5      1 2      4         1\n# ℹ 1 more variable: gaptime &lt;int&gt;\n\n\n\n\nFit the Andersen-Gill Model\nUse coxph() with a Surv(tstart, tstop, event) object:\n\n\nCode\n# Fit AG model\nag_model &lt;- coxph(Surv(start, stop, status) ~ treatment + number + size + cluster(id), \n                data = bladder_trunc, robust = TRUE)\n\nsummary(ag_model)\n\n\nCall:\ncoxph(formula = Surv(start, stop, status) ~ treatment + number + \n    size, data = bladder_trunc, robust = TRUE, cluster = id)\n\n  n= 216, number of events= 149 \n\n                         coef exp(coef)  se(coef) robust se      z Pr(&gt;|z|)   \ntreatmentpyridoxine -0.136192  0.872675  0.202938  0.317218 -0.429  0.66768   \ntreatmentthiotepa   -0.375981  0.686615  0.200102  0.261779 -1.436  0.15093   \nnumber               0.164861  1.179229  0.038932  0.055719  2.959  0.00309 **\nsize                 0.008494  1.008531  0.048500  0.070791  0.120  0.90449   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n                    exp(coef) exp(-coef) lower .95 upper .95\ntreatmentpyridoxine    0.8727     1.1459    0.4686     1.625\ntreatmentthiotepa      0.6866     1.4564    0.4110     1.147\nnumber                 1.1792     0.8480    1.0572     1.315\nsize                   1.0085     0.9915    0.8779     1.159\n\nConcordance= 0.631  (se = 0.03 )\nLikelihood ratio test= 17.22  on 4 df,   p=0.002\nWald test            = 9.83  on 4 df,   p=0.04\nScore (logrank) test = 20.06  on 4 df,   p=5e-04,   Robust = 13.34  p=0.01\n\n  (Note: the likelihood ratio and score tests assume independence of\n     observations within a cluster, the Wald and robust score tests do not).\n\n\n\n\nModel Diagnostics\n\nProportional Hazards Assumption\nUse cox.zph():\n\n\nCode\nzph_test &lt;- cox.zph(ag_model)\nprint(zph_test)\n\n\n           chisq df    p\ntreatment 1.9536  2 0.38\nnumber    0.0217  1 0.88\nsize      2.3347  1 0.13\nGLOBAL    4.1808  4 0.38\n\n\nCode\nggcoxzph(zph_test)  # plot\n\n\n\n\n\n\n\n\n\n\nIf p &lt; 0.05 for a covariate, PH assumption may be violated. Consider time-interactions (e.g., tt() function).\n\n\n\nResiduals\nCheck martingale or deviance residuals (less common for recurrent events):\n\n\nCode\nmart_res &lt;- residuals(ag_model, type = \"martingale\", collapse = bladder_trunc$id)\nplot(mart_res ~ bladder_trunc$number[match(names(mart_res), bladder_trunc$id)], \n     xlab = \"Initial Tumors\", ylab = \"Martingale Residuals\")\nabline(h = 0, lty = 2)\n\n\n\n\n\n\n\n\n\n\n\n\nCompute and Plot Cumulative Incidence Function (CIF)\nWhile the AG model estimates hazard ratios, the Cumulative Incidence Function (CIF) shows the expected number of events over time.\nThe Cumulative Incidence Function (CIF) is a key concept in survival analysis, particularly in the context of competing risks—situations where multiple distinct types of events can occur, and the occurrence of one event precludes the others.\nThe Cumulative Incidence Function for a specific event type ( k ) at time ( t ) is defined as:\n\\[\n\\text{CIF}_k(t) = P(T \\leq t \\text{ and event type } = k)\n\\] In words:\n\\(CIF(_k)(t)\\) is the probability that an individual experiences event type \\(k\\) by time \\(t\\), in the presence of other competing event types.\nThis differs from the standard Kaplan-Meier (KM) estimator, which treats all other event types as censored. In competing risks settings, censoring competing events leads to overestimation of the event probability, because it assumes those individuals could still experience the event of interest later—which is not true if a competing event (e.g., death from another cause) has already occurred.\n\n\nCode\nbase_ag &lt;- basehaz(ag_model, centered = FALSE)\nplot(base_ag$hazard ~ base_ag$time, type = \"s\", xlab = \"Time\", ylab = \"Cumulative Hazard\", \n     main = \"CMF by Stratum (AG-Model)\")",
    "crumbs": [
      "**Recurrent Event Models**",
      "Andersen-Gill (AG) Model"
    ]
  },
  {
    "objectID": "02-07-04-01-survival-analysis-andersen-gill-model-r.html#summary-conclusion",
    "href": "02-07-04-01-survival-analysis-andersen-gill-model-r.html#summary-conclusion",
    "title": "4.1 Andersen-Gill (AG) Model",
    "section": "Summary & Conclusion",
    "text": "Summary & Conclusion\nThe Andersen-Gill (AG) model is appropriate when subjects experience multiple events of the same type—such as hospital readmissions or recurrent infections—and there is no terminal event (like death) that permanently stops the event process (or if such an event is handled separately). It is particularly useful when the research question focuses on the event rate over calendar time rather than the time between successive events. Among its strengths, the AG model is a straightforward extension of the standard Cox proportional hazards model, naturally accommodates time-varying covariates, and uses robust (sandwich) standard errors to account for within-subject correlation across recurrent events. However, it has important limitations: it assumes that, conditional on covariates, recurrent events are independent, and it does not explicitly model dependence on event history—such as changes in risk based on the number of prior events or the gap time since the last event. Additionally, the model may be inappropriate when a terminal event truncates follow-up, as it does not inherently account for this competing risk. In such cases, alternative approaches should be considered, including the Prentice-Williams-Peterson (PWP) model (which stratifies by event order and can model gap or total times), frailty models (which incorporate random effects to capture unobserved subject-specific heterogeneity), or joint models that simultaneously analyze recurrent events and associated terminal events like death.",
    "crumbs": [
      "**Recurrent Event Models**",
      "Andersen-Gill (AG) Model"
    ]
  },
  {
    "objectID": "02-07-04-01-survival-analysis-andersen-gill-model-r.html#resources",
    "href": "02-07-04-01-survival-analysis-andersen-gill-model-r.html#resources",
    "title": "4.1 Andersen-Gill (AG) Model",
    "section": "Resources",
    "text": "Resources\nBooks - Modeling Survival Data: Extending the Cox Model by Terry M. Therneau & Patricia M. Grambsch\n- The Statistical Analysis of Recurrent Events by Richard J. Cook & Jerald F. Lawless\nR Packages - survival: Core survival analysis - reda: Recurrent event data analysis (MCF, simulation) - frailtypack: Frailty models for recurrent events\nVignettes & Tutorials - vignette(\"timedep\", package = \"survival\") - vignette(\"reda-MCF\", package = \"reda\") - Therneau’s Survival Analysis Tutorial",
    "crumbs": [
      "**Recurrent Event Models**",
      "Andersen-Gill (AG) Model"
    ]
  },
  {
    "objectID": "02-07-04-02-survival-analysis-prentice-williams-peterson-model-r.html#overview",
    "href": "02-07-04-02-survival-analysis-prentice-williams-peterson-model-r.html#overview",
    "title": "4.2 Prentice-Williams-Peterson (PWP) Models",
    "section": "Overview",
    "text": "Overview\nPrentice-Williams-Peterson (PWP) models are extensions of the Cox proportional hazards model specifically designed for analyzing recurrent event data, where the same type of event (e.g., infections, hospitalizations, or tumor recurrences) can occur multiple times for an individual. Unlike standard survival analysis for single events, PWP models account for the ordering and dependency of repeated events by stratifying the analysis based on the number of prior events. This stratification creates conditional risk sets: all subjects are at risk for the first event, but only those who experienced the first are at risk for the second, and so on.\nPWP models are particularly useful in epidemiology and clinical studies, such as tracking recurrent infections in patients with chronic conditions (e.g., cystic fibrosis or kidney disease) or repeated sports injuries. They allow for event-specific covariate effects, meaning the impact of predictors like treatment or age can vary across event orders. However, if the number of events is large, risk sets for later events may become small, leading to unstable estimates—often requiring truncation (e.g., analyzing only the first 3-4 events).\nThe Andersen-Gill (AG) model is an intensity-based extension of the Cox model for recurrent events, treating the data as a counting process. It models the instantaneous rate (intensity) of events conditional on the event history and covariates, assuming a common baseline hazard across all events:\n\\[\n\\lambda(t | \\bar{N}(t-), Z(t)) = \\lambda_0(t) \\exp(\\beta^T Z(t))\n\\]\nHere, \\(\\lambda_0(t)\\) is the baseline intensity, \\(Z(t)\\) are (possibly time-dependent) covariates, and \\(\\bar{N}(t-)\\) is the event history up to just before t. The AG model assumes that correlations between events are fully explained by included covariates (e.g., a time-dependent covariate for the number of prior events). If not, robust standard errors (via clustering on subject ID) are used to account for within-subject dependence.\nThere are two main variants:\n\nPWP-TT (Total Time or Conditional Risk Set Model): Measures time from study entry to each event, similar to calendar time. It’s suitable when the overall timeline matters, and it assumes a common baseline hazard within each stratum (event order) but allows covariate effects to differ.\nPWP-GT (Gap Time Model): Measures the time between consecutive events (inter-event gaps), resetting the clock after each event. This assumes a renewal process and is ideal for focusing on waiting times between recurrences, such as predicting the time to the next event.\n\n\n\n\n\n\n\n\n\nModel\nTime Scale\nInterpretation\n\n\n\n\nPWP-GT (Gap Time)\nTime since previous event\n“What is the risk of the k-th event given the (k–1)-th occurred?”\n\n\nPWP-TT (Total Time)\nTime since study entry\n“What is the risk of the k-th event at calendar time t?”\n\n\n\nThe PWP model stratifies by event number (1st event, 2nd event, etc.), allowing the baseline hazard to differ across event orders. This acknowledges that the risk of a second event may differ from the first due to biological, behavioral, or mechanical factors.\n\nKey Assumptions\n\nSubjects are not at risk for the k-th event until the (k–1)-th event has occurred.\nThe baseline hazard is unspecified and unique for each event order (handled via stratification).\nCovariate effects (β) are often assumed constant across event orders (can be relaxed).\n\n\n\nWhen to Use PWP\n\nEvent history matters (e.g., risk changes after first recurrence)\n\nInterest in time between events (PWP-GT) or calendar-time risk of ordered events (PWP-TT)\n\nEvents are ordered and of the same type\n\n\nNot suitable if subjects can experience events without prior ones (e.g., simultaneous events).\n\n\n\nStrengths\n\nAccounts for event order via stratification.\nFlexible: allows different baseline hazards per event.\nCan incorporate time-varying covariates.\n\n\n\nLimitations\n\nDoes not model correlation between gap times beyond stratification (frailty models may be better).\nExcludes subjects from risk sets for higher-order events until prior events occur (reduces power).\nInterpretation is conditional on having reached that event order.\n\n\n\nPWP vs. AG Model\n\n\n\n\n\n\n\n\nFeature\nAG Model\nPWP Model\n\n\n\n\nTime scale\nCalendar time\nTT: calendar; GT: gap time\n\n\nEvent dependence\nAssumes independence (robust SEs)\nExplicitly models order via strata\n\n\nAt-risk assumption\nAlways at risk until censoring\nOnly at risk for k-th event after (k–1)-th occurs\n\n\nBest for\nEvent rate over time\nEvent timing/order",
    "crumbs": [
      "**Recurrent Event Models**",
      "Prentice-Williams-Peterson (PWP) Models"
    ]
  },
  {
    "objectID": "02-07-04-02-survival-analysis-prentice-williams-peterson-model-r.html#implementation-in-r",
    "href": "02-07-04-02-survival-analysis-prentice-williams-peterson-model-r.html#implementation-in-r",
    "title": "4.2 Prentice-Williams-Peterson (PWP) Models",
    "section": "Implementation in R",
    "text": "Implementation in R\nWe’ll use simulated recurrent event data and fit both PWP-GT and PWP-TT models using the survival package.\n\nInstall Required R Packages\nFollowing R packages are required to run this notebook. If any of these packages are not installed, you can install them using the code below:\n\n\nCode\npackages &lt;-c(\n         'tidyverse',\n         'survival',\n         'survminer',\n         'ggsurvfit',\n         'tidycmprsk',\n         'ggfortify',\n         'timereg',\n         'cmprsk',\n         'riskRegression',\n         'reda'\n         )\n\n\n{r  #| warning: false #| error: false # Install missing packages new_packages &lt;- packages[!(packages %in% installed.packages()[,\"Package\"])] if(length(new_packages)) install.packages(new_packages) devtools::install_github(\"ItziarI/WeDiBaDis\")\n\n\nCode\n# Verify installation\ncat(\"Installed packages:\\n\")\n\n\nInstalled packages:\n\n\nCode\nprint(sapply(packages, requireNamespace, quietly = TRUE))\n\n\nRegistered S3 method overwritten by 'riskRegression':\n  method        from \n  nobs.multinom broom\n\n\n     tidyverse       survival      survminer      ggsurvfit     tidycmprsk \n          TRUE           TRUE           TRUE           TRUE           TRUE \n     ggfortify        timereg         cmprsk riskRegression           reda \n          TRUE           TRUE           TRUE           TRUE           TRUE \n\n\n\n\nLoad Packages\n\n\nCode\n# Load packages with suppressed messages\ninvisible(lapply(packages, function(pkg) {\n  suppressPackageStartupMessages(library(pkg, character.only = TRUE))\n}))\n\n\n\n\nCode\n# Check loaded packages\ncat(\"Successfully loaded packages:\\n\")\n\n\nSuccessfully loaded packages:\n\n\nCode\nprint(search()[grepl(\"package:\", search())])\n\n\n [1] \"package:reda\"           \"package:riskRegression\" \"package:cmprsk\"        \n [4] \"package:timereg\"        \"package:ggfortify\"      \"package:tidycmprsk\"    \n [7] \"package:ggsurvfit\"      \"package:survminer\"      \"package:ggpubr\"        \n[10] \"package:survival\"       \"package:lubridate\"      \"package:forcats\"       \n[13] \"package:stringr\"        \"package:dplyr\"          \"package:purrr\"         \n[16] \"package:readr\"          \"package:tidyr\"          \"package:tibble\"        \n[19] \"package:ggplot2\"        \"package:tidyverse\"      \"package:stats\"         \n[22] \"package:graphics\"       \"package:grDevices\"      \"package:utils\"         \n[25] \"package:datasets\"       \"package:methods\"        \"package:base\"          \n\n\n\n\nData\nWe use the survival package and bladder1 dataset, which contains recurrent bladder tumor data. The data set contains multiple rows per patient, with start and stop times for each interval, event indicators, and covariates.\nid: Patient id treatment: Placebo, pyridoxine (vitamin B6), or thiotepa number: Initial number of tumours (8=8 or more) size: Size (cm) of largest initial tumour recur: Number of recurrences start,stop: The start and end time of each time interval status: End of interval code, 0=censored, 1=recurrence, 2=death from bladder disease, 3=death other/unknown cause rtumor: Number of tumors found at the time of a recurrence rsize: Size of largest tumor at a recurrence enum: Event number (observation number within patient)\n\n\nCode\n# Load bladder1 (long format)\ndata(bladder1)\n\n\nWarning in data(bladder1): data set 'bladder1' not found\n\n\nCode\nstr(bladder1)\n\n\n'data.frame':   294 obs. of  11 variables:\n $ id       : int  1 2 3 4 5 6 6 7 8 9 ...\n $ treatment: Factor w/ 3 levels \"placebo\",\"pyridoxine\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ number   : int  1 1 2 1 5 4 4 1 1 1 ...\n $ size     : int  1 3 1 1 1 1 1 1 1 3 ...\n $ recur    : int  0 0 0 0 0 1 1 0 0 1 ...\n $ start    : int  0 0 0 0 0 0 6 0 0 0 ...\n $ stop     : int  0 1 4 7 10 6 10 14 18 5 ...\n $ status   : num  3 3 0 0 3 1 3 0 0 1 ...\n $ rtumor   : chr  \".\" \".\" \".\" \".\" ...\n $ rsize    : chr  \".\" \".\" \".\" \".\" ...\n $ enum     : num  1 1 1 1 1 1 2 1 1 1 ...\n\n\n\n\nData Preparation\nWe will create gaptime for PWP-GT and truncate to the first 4 events for stable risk sets:\n\n\nCode\n# Identify problematic rows\ninvalid_intervals &lt;- bladder1[bladder1$stop &lt;= bladder1$start, ]\ninvalid_status &lt;- bladder1[!bladder1$status %in% c(0, 1), ]\n\n# Remove invalid rows\nbladder1_clean &lt;- bladder1 %&gt;%\n  filter(stop &gt; start, status %in% c(0, 1))\n\n# Create gaptime for PWP-GT\nbladder1_clean &lt;- bladder1_clean %&gt;%\n  group_by(id) %&gt;%\n  mutate(gaptime = stop - start) %&gt;%\n  ungroup()\n\n# Truncate to first 4 events for PWP models\nbladder_trunc &lt;- bladder1_clean[bladder1_clean$enum &lt;= 4, ]\n\n# Verify\nhead(bladder_trunc)\n\n\n# A tibble: 6 × 12\n     id treatment number  size recur start  stop status rtumor rsize  enum\n  &lt;int&gt; &lt;fct&gt;      &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;  &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt; &lt;dbl&gt;\n1     3 placebo        2     1     0     0     4      0 .      .         1\n2     4 placebo        1     1     0     0     7      0 .      .         1\n3     6 placebo        4     1     1     0     6      1 1      1         1\n4     7 placebo        1     1     0     0    14      0 .      .         1\n5     8 placebo        1     1     0     0    18      0 .      .         1\n6     9 placebo        1     3     1     0     5      1 2      4         1\n# ℹ 1 more variable: gaptime &lt;int&gt;\n\n\n\n\nModel Fitting\n\nPWP-TT (otal Time, stratified by event order) Model\n\n\nCode\npwp_tt_fit &lt;- coxph(Surv(start, stop, status) ~ treatment + number + size + strata(enum) + cluster(id), \n                    data = bladder_trunc, robust = TRUE)\nsummary(pwp_tt_fit)\n\n\nCall:\ncoxph(formula = Surv(start, stop, status) ~ treatment + number + \n    size + strata(enum), data = bladder_trunc, robust = TRUE, \n    cluster = id)\n\n  n= 216, number of events= 149 \n\n                        coef exp(coef) se(coef) robust se      z Pr(&gt;|z|)  \ntreatmentpyridoxine  0.01842   1.01859  0.21156   0.25150  0.073   0.9416  \ntreatmentthiotepa   -0.21361   0.80766  0.21185   0.18601 -1.148   0.2508  \nnumber               0.10925   1.11544  0.04400   0.04574  2.388   0.0169 *\nsize                 0.01889   1.01907  0.05110   0.05280  0.358   0.7206  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n                    exp(coef) exp(-coef) lower .95 upper .95\ntreatmentpyridoxine    1.0186     0.9818    0.6222     1.668\ntreatmentthiotepa      0.8077     1.2381    0.5609     1.163\nnumber                 1.1154     0.8965    1.0198     1.220\nsize                   1.0191     0.9813    0.9189     1.130\n\nConcordance= 0.615  (se = 0.029 )\nLikelihood ratio test= 6.42  on 4 df,   p=0.2\nWald test            = 6.06  on 4 df,   p=0.2\nScore (logrank) test = 6.91  on 4 df,   p=0.1,   Robust = 8.71  p=0.07\n\n  (Note: the likelihood ratio and score tests assume independence of\n     observations within a cluster, the Wald and robust score tests do not).\n\n\n\n\nPWP-GT (Gap Time, stratified by event order) Model\n\n\nCode\npwp_gt_fit &lt;- coxph(Surv(gaptime, status) ~ treatment + number + size + strata(enum) + cluster(id), \n                    data = bladder_trunc, robust = TRUE)\nsummary(pwp_gt_fit)\n\n\nCall:\ncoxph(formula = Surv(gaptime, status) ~ treatment + number + \n    size + strata(enum), data = bladder_trunc, robust = TRUE, \n    cluster = id)\n\n  n= 216, number of events= 149 \n\n                         coef exp(coef)  se(coef) robust se      z Pr(&gt;|z|)   \ntreatmentpyridoxine -0.003212  0.996793  0.206778  0.229990 -0.014  0.98886   \ntreatmentthiotepa   -0.204154  0.815337  0.205963  0.203142 -1.005  0.31491   \nnumber               0.142757  1.153450  0.043835  0.045034  3.170  0.00152 **\nsize                 0.024727  1.025035  0.049948  0.052969  0.467  0.64063   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n                    exp(coef) exp(-coef) lower .95 upper .95\ntreatmentpyridoxine    0.9968     1.0032    0.6351     1.564\ntreatmentthiotepa      0.8153     1.2265    0.5475     1.214\nnumber                 1.1534     0.8670    1.0560     1.260\nsize                   1.0250     0.9756    0.9240     1.137\n\nConcordance= 0.602  (se = 0.029 )\nLikelihood ratio test= 9.94  on 4 df,   p=0.04\nWald test            = 10.83  on 4 df,   p=0.03\nScore (logrank) test = 11.06  on 4 df,   p=0.03,   Robust = 12.39  p=0.01\n\n  (Note: the likelihood ratio and score tests assume independence of\n     observations within a cluster, the Wald and robust score tests do not).\n\n\n\nNote: - cluster(id) provides robust standard errors. - strata(event_order) allows baseline hazard to vary by event number. - In PWP-GT, the first event is often analyzed separately (as it has no prior gap).\n\n\n\n\nModel Diagnostic\n\nProportional Hazards Check\n\n\nCode\n# For PWP-TT\nzph_tt &lt;- cox.zph(pwp_tt_fit)\nprint(zph_tt)\n\n\n          chisq df    p\ntreatment 0.355  2 0.84\nnumber    0.880  1 0.35\nsize      1.053  1 0.30\nGLOBAL    2.728  4 0.60\n\n\nCode\nggcoxzph(zph_tt)\n\n\nWarning: ggtheme is not a valid theme.\nPlease use `theme()` to construct themes.\nggtheme is not a valid theme.\nPlease use `theme()` to construct themes.\nggtheme is not a valid theme.\nPlease use `theme()` to construct themes.\n\n\n\n\n\n\n\n\n\n\n\nCode\n# For PWP-GT\nzph_gt &lt;- cox.zph(pwp_gt_fit)\nprint(zph_gt)\n\n\n           chisq df    p\ntreatment 2.8188  2 0.24\nnumber    0.0116  1 0.91\nsize      0.2162  1 0.64\nGLOBAL    3.2800  4 0.51\n\n\nCode\nggcoxzph(zph_gt)\n\n\nWarning: ggtheme is not a valid theme.\nPlease use `theme()` to construct themes.\nggtheme is not a valid theme.\nPlease use `theme()` to construct themes.\nggtheme is not a valid theme.\nPlease use `theme()` to construct themes.\n\n\n\n\n\n\n\n\n\n\nIf PH violated for a stratum, consider time-dependent effects or separate models per event order.\n\n\n\nResiduals\nMartingale residuals can be examined per stratum:\n\n\nCode\nres_PWP_tt &lt;- residuals(pwp_tt_fit, type = \"martingale\", collapse = bladder_trunc$id)\nplot(res_PWP_tt ~bladder_trunc$number[match(names(res_PWP_tt), bladder_trunc$id)], \n     xlab = \"Initial Tumors\", \n     ylab = \"Martingale Residuals\",\n     main = \"PWP-TT Model\")\nabline(h = 0, lty = 2)\n\n\n\n\n\n\n\n\n\n\n\nCode\nres_PWP_gt &lt;- residuals(pwp_gt_fit, type = \"martingale\", collapse = bladder_trunc$id)\nplot(res_PWP_gt ~bladder_trunc$number[match(names(res_PWP_gt), bladder_trunc$id)], \n     xlab = \"Initial Tumors\", \n     ylab = \"Martingale Residuals\",\n     main = \"PWP-GT Model\")\nabline(h = 0, lty = 2)\n\n\n\n\n\n\n\n\n\n\n\n\nEstimating and Plotting the Mean Cumulative Function (MCF)\nWhile PWP models estimate hazard ratios per event order, the Mean Cumulative Function (MCF) shows the average number of events per subject over time—the recurrent-event analog of CIF.\nWe’ll use the reda package:\n\n\nCode\nbase_tt &lt;- basehaz(pwp_tt_fit, centered = FALSE)\nplot(base_tt$hazard ~ base_tt$time, type = \"s\", xlab = \"Time\", ylab = \"Cumulative Hazard\", \n     main = \"CMF by Stratum (PWP-TT)\")\n\n\n\n\n\n\n\n\n\n\n\nCode\nbase_gt &lt;- basehaz(pwp_gt_fit, centered = FALSE)\nplot(base_gt$hazard ~ base_gt$time, type = \"s\", xlab = \"Time\", ylab = \"Cumulative Hazard\", \n     main = \"CMF by Stratum (PWP-GT)\")\n\n\n\n\n\n\n\n\n\n\nNote: The MCF is not directly output by PWP models—it’s a descriptive summary of the event process, often used alongside modeling.",
    "crumbs": [
      "**Recurrent Event Models**",
      "Prentice-Williams-Peterson (PWP) Models"
    ]
  },
  {
    "objectID": "02-07-04-02-survival-analysis-prentice-williams-peterson-model-r.html#summary-conclusions",
    "href": "02-07-04-02-survival-analysis-prentice-williams-peterson-model-r.html#summary-conclusions",
    "title": "4.2 Prentice-Williams-Peterson (PWP) Models",
    "section": "Summary & Conclusions",
    "text": "Summary & Conclusions\nThis tutorial demonstrated how to implement Prentice-Williams-Peterson (PWP) models for recurrent event data in R, covering both PWP-TT and PWP-GT variants. Key steps included: data preparation, model fitting with coxph(), diagnostics, and visualization of the Mean Cumulative Function (MCF). This tutorial provides a foundational implementation. For full applications, consult the survival package vignettes or supplementary materials from epidemiological studies.",
    "crumbs": [
      "**Recurrent Event Models**",
      "Prentice-Williams-Peterson (PWP) Models"
    ]
  },
  {
    "objectID": "02-07-04-02-survival-analysis-prentice-williams-peterson-model-r.html#resources",
    "href": "02-07-04-02-survival-analysis-prentice-williams-peterson-model-r.html#resources",
    "title": "4.2 Prentice-Williams-Peterson (PWP) Models",
    "section": "Resources",
    "text": "Resources\n\nBooks\n\nThe Statistical Analysis of Recurrent Events by Cook & Lawless\n\nModeling Survival Data: Extending the Cox Model by Therneau & Grambsch\n\n\n\nR Packages\n\nsurvival: Core Cox models with strata()\nreda: MCF estimation and recurrent event simulation\nfrailtypack: Frailty models for recurrent events\n\n\n\nVignettes & Tutorials\n\nvignette(\"survival\") and vignette(\"timedep\", package = \"survival\")\nvignette(\"reda-intro\", package = \"reda\")\nTherneau’s Advanced Survival Analysis Notes",
    "crumbs": [
      "**Recurrent Event Models**",
      "Prentice-Williams-Peterson (PWP) Models"
    ]
  },
  {
    "objectID": "02-07-05-01-survival-analysis-cause-specific-hazard-regression.html#overview",
    "href": "02-07-05-01-survival-analysis-cause-specific-hazard-regression.html#overview",
    "title": "Cause-Specific Hazard Regression",
    "section": "Overview",
    "text": "Overview\nThis models the cause-specific hazard (the rate of a specific event among those still at risk and event-free) for each competing event separately, typically using Cox proportional hazards models. The CIF is then derived by integrating the cause-specific hazards with the overall survival function. Coefficients represent hazard ratios for the effect of covariates on the event rate. It is ideal for etiologic questions (understanding causal mechanisms) but does not directly model absolute risks, which can lead to indirect interpretations in prediction. For example, in heart failure data, this approach might show no significant effect of cancer on cardiac death hazard.\n\nKey Concepts\n\nCompeting Risks Setting:\n\nIn many real-world scenarios, individuals are at risk of more than one mutually exclusive event.\nExample: In a study of cancer patients, the outcome of interest might be death due to cancer, but patients could also die from heart disease, accidents, or other causes. These are competing risks because the occurrence of one event (e.g., death from heart disease) prevents the observation of the other (e.g., death from cancer).\n\n\n\n\nCause-specific Cox regression\nGiven covariates \\((X, Z)\\), let \\(S_0(t|x, z) = P(T &gt; t|X = x, Z = z)\\) denote the event-free survival function and \\(F_j(t|x, z) = P(T \\leq t, D = j|X = x, Z = z)\\) the cumulative incidence function for event \\(j\\). The cause-specific hazard rates are defined as \\(\\lambda_{j,z}(t|x) = \\frac{dF_j(t|x, z)}{S_0(t|x, z)}\\) (Andersen et al., 1993). We also denote the cumulative hazard rates by \\(\\Lambda_{j,z}(t|x) = \\int_0^t \\lambda_{j,z}(s|x)\\,ds\\). The stratified Cox regression model (Cox, 1972) for cause \\(j\\) is given by\n\\[\n\\lambda_{j,z}(t|x) = \\lambda_{0j,z}(t) \\exp(x\\beta_j), \\quad (1)\n\\]\nwhere\n\\(\\beta_j = (\\beta_{1j}, \\dots, \\beta_{pj})^\\top\\) is a \\(p\\)-dimensional vector of regression coefficients (the log-hazard ratios), and \\(\\{\\lambda_{0j,z}(t) : z = 1, \\dots, L\\}\\) a set of unspecified baseline hazard functions.\n\n\nPredicting the absolute risk of an event\nThe cause-specific Cox regression models can be combined into a prediction of the absolute risk of an event of type 1 until time \\(t\\) conditional on the covariates \\(x, z\\). For the case where \\(K = 2\\) the absolute risk formula of Benichou and Gail (1990) is given by:\n\\[\nF_1(t|x, z) = \\int_0^t S(s- |x, z)\\lambda_{1,z}(s|x)\\,ds. \\quad (2)\n\\]\nwhere \\(s-\\) denotes the right sided limit, e.g. \\(\\Lambda_{1,z}(s^- |x) = \\lim_{v\\to s, v&lt;s} \\Lambda_{1,z}(v|x)\\). The absolute risk accumulates over time the product between the event-free survival and the hazard of experiencing the event of interest, both conditional to the baseline covariates and to the strata variable. The event free survival can be estimated from the cause-specific hazards using the product integral estimator:\n\\[\nS(t|x, z) = \\prod_{s\\leq t} \\left(1 - d\\Lambda_{1,z}(t|x) - d\\Lambda_{2,z}(t|x)\\right)\n\\]\nor the exponential approximation:\n\\[\n\\hat{S}(t|x, z) = \\exp \\left[ -\\hat{\\Lambda}_{1,z}(t|x) - \\hat{\\Lambda}_{2,z}(t|x) \\right]. \\quad\n\\]\nwhich is asymptotically equivalent to the product-limit estimator if the distribution of the event times is continuous. Using the product integral estimator ensures that \\(S(t|x, z) + F_1(t|x, z) + F_2(t|x, z)\\) equals exactly 1. This is a desirable property since the sum of the transition probabilities over all possible transitions should sum to one.\n\n\nAdvantages\n\nDirectly models the biological or clinical mechanism of a specific event type.\nCoefficients have a clear interpretation in terms of instantaneous risk.\nWorks well with standard survival software (e.g., coxph in R, with appropriate censoring coding).\n\n\n\nLimitations\n\nDoes not directly estimate the cumulative incidence (i.e., the actual probability of experiencing the event by time (t)), because it ignores the impact of competing risks on the overall event probability.\nFor predicting absolute risk, Fine-Gray subdistribution hazard models are often preferred.\nResults can be sensitive to assumptions about censoring (e.g., assuming competing events are non-informative).\n\n\n\nExample\nSuppose you study patients after a bone marrow transplant: - Event of interest: relapse of leukemia. - Competing event: death without relapse.\nA cause-specific hazard model for relapse would: - Treat death without relapse as censoring at the time of death. - Estimate how factors (e.g., age, donor match) affect the instantaneous risk of relapse, among patients who have neither relapsed nor died up to that time.",
    "crumbs": [
      "**Risk Regression**",
      "Andersen-Gill (AG) Model"
    ]
  },
  {
    "objectID": "02-07-05-01-survival-analysis-cause-specific-hazard-regression.html#cause-specific-hazard-regression-for-competing-risks-in-r",
    "href": "02-07-05-01-survival-analysis-cause-specific-hazard-regression.html#cause-specific-hazard-regression-for-competing-risks-in-r",
    "title": "Cause-Specific Hazard Regression",
    "section": "Cause-Specific Hazard Regression for Competing Risks in R",
    "text": "Cause-Specific Hazard Regression for Competing Risks in R\nIn competing risks analysis, cause-specific hazard (CSH) regression models the instantaneous risk of a specific event type, treating other event types as censoring. This tutorial demonstrates how to fit, interpret, and visualize CSH models using the Melanoma dataset from the `riskRegression`` package.\n\nInstall Required R Packages\nFollowing R packages are required to run this notebook. If any of these packages are not installed, you can install them using the code below:\n\n\nCode\npackages &lt;-c(\n         'tidyverse',\n         'survival',\n         'survminer',\n         'ggsurvfit',\n         'tidycmprsk',\n         'ggfortify',\n         'timereg',\n         'cmprsk',\n         'condSURV',\n         'riskRegression',\n         'prodlim',\n         'lava',\n         'mstate',\n         'regplot',\n         'cmprskcoxmsm'\n         )\n\n\n#| warning: false\n#| error: false\n\n# Install missing packages\nnew_packages &lt;- packages[!(packages %in% installed.packages()[,\"Package\"])]\nif(length(new_packages)) install.packages(new_packages)\n devtools::install_github(\"ItziarI/WeDiBaDis\")\n\n\nVerify installation\n\n\nCode\n# Verify installation\ncat(\"Installed packages:\\n\")\n\n\nInstalled packages:\n\n\nCode\nprint(sapply(packages, requireNamespace, quietly = TRUE))\n\n\nRegistered S3 method overwritten by 'riskRegression':\n  method        from \n  nobs.multinom broom\n\n\n     tidyverse       survival      survminer      ggsurvfit     tidycmprsk \n          TRUE           TRUE           TRUE           TRUE           TRUE \n     ggfortify        timereg         cmprsk       condSURV riskRegression \n          TRUE           TRUE           TRUE           TRUE           TRUE \n       prodlim           lava         mstate        regplot   cmprskcoxmsm \n          TRUE           TRUE           TRUE           TRUE           TRUE \n\n\n\n\nLoad Packages\n\n\nCode\n# Load packages with suppressed messages\ninvisible(lapply(packages, function(pkg) {\n  suppressPackageStartupMessages(library(pkg, character.only = TRUE))\n}))\n\n\n\n\nCode\n# Check loaded packages\ncat(\"Successfully loaded packages:\\n\")\n\n\nSuccessfully loaded packages:\n\n\nCode\nprint(search()[grepl(\"package:\", search())])\n\n\n [1] \"package:cmprskcoxmsm\"   \"package:regplot\"        \"package:mstate\"        \n [4] \"package:lava\"           \"package:prodlim\"        \"package:riskRegression\"\n [7] \"package:condSURV\"       \"package:cmprsk\"         \"package:timereg\"       \n[10] \"package:ggfortify\"      \"package:tidycmprsk\"     \"package:ggsurvfit\"     \n[13] \"package:survminer\"      \"package:ggpubr\"         \"package:survival\"      \n[16] \"package:lubridate\"      \"package:forcats\"        \"package:stringr\"       \n[19] \"package:dplyr\"          \"package:purrr\"          \"package:readr\"         \n[22] \"package:tidyr\"          \"package:tibble\"         \"package:ggplot2\"       \n[25] \"package:tidyverse\"      \"package:stats\"          \"package:graphics\"      \n[28] \"package:grDevices\"      \"package:utils\"          \"package:datasets\"      \n[31] \"package:methods\"        \"package:base\"          \n\n\n\n\nData Preparation\nn the period 1962-77, 205 patients with malignant melanoma (cancer of the skin) had a radical operation performed at Odense University Hospital, Denmark. All patients were followed until the end of 1977 by which time 134 were still alive while 71 had died (of out whom 57 had died from cancer and 14 from other causes).\ntime: time in days from operation\nstatus: a numeric with values 0=censored 1=death.malignant.melanoma 2=death.other.causes\nevent: a factor with levels censored death.malignant.melanoma death.other.causes\ninvasion: a factor with levels level.0, level.1, level.2\nici: inflammatory cell infiltration (IFI): 0, 1, 2 or 3\nepicel: a factor with levels not present present\nulcer: a factor with levels not present present\nthick: tumour thickness (in 1/100 mm)\nsex: a factor with levels Female Male\nage: age at operation (years)\nlogthick: tumour thickness on log-scale\n\n\nCode\ndata(Melanoma)\nstr(Melanoma)\n\n\n'data.frame':   205 obs. of  11 variables:\n $ time    : int  10 30 35 99 185 204 210 232 232 279 ...\n $ status  : num  2 2 0 2 1 1 1 1 2 1 ...\n $ event   : Factor w/ 3 levels \"censored\",\"death.malignant.melanoma\",..: 3 3 1 3 2 2 2 2 3 2 ...\n $ invasion: Factor w/ 3 levels \"level.0\",\"level.1\",..: 2 1 2 1 3 3 3 3 2 1 ...\n $ ici     : Factor w/ 4 levels \"0\",\"1\",\"2\",\"3\": 3 1 3 3 3 3 3 3 4 3 ...\n $ epicel  : Factor w/ 2 levels \"not present\",..: 2 1 1 1 2 1 2 1 1 1 ...\n $ ulcer   : Factor w/ 2 levels \"not present\",..: 2 1 1 1 2 2 2 2 2 2 ...\n $ thick   : num  6.76 0.65 1.34 2.9 12.08 ...\n $ sex     : Factor w/ 2 levels \"Female\",\"Male\": 2 2 2 1 2 2 2 2 1 1 ...\n $ age     : int  76 56 41 71 52 28 77 49 60 68 ...\n $ logthick: num  1.911 -0.431 0.293 1.065 2.492 ...\n\n\n\n\nEstimation of the cause-specific hazards\nCSC() provides Cause-specific Cox proportional hazard regression with riskRegression utilities. CSC() the argument formula is used to define the outcome variables with the help of the function prodlim::Hist().\nHere is an example of fitting a cause-specific hazard model using riskRegression::CSC()\n\n\nCode\ncfit0 &lt;- CSC(formula = Hist(time,status) ~ age + logthick + epicel + strata(sex), data = Melanoma)\ncoef(cfit0)\n\n\n$`Cause 1`\n          age      logthick epicelpresent \n   0.01548722    0.68178505   -0.73848649 \n\n$`Cause 2`\n          age      logthick epicelpresent \n   0.07680909    0.04750975    0.31497177 \n\n\n*Note**: Hist() is from prodlim (loaded with riskRegression) and handles competing risks event history.\n\n\nCode\nh &lt;- with(Melanoma, prodlim::Hist(time,status))\nh\n\n\n\nRight-censored response of a competing.risks model\n\nNo.Observations: 205 \n\nPattern:\n         \nCause     event right.censored\n  1          57              0\n  2          14              0\n  unknown     0            134\n\n\nIf only one formula is provided, the CSC() function will use the same baseline covariates and strata variables for all cause-specific Cox regression models. Instead one may feed a list of formulas into the argument formula, one for each cause:\n\n\nCode\ncfit1 &lt;- CSC(formula = list(Hist(time,status) ~ age + logthick + epicel + strata(sex), Hist(time,status) ~ age + strata(sex)), data = Melanoma)\ncoef(cfit1)\n\n\n$`Cause 1`\n          age      logthick epicelpresent \n   0.01548722    0.68178505   -0.73848649 \n\n$`Cause 2`\n       age \n0.07919648 \n\n\nNote that the selection of baseline covariates for each cause in this example is not guided by clinical or statistical rationale; it serves solely to demonstrate the software’s capabilities. The causes are automatically ordered based on the levels of the status variable if it is a factor; otherwise, they are sorted using sort(as.character(unique(status))). This ordering is stored in cfit1[[\"causes\"]]. Consequently, the first formula fits a Cox model for the first cause, the second formula for the second cause, and so forth.\nInternally, CSC() generates a dummy variable for each cause and invokes the function specified by the fitter argument on an appropriately formatted Surv() object. By default, cause-specific Cox models are fitted using survival::coxph(). Users can alternatively specify a different fitting function via the fitter argument, such as cph.\n\n\nCode\nplot(h, arrowLabelStyle = \"count\",\nstateLabels = c(\"Radical\\noperation\", \"Cancer\\nrelated death\", \"Death\\nother causes\"))\n\n\n\n\n\n\n\n\n\n\n\nVisualizing Cause-Specific Hazards (absolute risk)\nAbsolute risk (CIF) plots can be created using prodlim package. Absolute risk is the probability of experiencing a specific event by a certain time, accounting for competing risks.\n\n\nCode\nlibrary(prodlim)\nplot(prodlim(Hist(time,status) ~1, data = Melanoma),\natrisk.at = seq(0,3652.5,365.25), xlim = c(0,3652.5),\naxis1.at = seq(0,3652.5,365.25), axis1.lab = 0:10,\nxlab = \"Years\", ylab = \"Absolute risk\", cause = 1)\n\n\n\n\n\n\n\n\n\n\n\nPredicting Absolute Risk from Cause-Specific Hazard Models\nThe object returned by CSC() is of class \"CauseSpecificCox\". The next step involves invoking the associated predict() method. This requires three additional arguments beyond the CSC object: newdata, times, and cause.\n\nnewdata must be a data.frame containing the covariates \\(X\\) and \\(Z\\) in the same structure as the data used to fit the CSC() model.\n\ncause specifies the event type of interest (\\(D\\)).\n\ntimes is a vector of prediction time point(s), each serving as the upper limit \\(t\\) in formula (2).\n\nThe predict() method then calculates the absolute risk (per formula (2)) for each row in newdata and each specified value in times.\n\n\nCode\nnewdata &lt;- data.frame(age = c(45,67), logthick = c(0.1,0.2), epicel = c(\"present\",\"not present\"), sex = c(\"Female\",\"Male\"))\npfit1 &lt;- predict(cfit1, newdata = newdata, cause = 1, times = c(867,3500))\nprint(pfit1)\n\n\n   observation   age logthick      epicel    sex times     strata absRisk\n         &lt;int&gt; &lt;num&gt;    &lt;num&gt;      &lt;char&gt; &lt;char&gt; &lt;num&gt;     &lt;fctr&gt;   &lt;num&gt;\n1:           1    45      0.1     present Female   867 sex=Female   0.021\n2:           2    67      0.2 not present   Male   867   sex=Male   0.149\n3:           1    45      0.1     present Female  3500 sex=Female   0.117\n4:           2    67      0.2 not present   Male  3500   sex=Male   0.428\n\n\nStandard errors and confidence intervals for the absolute risk can be obtained setting the argument se to TRUE:\n\n\nCode\npfit2 &lt;- predict(cfit1, newdata = newdata, cause = 1, times = c(867,3500), se = TRUE)\nprint(pfit2)\n\n\n   observation   age logthick      epicel    sex times     strata absRisk\n         &lt;int&gt; &lt;num&gt;    &lt;num&gt;      &lt;char&gt; &lt;char&gt; &lt;num&gt;     &lt;fctr&gt;   &lt;num&gt;\n1:           1    45      0.1     present Female   867 sex=Female   0.021\n2:           2    67      0.2 not present   Male   867   sex=Male   0.149\n3:           1    45      0.1     present Female  3500 sex=Female   0.117\n4:           2    67      0.2 not present   Male  3500   sex=Male   0.428\n   absRisk.se absRisk.lower absRisk.upper\n        &lt;num&gt;         &lt;num&gt;         &lt;num&gt;\n1:    0.00992       0.00738        0.0478\n2:    0.04582       0.07361        0.2501\n3:    0.03794       0.05552        0.2025\n4:    0.11577       0.20491        0.6348\n\n\nThe components $absRisk, $absRisk.se, $absRisk.lower, and $absRisk.upper are matrices in which each row corresponds to an observation in newdata and each column corresponds to a time point in the times vector. These matrices preserve the original ordering of newdata and times. To easily extract specific subsets of the results, first convert the output using as.data.table.predictCSC(), which combines them into a single \"data.table\" object. Here is an example:\n\n\nCode\nlibrary(data.table)\n\n\n\nAttaching package: 'data.table'\n\n\nThe following objects are masked from 'package:lubridate':\n\n    hour, isoweek, mday, minute, month, quarter, second, wday, week,\n    yday, year\n\n\nThe following objects are masked from 'package:dplyr':\n\n    between, first, last\n\n\nThe following object is masked from 'package:purrr':\n\n    transpose\n\n\nCode\nptable1 &lt;- as.data.table(pfit2)\nptable1[times == 3500 & observation == 1, .(times,absRisk,absRisk.lower,absRisk.upper)]\n\n\n   times   absRisk absRisk.lower absRisk.upper\n   &lt;num&gt;     &lt;num&gt;         &lt;num&gt;         &lt;num&gt;\n1:  3500 0.1166383    0.05552294     0.2025063\n\n\nIn the same way confidence bands can be obtained by setting the argument band to TRUE:\n\n\nCode\nvec.times &lt;- cfit1$eventTimes \npfit1band &lt;- predict(cfit1, newdata = newdata, cause = 1, times = vec.times, se = TRUE, band = TRUE)\n\n\n\n\nCode\nfigure3 &lt;- autoplot(pfit1band, band = TRUE, ci = TRUE)$plot\n\n\nWarning: `aes_string()` was deprecated in ggplot2 3.0.0.\nℹ Please use tidy evaluation idioms with `aes()`.\nℹ See also `vignette(\"ggplot2-in-packages\")` for more information.\nℹ The deprecated feature was likely used in the riskRegression package.\n  Please report the issue at\n  &lt;https://github.com/tagteam/riskRegression/issues&gt;.\n\n\n\n\n\n\n\n\n\nCode\nfigure3 &lt;- figure3 + xlab(\"Time (days)\") + ylab(\"Absolute risk\")\nprint(figure3)",
    "crumbs": [
      "**Risk Regression**",
      "Andersen-Gill (AG) Model"
    ]
  },
  {
    "objectID": "02-07-05-01-survival-analysis-cause-specific-hazard-regression.html#summary-and-conclusions",
    "href": "02-07-05-01-survival-analysis-cause-specific-hazard-regression.html#summary-and-conclusions",
    "title": "Cause-Specific Hazard Regression",
    "section": "Summary and Conclusions",
    "text": "Summary and Conclusions\nCause-Specific Hazard Regression is a foundational tool in competing risks analysis that models the instantaneous risk of a specific event type, treating other events as censoring. It is ideal for understanding etiology or mechanisms but should be complemented with cumulative incidence functions (CIFs) for risk prediction. This tutorial demonstrated how to fit, interpret, and visualize cause-specific hazard models using the riskRegression package in R, providing a practical framework for analyzing competing risks data. At the end of this tutorial, you should be able to:\n\nUnderstand the concept of cause-specific hazards in competing risks\nFit cause-specific hazard models using riskRegression::CSC()\nPredict absolute risks (CIFs) from cause-specific hazard models",
    "crumbs": [
      "**Risk Regression**",
      "Andersen-Gill (AG) Model"
    ]
  },
  {
    "objectID": "02-07-05-01-survival-analysis-cause-specific-hazard-regression.html#resources",
    "href": "02-07-05-01-survival-analysis-cause-specific-hazard-regression.html#resources",
    "title": "Cause-Specific Hazard Regression",
    "section": "Resources",
    "text": "Resources\n\nRosthøj, S., & Keiding, N. (2017). Competing risks: A simple re-parametrization of the cause-specific proportional hazards model. Research Report No. 62, Department of Mathematical Sciences, Aalborg University. https://vbn.aau.dk/ws/portalfiles/portal/271536290/RJ_2017_062.pdf\nPutter, H., Fiocco, M., & Geskus, R. B. (2007). Tutorial in biostatistics: Competing risks and multi-state models. Statistics in Medicine.\n\n3 Andersen, P. K., & Keiding, N. (2012). Interpretability and importance of functionals in competing risks and multi-state models. Statistics in Medicine.",
    "crumbs": [
      "**Risk Regression**",
      "Andersen-Gill (AG) Model"
    ]
  },
  {
    "objectID": "02-07-05-02-survival-analysis-proportional-subdistribution-hazard-regression.html#overview",
    "href": "02-07-05-02-survival-analysis-proportional-subdistribution-hazard-regression.html#overview",
    "title": "Subdistribution Hazard Regression",
    "section": "Overview",
    "text": "Overview\nFor an event of interest (e.g., cause 1), the subdistribution hazard is defined as:\n\\[\n\\lambda_1^{SD}(t \\mid \\mathbf{x}) = \\lim_{\\Delta t \\to 0} \\frac{\\mathbb{P}(t \\leq T \\leq t + \\Delta t, \\text{cause} = 1 \\mid T &gt; t \\text{ or } (T \\leq t \\text{ and cause} \\neq 1), \\mathbf{x})}{\\Delta t}\n\\]\nIn words:\n\nIt’s the instantaneous rate of failing from cause 1 at time t, among those who have not yet experienced cause 1—including individuals who already failed from a competing cause.\n\nThis is not a true hazard (since the risk set includes people who are no longer at risk), but a “pseudo-hazard” designed to link directly to the CIF.\nUnder the Fine–Gray proportional subdistribution hazards model:\n\\[\n\\lambda_1^{SD}(t \\mid \\mathbf{x}) = \\lambda_{10}^{SD}(t) \\exp(\\boldsymbol{\\beta}^\\top \\mathbf{x})\n\\]\nThen the cumulative incidence function is:\n\\[\nF_1(t \\mid \\mathbf{x}) = 1 - \\left[1 - F_1(t \\mid \\mathbf{x} = \\mathbf{0})\\right]^{\\exp(\\boldsymbol{\\beta}^\\top \\mathbf{x})}\n\\]\nThus, covariate effects directly scale the baseline CIF, making interpretation in terms of absolute risk differences more intuitive.\n\nWhen to Use Subdistribution Hazard Regression?\nUse the Fine–Gray model when your goal is:\n\nRisk prediction: estimating absolute risk of an event in the presence of competing risks.\nClinical decision-making: e.g., “What is the 5-year risk of cancer recurrence, given that death from other causes is possible?”\nPublic health planning: understanding population-level burden of specific outcomes.\n\n\nNote: Do NOT use Fine–Gray for etiologic inference (e.g., “Does this gene cause the disease?”). For that, use cause-specific hazards.\n\n\n\nCSH vs. Fine–Gray\n\n\n\n\n\n\n\n\nFeature\nCause-Specific Hazard (CSH)\nSubdistribution Hazard (Fine–Gray)\n\n\n\n\nRisk set\nOnly those still event-free\nThose not yet failed from this cause (includes competing failures)\n\n\nModels\nInstantaneous event rate\nCumulative incidence (absolute risk)\n\n\nUse for\nEtiology, biological mechanisms\nRisk prediction, clinical prognosis\n\n\nInterpretation\n“Effect on rate of event”\n“Effect on absolute risk of event”\n\n\nCensoring competing events?\nYes\nNo (they remain in risk set)",
    "crumbs": [
      "**Risk Regression**",
      "Subdistribution Hazard Regression (Fine-Gray Model)"
    ]
  },
  {
    "objectID": "02-07-05-02-survival-analysis-proportional-subdistribution-hazard-regression.html#subdistribution-hazard-regression-for-competing-risks-in-r",
    "href": "02-07-05-02-survival-analysis-proportional-subdistribution-hazard-regression.html#subdistribution-hazard-regression-for-competing-risks-in-r",
    "title": "Subdistribution Hazard Regression",
    "section": "Subdistribution Hazard Regression for Competing Risks in R",
    "text": "Subdistribution Hazard Regression for Competing Risks in R\nThis tutorial demonstrates Fine-Gray subdistribution hazard modeling for competing risks using four modern R packages:\n\n\n\nPackage\nKey Functions\n\n\n\n\ncmprsk\ncrr() – original Fine-Gray implementation\n\n\ntidycmprsk\ncuminc(), crr() – tidy + ggplot2 friendly\n\n\nriskRegression\nFGR() – integrates with predict() and CSC()\n\n\nprodlim\ncomp.risk() – non-parametric CIF estimation\n\n\n\nWe’ll use the Melanoma dataset (included in riskRegression).\n\nInstall Required R Packages\nFollowing R packages are required to run this notebook. If any of these packages are not installed, you can install them using the code below:\n\n\nCode\npackages &lt;-c(\n         'tidyverse',\n         'survival',\n         'survminer',\n         'ggsurvfit',\n         'tidycmprsk',\n         'ggfortify',\n         'timereg',\n         'cmprsk',\n         'condSURV',\n         'riskRegression',\n         'prodlim',\n         'lava',\n         'mstate',\n         'regplot',\n         'cmprskcoxmsm'\n         )\n\n\n#| warning: false\n#| error: false\n\n# Install missing packages\nnew_packages &lt;- packages[!(packages %in% installed.packages()[,\"Package\"])]\nif(length(new_packages)) install.packages(new_packages)\n devtools::install_github(\"ItziarI/WeDiBaDis\")\n\n\nVerify installation\n\n\nCode\n# Verify installation\ncat(\"Installed packages:\\n\")\n\n\nInstalled packages:\n\n\nCode\nprint(sapply(packages, requireNamespace, quietly = TRUE))\n\n\nRegistered S3 method overwritten by 'riskRegression':\n  method        from \n  nobs.multinom broom\n\n\n     tidyverse       survival      survminer      ggsurvfit     tidycmprsk \n          TRUE           TRUE           TRUE           TRUE           TRUE \n     ggfortify        timereg         cmprsk       condSURV riskRegression \n          TRUE           TRUE           TRUE           TRUE           TRUE \n       prodlim           lava         mstate        regplot   cmprskcoxmsm \n          TRUE           TRUE           TRUE           TRUE           TRUE \n\n\n\n\nLoad Packages\n\n\nCode\n# Load packages with suppressed messages\ninvisible(lapply(packages, function(pkg) {\n  suppressPackageStartupMessages(library(pkg, character.only = TRUE))\n}))\n\n\n\n\nCode\n# Check loaded packages\ncat(\"Successfully loaded packages:\\n\")\n\n\nSuccessfully loaded packages:\n\n\nCode\nprint(search()[grepl(\"package:\", search())])\n\n\n [1] \"package:cmprskcoxmsm\"   \"package:regplot\"        \"package:mstate\"        \n [4] \"package:lava\"           \"package:prodlim\"        \"package:riskRegression\"\n [7] \"package:condSURV\"       \"package:cmprsk\"         \"package:timereg\"       \n[10] \"package:ggfortify\"      \"package:tidycmprsk\"     \"package:ggsurvfit\"     \n[13] \"package:survminer\"      \"package:ggpubr\"         \"package:survival\"      \n[16] \"package:lubridate\"      \"package:forcats\"        \"package:stringr\"       \n[19] \"package:dplyr\"          \"package:purrr\"          \"package:readr\"         \n[22] \"package:tidyr\"          \"package:tibble\"         \"package:ggplot2\"       \n[25] \"package:tidyverse\"      \"package:stats\"          \"package:graphics\"      \n[28] \"package:grDevices\"      \"package:utils\"          \"package:datasets\"      \n[31] \"package:methods\"        \"package:base\"          \n\n\n\n\nData Preparation\nn the period 1962-77, 205 patients with malignant melanoma (cancer of the skin) had a radical operation performed at Odense University Hospital, Denmark. All patients were followed until the end of 1977 by which time 134 were still alive while 71 had died (of out whom 57 had died from cancer and 14 from other causes).\ntime: time in days from operation\nstatus: a numeric with values 0=censored 1=death.malignant.melanoma 2=death.other.causes\nevent: a factor with levels censored death.malignant.melanoma death.other.causes\ninvasion: a factor with levels level.0, level.1, level.2\nici: inflammatory cell infiltration (IFI): 0, 1, 2 or 3\nepicel: a factor with levels not present present\nulcer: a factor with levels not present present\nthick: tumour thickness (in 1/100 mm)\nsex: a factor with levels Female Male\nage: age at operation (years)\nlogthick: tumour thickness on log-scale\n\n\nCode\ndata(Melanoma)\nstr(Melanoma)\n\n\n'data.frame':   205 obs. of  11 variables:\n $ time    : int  10 30 35 99 185 204 210 232 232 279 ...\n $ status  : num  2 2 0 2 1 1 1 1 2 1 ...\n $ event   : Factor w/ 3 levels \"censored\",\"death.malignant.melanoma\",..: 3 3 1 3 2 2 2 2 3 2 ...\n $ invasion: Factor w/ 3 levels \"level.0\",\"level.1\",..: 2 1 2 1 3 3 3 3 2 1 ...\n $ ici     : Factor w/ 4 levels \"0\",\"1\",\"2\",\"3\": 3 1 3 3 3 3 3 3 4 3 ...\n $ epicel  : Factor w/ 2 levels \"not present\",..: 2 1 1 1 2 1 2 1 1 1 ...\n $ ulcer   : Factor w/ 2 levels \"not present\",..: 2 1 1 1 2 2 2 2 2 2 ...\n $ thick   : num  6.76 0.65 1.34 2.9 12.08 ...\n $ sex     : Factor w/ 2 levels \"Female\",\"Male\": 2 2 2 1 2 2 2 2 1 1 ...\n $ age     : int  76 56 41 71 52 28 77 49 60 68 ...\n $ logthick: num  1.911 -0.431 0.293 1.065 2.492 ...\n\n\n\nCompeting risks:\n\n\n\nEvent of interest: death from melanoma (status == 1)\n\nCompeting event: death from other causes (status == 2)\n\n\n\n\nNon-Parametric CIF Estimation (Baseline)\n\n\nCode\n# Recode status for clarity\nMelanoma &lt;- Melanoma %&gt;%\n  mutate(event = case_when(\n    status == 1 ~ \"Melanoma death\",\n    status == 2 ~ \"Other death\",\n    status == 0 ~ \"Censored\"\n  ))\n\n# Non-parametric CIF using prodlim\nfit_prod &lt;- prodlim(Hist(time, event) ~ 1, data = Melanoma)\nsummary(fit_prod)\n\n\n   time          cause n.risk n.event n.lost  cuminc se.cuminc   lower  upper\n1    10       Censored    205       0      0 0.00000   0.00000 0.00000 0.0000\n2  1513       Censored    155       0      0 0.02439   0.01077 0.00327 0.0455\n3  2006       Censored    102       0      0 0.22927   0.02936 0.17172 0.2868\n4  3042       Censored     52       0      0 0.42439   0.03452 0.35673 0.4920\n5  5565       Censored      1       1      0 0.65366       NaN     NaN    NaN\n6    10 Melanoma death    205       0      0 0.00000   0.00000 0.00000 0.0000\n7  1513 Melanoma death    155       0      0 0.18049   0.02686 0.12784 0.2331\n8  2006 Melanoma death    102       0      0 0.22439   0.02914 0.16728 0.2815\n9  3042 Melanoma death     52       0      0 0.26829   0.03095 0.20764 0.3289\n10 5565 Melanoma death      1       0      0 0.27805       NaN     NaN    NaN\n11   10    Other death    205       1      0 0.00488   0.00487 0.00000 0.0144\n12 1513    Other death    155       0      0 0.03902   0.01353 0.01252 0.0655\n13 2006    Other death    102       0      0 0.04878   0.01504 0.01929 0.0783\n14 3042    Other death     52       0      0 0.05366   0.01574 0.02281 0.0845\n15 5565    Other death      1       0      0 0.06829       NaN     NaN    NaN\n\n\n\n\nCode\n# Plot cumulative incidence\nplot(fit_prod,\n     cause = c(\"Melanoma death\", \"Other death\"),\n     atrisk = TRUE,\n     col = c(\"firebrick\", \"steelblue\"),\n     lwd = 2,\n     xlab = \"Days\",\n     ylab = \"Cumulative Incidence\",\n     legend.title = \"Cause\")\n\n\n\n\n\n\n\n\n\n\nShows absolute risk over time without covariates.\n\n\n\nFine-Gray Model with cmprsk::crr()\n\n\nCode\n# Prepare data\nftime   &lt;- Melanoma$time                # failure/censoring time\nfstatus &lt;- Melanoma$status              # 0 = censored, 1 = melanoma, 2 = other\ncov     &lt;- Melanoma %&gt;% \n  select(age, sex, thick, ulcer) %&gt;%    # keep only the covariates you want\n  mutate(\n    sex = as.numeric(sex),              # 0/1 → numeric (already is, but be safe)\n    ulcer = as.numeric(ulcer)\n  )\n\n## any missing values?\nany(is.na(ftime))      # FALSE\n\n\n[1] FALSE\n\n\nCode\nany(is.na(fstatus))    # FALSE\n\n\n[1] FALSE\n\n\nCode\nany(is.na(cov))        # FALSE → if TRUE you must remove/impute rows\n\n\n[1] FALSE\n\n\nCode\n# Fit Fine-Gray model for melanoma death (failcode = 1)\nfit_crr &lt;- crr(ftime = ftime, \n               fstatus = fstatus,\n               cov1 = cov,\n               failcode = 1,     # cause of interest\n               cencode = 0)      # censoring code\n\nsummary(fit_crr)\n\n\nCompeting Risks Regression\n\nCall:\ncrr(ftime = ftime, fstatus = fstatus, cov1 = cov, failcode = 1, \n    cencode = 0)\n\n         coef exp(coef) se(coef)     z p-value\nage   0.00593      1.01  0.00929 0.638  0.5200\nsex   0.40503      1.50  0.27558 1.470  0.1400\nthick 0.08999      1.09  0.03836 2.346  0.0190\nulcer 1.12863      3.09  0.30344 3.719  0.0002\n\n      exp(coef) exp(-coef)  2.5% 97.5%\nage        1.01      0.994 0.988  1.02\nsex        1.50      0.667 0.874  2.57\nthick      1.09      0.914 1.015  1.18\nulcer      3.09      0.323 1.706  5.60\n\nNum. cases = 205\nPseudo Log-likelihood = -268 \nPseudo likelihood ratio test = 35.4  on 4 df,\n\n\n\n\nCode\n# remove all objects from the environment\nrm(list = ls())\ndetach(\"package:cmprsk\", unload = TRUE)\n\n\nWarning: 'cmprsk' namespace cannot be unloaded:\n  namespace 'cmprsk' is imported by 'riskRegression' so cannot be unloaded\n\n\n\n\nFine-Gray with tidycmprsk (Tidy + ggplot2)\n\n\nCode\n# Load data\ndata(Melanoma, package = \"riskRegression\")\n\n# Prepare original data\nMelanoma &lt;- Melanoma %&gt;%\n  filter(complete.cases(.)) %&gt;%\n  mutate(\n    status_factor = factor(status,\n                          levels = c(0, 1, 2),\n                          labels = c(\"censored\", \"melanoma\", \"other\")),\n    profile = \"Observed\"\n  )\n\n\n\n\nCode\nfit_tidy &lt;- tidycmprsk::crr(\n  formula  = Surv(time, status_factor) ~ age + sex + thick + ulcer,\n  data     = Melanoma,\n  failcode = \"melanoma\"   # use label, not number!\n)\n\n# View results\ntidy(fit_tidy)\n\n\n# A tibble: 4 × 5\n  term         estimate std.error statistic p.value\n  &lt;chr&gt;           &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1 age           0.00593   0.00929     0.638  0.52  \n2 sexMale       0.405     0.276       1.47   0.14  \n3 thick         0.0900    0.0384      2.35   0.019 \n4 ulcerpresent  1.13      0.303       3.72   0.0002\n\n\n\n\nPlot CIF for two profiles\n\n\nCode\n# Create newdata with EXACT same types\nnewdata &lt;- tibble(\n  time = 0,\n  status = 0,\n  age = c(50, 50),\n  sex = factor(c(\"M\", \"M\"), levels = c(\"F\", \"M\")),\n  thick = c(1, 6),\n  ulcer = factor(c(\"0\", \"1\"), levels = c(\"0\", \"1\")),\n  status_factor = factor(\"censored\",\n                        levels = c(\"censored\", \"melanoma\", \"other\")),\n  profile = paste(\"Profile\", 1:2)\n)\n\n# Combine\nplot_df &lt;- bind_rows(Melanoma, newdata)\n\n# Cumulative incidence\ncif &lt;- cuminc(\n  Surv(time, status_factor) ~ profile,\n  data = plot_df\n)\n\n# Plot\nggcuminc(cif, outcome = \"melanoma\") +\n  labs(title = \"Predicted CIF: Melanoma Death\", x = \"Days\", y = \"Risk\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\nCode\n# remove all objects from the environment\nrm(list = ls())\n\n\n\n\nFine-Gray with riskRegression::FGR()\n\n\nCode\ndata(Melanoma, package = \"riskRegression\")\nMelanoma &lt;- Melanoma %&gt;%\n  select(time, status, age, sex, thick, ulcer) %&gt;%\n  filter(complete.cases(.))\n\n\n\n\nCode\n# Fit FGR model\nfit_fgr &lt;- FGR(Hist(time, status) ~ age + sex + thick + ulcer,\n               data = Melanoma,\n               cause = 1)\n\nsummary(fit_fgr)\n\n\nCompeting Risks Regression\n\nCall:\nFGR(formula = Hist(time, status) ~ age + sex + thick + ulcer, \n    data = Melanoma, cause = 1)\n\n                coef exp(coef) se(coef)     z p-value\nage          0.00593      1.01  0.00929 0.638  0.5200\nsexMale      0.40503      1.50  0.27558 1.470  0.1400\nthick        0.08999      1.09  0.03836 2.346  0.0190\nulcerpresent 1.12863      3.09  0.30344 3.719  0.0002\n\n             exp(coef) exp(-coef)  2.5% 97.5%\nage               1.01      0.994 0.988  1.02\nsexMale           1.50      0.667 0.874  2.57\nthick             1.09      0.914 1.015  1.18\nulcerpresent      3.09      0.323 1.706  5.60\n\nNum. cases = 205\nPseudo Log-likelihood = -268 \nPseudo likelihood ratio test = 35.4  on 4 df,\n\n\nSame coefficients as crr() – confirms consistency.\n\n\nPredict Absolute Risk\n\n\nCode\n# Define two patient profiles\nnew_patients &lt;- data.frame(\n  age = c(45, 65),\n  sex = c(0, 1),\n  thick = c(0.8, 4.5),\n  ulcer = c(0, 1)\n)\n\n# Predict CIF at 5 years (1825 days) and 10 years (3650 days)\npred &lt;- predict(fit_fgr, \n                newdata = new_patients,\n                times = c(1825, 3650),\n                cause = 1)\n\npred\n\n\n           [,1]      [,2]\n[1,] 0.08105545 0.1326066\n[2,] 0.45958341 0.6450363\n\n\n\n\nCompare Cause-Specific vs Fine-Gray\n\n\nCode\n# Cause-specific Cox for melanoma death\nfit_csc1 &lt;- coxph(Surv(time, status == 1) ~ age + sex + thick + ulcer, data = Melanoma)\n\n# Cause-specific Cox for other death\nfit_csc2 &lt;- coxph(Surv(time, status == 2) ~ age + sex + thick + ulcer, data = Melanoma)\n\n# Combine into CSC object\ncsc &lt;- CSC(\n  list(\n    melanoma = Hist(time, status) ~ age + sex + thick + ulcer,\n    other    = Hist(time, status) ~ age + sex + thick + ulcer\n  ),\n  data = Melanoma\n)\ncoef(csc)\n\n\n$`Cause 1`\n         age      sexMale        thick ulcerpresent \n  0.01219844   0.43281709   0.10894525   1.16447890 \n\n$`Cause 2`\n         age      sexMale        thick ulcerpresent \n  0.07255223   0.35801080   0.04958006   0.10936653 \n\n\n\n\nCode\nnew_patients &lt;- tibble(\n  age   = c(50, 50),\n  sex   = factor(c(\"Male\", \"Male\"),\n                 levels = levels(Melanoma$sex)),          # \"Female\",\"Male\"\n  thick = c(1, 6),\n  ulcer = factor(c(\"not present\", \"present\"),\n                 levels = levels(Melanoma$ulcer))        # \"not present\",\"present\"\n)\n\n\n\n\nCode\npred_csc &lt;- predict(\n  csc,\n  newdata = new_patients,\n  times   = 1825,\n  cause   = 1,                    # melanoma death\n  product.limit = TRUE\n)\n\npred_csc\n\n\n   observation   age    sex thick       ulcer times absRisk\n         &lt;int&gt; &lt;num&gt; &lt;fctr&gt; &lt;num&gt;      &lt;fctr&gt; &lt;num&gt;   &lt;num&gt;\n1:           1    50   Male     1 not present  1825   0.120\n2:           2    50   Male     6     present  1825   0.503\n\n\n\n\nPlot cumulative incidence for the two profiles\n\n\nCode\n# Build plot_df \n\nplot_df &lt;- bind_rows(\n  Melanoma %&gt;% \n    mutate(\n      status_factor = factor(\n        status,\n        levels = c(0, 1, 2),\n        labels = c(\"censored\", \"melanoma\", \"other\")\n      ),\n      profile = \"Observed\"\n    ),\n  new_patients %&gt;% \n    mutate(\n      time = 0,\n      status = 0,  # keep original numeric if you want, but not used\n      status_factor = factor(\n        \"censored\",\n        levels = c(\"censored\", \"melanoma\", \"other\")\n      ),\n      profile = paste(\"Profile\", 1:2)\n    )\n)\n\n# Cumulative incidence\ncif &lt;- cuminc(\n  Surv(time, status_factor) ~ profile,\n  data = plot_df\n)\n\n# Plot\n\nggcuminc(cif, outcome = \"melanoma\") +\n  labs(\n    title = \"Predicted Risk of Melanoma Death (5 Years)\",\n    x = \"Days\", y = \"Cumulative Incidence\"\n  ) +\n  theme_minimal()",
    "crumbs": [
      "**Risk Regression**",
      "Subdistribution Hazard Regression (Fine-Gray Model)"
    ]
  },
  {
    "objectID": "02-07-05-02-survival-analysis-proportional-subdistribution-hazard-regression.html#summary-and-conclusion",
    "href": "02-07-05-02-survival-analysis-proportional-subdistribution-hazard-regression.html#summary-and-conclusion",
    "title": "Subdistribution Hazard Regression",
    "section": "Summary and Conclusion",
    "text": "Summary and Conclusion\nFine and Gray’s subdistribution hazard model is a powerful tool for analyzing competing risks data, allowing for direct modeling of cumulative incidence functions. By using R packages like cmprsk, tidycmprsk, and riskRegression, researchers can effectively estimate and interpret absolute risks in the presence of competing events. This approach is particularly useful for clinical decision-making and public health planning, where understanding the absolute risk of specific outcomes is crucial. This tutorial has provided a comprehensive overview of the Fine-Gray model, its implementation in R, and practical examples using the Melanoma dataset.",
    "crumbs": [
      "**Risk Regression**",
      "Subdistribution Hazard Regression (Fine-Gray Model)"
    ]
  },
  {
    "objectID": "02-07-05-02-survival-analysis-proportional-subdistribution-hazard-regression.html#resources",
    "href": "02-07-05-02-survival-analysis-proportional-subdistribution-hazard-regression.html#resources",
    "title": "Subdistribution Hazard Regression",
    "section": "Resources",
    "text": "Resources\n\nFine JP, Gray RJ (1999). A Proportional Hazards Model for the Subdistribution of a Competing Risk. JASA.\nriskRegression vignette: https://cran.r-project.org/package=riskRegression\ntidycmprsk: https://mskcc-epi-chevo.github.io/tidycmprsk/",
    "crumbs": [
      "**Risk Regression**",
      "Subdistribution Hazard Regression (Fine-Gray Model)"
    ]
  },
  {
    "objectID": "02-07-05-04-survival-analysis-aalen-model-r.html#overview",
    "href": "02-07-05-04-survival-analysis-aalen-model-r.html#overview",
    "title": "5.4 Aalen’s Additive Regression Model",
    "section": "Overview",
    "text": "Overview\nThe Aalen’s Additive regression model, also known as the Aalen’s additive hazards model, is a statistical technique used in survival analysis. Unlike the more commonly used Cox Proportional Hazards model, which assumes a multiplicative relationship between the covariates and the hazard function, the Aalen model assumes an additive relationship between the covariates and the hazard function. This model is particularly useful when the proportional hazards assumption of the Cox model does not hold, meaning that the effects of covariates on survival are not constant over time.\nKey Concepts of the Aalen Model:\n\nAdditive Hazards:\n\nIn the Aalen model, the hazard at time \\(t\\) is expressed as the sum of contributions from the covariates. The contribution of each covariate to the hazard function can change over time, making the model more flexible than the Cox model.\nThe hazard function \\(h(t)\\)is expressed as an additive function of the covariates.\n\nTime-Varying Coefficients:\n\nThe Aalen model allows for time-varying effects of the covariates, meaning that the effect of each covariate on the hazard can change over time. This is an important feature that differentiates it from other survival models like the Cox model, which assumes constant covariate effects.\n\nNo Proportional Hazards Assumption:\n\nThe model does not rely on the proportional hazards assumption, making it more appropriate for situations where the covariates’ effects vary over time.\n\nInterpretation:\n\nThe coefficients in the Aalen model represent the incremental effects of the covariates on the hazard function at each time point. These coefficients are not hazard ratios, as in the Cox model, but are rather additive effects.\n\n\nThe Aalen additive hazards model can be written as:\n\\[ h(t|X) = h_0(t) + \\beta_1(t) X_1 + \\beta_2(t) X_2 + \\dots + \\beta_p(t) X_p \\]\nWhere:\n\n\\(h(t|X)\\) is the hazard at time \\(t\\) for an individual with covariates \\(X_1, X_2, \\dots, X_p\\).\n\\(h_0(t)\\) is the baseline hazard function at time \\(t\\).\n\\(\\beta_1(t), \\beta_2(t), \\dots, \\beta_p(t)\\) are the time-varying coefficients for each covariate \\(X_1, X_2, \\dots, X_p\\), indicating how the effect of each covariate on the hazard changes over time.\n\nKey Differences Between the Aalen Model and Cox Model:\n\nAdditive vs. Multiplicative:\n\nAalen model: Assumes an additive effect of covariates on the hazard function.\nCox model: Assumes a multiplicative effect of covariates on the hazard function.\n\nTime-Varying Coefficients:\n\nAalen model: Allows for time-varying effects of covariates (i.e., the effect of a covariate can change over time).\nCox model: Assumes the effects of covariates (hazard ratios) are constant over time.\n\nProportional Hazards Assumption:\n\nAalen model: Does not require the proportional hazards assumption.\nCox model: Requires the proportional hazards assumption (the effect of covariates on the hazard is proportional over time).\n\nInterpretation:\n\nAalen model: Coefficients represent additive contributions to the hazard rate, which vary over time.\nCox model: Coefficients represent multiplicative hazard ratios, which are assumed to be constant over time.\n\n\nApplications of the Aalen Model:\n\nTime-Varying Effects:\n\nThe Aalen model is particularly useful when the effects of covariates are expected to change over time. For example, in medical research, the effect of a treatment may change as patients progress through different stages of their disease.\n\nExploratory Analysis:\n\nIt is often used for exploratory analysis to check whether the proportional hazards assumption holds. If the covariate effects appear to be time-varying in the Aalen model, it suggests that the Cox model may not be appropriate.\n\nFlexible Survival Modeling:\n\nThe model provides more flexibility than the Cox model, especially when the effects of covariates are not constant over time.\n\n\nAdvantages of the Aalen Model:\n\nFlexibility:\n\nThe model allows for time-varying covariate effects, making it useful when the proportional hazards assumption does not hold.\n\nExploratory Tool:\n\nIt can be used to explore time-varying effects of covariates and identify whether a simpler Cox model is appropriate.\n\nNo Proportional Hazards Assumption:\n\nIt does not require the proportional hazards assumption, making it more robust in certain settings.\n\n\nLimitations:\n\nInterpretability:\n\nThe coefficients are harder to interpret compared to hazard ratios in the Cox model, especially because they are time-varying and represent additive effects.\n\nLess Popular:\n\nThe Aalen model is less commonly used than the Cox model, which means there is less software support and fewer resources available for interpreting results.\n\nLess Efficient:\n\nIf the true underlying model is multiplicative (as assumed by the Cox model), the Aalen model may be less efficient in estimating the effects of covariates.",
    "crumbs": [
      "**Risk Regression**",
      "Aalen's Additive Regression Model"
    ]
  },
  {
    "objectID": "02-07-05-04-survival-analysis-aalen-model-r.html#aalen-model-in-r",
    "href": "02-07-05-04-survival-analysis-aalen-model-r.html#aalen-model-in-r",
    "title": "5.4 Aalen’s Additive Regression Model",
    "section": "Aalen Model in R",
    "text": "Aalen Model in R\nThis tutorial is mostly used two R packages {survival}, and {timereg}.\n\nInstall Required R Packages\nFollowing R packages are required to run this notebook. If any of these packages are not installed, you can install them using the code below:\n\n\nCode\npackages &lt;-c(\n         'tidyverse',\n         'report',\n         'performance',\n         'gtsummary',\n         'MASS',\n         'epiDisplay',\n         'survival',\n         'survminer',\n         'ggsurvfit',\n         'tidycmprsk',\n         'ggfortify',\n         'timereg',\n         'cmprsk',\n         'condSURV',\n         'riskRegression'\n         )\n\n\n#| warning: false\n#| error: false\n\n# Install missing packages\nnew_packages &lt;- packages[!(packages %in% installed.packages()[,\"Package\"])]\nif(length(new_packages)) install.packages(new_packages)\n \ndevtools::install_github(\"ItziarI/WeDiBaDis\")\n\n\nVerify Installation\n\n\nCode\n# Verify installation\ncat(\"Installed packages:\\n\")\n\n\nInstalled packages:\n\n\nCode\nprint(sapply(packages, requireNamespace, quietly = TRUE))\n\n\nRegistered S3 method overwritten by 'rms':\n  method       from      \n  print.lrtest epiDisplay\n\n\nRegistered S3 method overwritten by 'riskRegression':\n  method        from \n  nobs.multinom broom\n\n\n     tidyverse         report    performance      gtsummary           MASS \n          TRUE           TRUE           TRUE           TRUE           TRUE \n    epiDisplay       survival      survminer      ggsurvfit     tidycmprsk \n          TRUE           TRUE           TRUE           TRUE           TRUE \n     ggfortify        timereg         cmprsk       condSURV riskRegression \n          TRUE           TRUE           TRUE           TRUE           TRUE \n\n\n\n\nLoad Packages\n\n\nCode\n# Load packages with suppressed messages\ninvisible(lapply(packages, function(pkg) {\n  suppressPackageStartupMessages(library(pkg, character.only = TRUE))\n}))\n\n\n\n\nCode\n# Check loaded packages\ncat(\"Successfully loaded packages:\\n\")\n\n\nSuccessfully loaded packages:\n\n\nCode\nprint(search()[grepl(\"package:\", search())])\n\n\n [1] \"package:riskRegression\" \"package:condSURV\"       \"package:cmprsk\"        \n [4] \"package:timereg\"        \"package:ggfortify\"      \"package:tidycmprsk\"    \n [7] \"package:ggsurvfit\"      \"package:survminer\"      \"package:ggpubr\"        \n[10] \"package:epiDisplay\"     \"package:nnet\"           \"package:survival\"      \n[13] \"package:foreign\"        \"package:MASS\"           \"package:gtsummary\"     \n[16] \"package:performance\"    \"package:report\"         \"package:lubridate\"     \n[19] \"package:forcats\"        \"package:stringr\"        \"package:dplyr\"         \n[22] \"package:purrr\"          \"package:readr\"          \"package:tidyr\"         \n[25] \"package:tibble\"         \"package:ggplot2\"        \"package:tidyverse\"     \n[28] \"package:stats\"          \"package:graphics\"       \"package:grDevices\"     \n[31] \"package:utils\"          \"package:datasets\"       \"package:methods\"       \n[34] \"package:base\"          \n\n\n\n\nData\nIn this example, we will perform a Aalen additive hazards model using the veteran dataset from the {survival} package.\n\n\nCode\ndata(veteran)\n\n\nWarning in data(veteran): data set 'veteran' not found\n\n\nCode\nglimpse(veteran)\n\n\nRows: 137\nColumns: 8\n$ trt      &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ celltype &lt;fct&gt; squamous, squamous, squamous, squamous, squamous, squamous, s…\n$ time     &lt;dbl&gt; 72, 411, 228, 126, 118, 10, 82, 110, 314, 100, 42, 8, 144, 25…\n$ status   &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0…\n$ karno    &lt;dbl&gt; 60, 70, 60, 60, 70, 20, 40, 80, 50, 70, 60, 40, 30, 80, 70, 6…\n$ diagtime &lt;dbl&gt; 7, 5, 3, 9, 11, 5, 10, 29, 18, 6, 4, 58, 4, 9, 11, 3, 9, 2, 4…\n$ age      &lt;dbl&gt; 69, 64, 38, 63, 65, 49, 69, 68, 43, 70, 81, 63, 63, 52, 48, 6…\n$ prior    &lt;dbl&gt; 0, 10, 0, 10, 10, 0, 10, 0, 0, 0, 0, 10, 0, 10, 10, 0, 0, 0, …\n\n\nThe dataset contains information such as:\n\ntime: Survival time in days.\nstatus: Censoring indicator (1 = death, 0 = censored).\ntrt: Treatment group (1 = standard treatment, 2 = test treatment).\nage: Age of the patient.\ncelltype: Cell type of lung cancer.\nkarno: Karnofsky performance score (higher is better).\ndiagtime: Time since diagnosis in months.\nprior: Number of prior treatments.\n\n\n\nFit a Aalen Model using {survival} package\naareg() function of {survival} package using the survival time (time), censoring indicator (status), and several covariates such as age, trt, celltype, and karno,\n\n\nCode\nfit.aamodel &lt;-aareg(Surv(time, status) ~  age + trt + celltype + karno , \n                 data = veteran)\nsummary(fit.aamodel)\n\n\n                      slope      coef se(coef)      z        p\nIntercept          0.067600  3.74e-02 1.06e-02  3.540 4.00e-04\nage               -0.000249 -6.86e-05 1.28e-04 -0.537 5.91e-01\ntrt                0.006840  2.54e-03 2.62e-03  0.971 3.32e-01\ncelltypesmallcell  0.015100  6.64e-03 3.40e-03  1.950 5.07e-02\ncelltypeadeno      0.023500  1.04e-02 4.18e-03  2.490 1.27e-02\ncelltypelarge     -0.000918  3.87e-04 2.85e-03  0.136 8.92e-01\nkarno             -0.001250 -4.54e-04 8.81e-05 -5.160 2.52e-07\n\nChisq=40.27 on 6 df, p=4.02e-07; test weights=aalen\n\n\nThe summary of the model shows the time-varying estimates for each covariate (e.g., age, treatment, cell type, and Karnofsky score).\n\nIf the estimates for a covariate remain roughly constant over time, it suggests that its effect on the hazard is constant.\nIf the estimates for a covariate change over time, it indicates that the covariate has a time-varying effect on survival.\n\n\n\nCode\naareg(Surv(time, status) ~  age + trt + celltype + karno , \n                 data = veteran) |&gt;  \n  tbl_regression(exp = TRUE) \n\n\n\n\n\n\n\n\n\nCharacteristic\nexp(Beta)\n95% CI\np-value\n\n\n\n\nIntercept\n1.04\n1.02, 1.06\n&lt;0.001\n\n\nage\n1.00\n1.00, 1.00\n0.6\n\n\ntrt\n1.00\n1.00, 1.01\n0.3\n\n\ncelltypesmallcell\n1.01\n1.00, 1.01\n0.051\n\n\ncelltypeadeno\n1.01\n1.00, 1.02\n0.013\n\n\ncelltypelarge\n1.00\n1.0, 1.01\n0.9\n\n\nkarno\n1.00\n1.00, 1.00\n&lt;0.001\n\n\n\nAbbreviation: CI = Confidence Interval\n\n\n\n\n\n\n\n\n\nWe can visualize how the cumulative regression coefficients (i.e., the effect of each covariate) change over time by plotting the model.\n\n\nCode\nautoplot(fit.aamodel)\n\n\n\n\n\n\n\n\n\nThe plot shows how the cumulative effect of each covariate on the hazard function changes over time. This allows us to see whether the covariate effects are constant or time-varying.\n\nAge: If the cumulative coefficient for age increases over time, it suggests that older patients have a progressively higher risk of death as time goes on.\nTreatment (trt): The plot for trt will show how the effect of the treatment group changes over time. If the test treatment has a decreasing or flat cumulative coefficient, it suggests a positive effect on survival.\nKarnofsky score (karno): If the cumulative coefficient for karno decreases, it suggests that patients with a higher Karnofsky score (better performance status) have a reduced risk of death.\n\n\n\nFit a Aalen Model using {timereg} package\naalen() function of {timereg} package fits both the additive hazards model of Aalen and the semi-parametric additive hazards model of McKeague and Sasieni. Estimates are un-weighted. Time dependent variables and counting process data (multiple events per subject) are possible.\nmax.time: end of observation period where estimates are computed. Estimates thus computed from [start.time, max.time]. Default is max of data.\nn.sim: number of simulations in resampling.\n\n\nCode\n# Fit the Aalen additive hazards model\nfit.aalenreg &lt;- aalen(Surv(time, status) ~ age + trt + celltype + karno, \n                      data = veteran,\n                      max.time=30, # \n                      n.sim=1000)\n\n# Print the summary of the model\nsummary(fit.aalenreg)\n\n\nAdditive Aalen Model \n\nTest for nonparametric terms \n\nTest for non-significant effects \n                  Supremum-test of significance p-value H_0: B(t)=0\n(Intercept)                                4.40               0.000\nage                                        2.26               0.157\ntrt                                        1.76               0.457\ncelltypesmallcell                          1.66               0.408\ncelltypeadeno                              1.60               0.460\ncelltypelarge                              2.22               0.144\nkarno                                      5.37               0.000\n\nTest for time invariant effects \n                        Kolmogorov-Smirnov test p-value H_0:constant effect\n(Intercept)                             0.49700                       0.215\nage                                     0.00421                       0.419\ntrt                                     0.11300                       0.167\ncelltypesmallcell                       0.21600                       0.022\ncelltypeadeno                           0.07560                       0.805\ncelltypelarge                           0.12500                       0.068\nkarno                                   0.00279                       0.552\n                          Cramer von Mises test p-value H_0:constant effect\n(Intercept)                            1.210000                       0.343\nage                                    0.000131                       0.341\ntrt                                    0.101000                       0.134\ncelltypesmallcell                      0.533000                       0.011\ncelltypeadeno                          0.033100                       0.816\ncelltypelarge                          0.116000                       0.053\nkarno                                  0.000082                       0.289\n\n   \n   \n  Call: \naalen(formula = Surv(time, status) ~ age + trt + celltype + karno, \n    data = veteran, max.time = 30, n.sim = 1000)\n\n\nTo visualize how the effect of each covariate changes over time, you can plot the cumulative regression coefficients.\n\n\nCode\npar(mfrow=c(2,4))\nplot(fit.aalenreg)\n\n\n\n\n\n\n\n\n\n\n\nInterpretation of the Results\n\nKarnofsky Score (karno): A negative time-varying coefficient indicates that better performance status (higher Karnofsky score) is associated with a lower hazard of death. The cumulative coefficient shows how this effect accumulates over time.\nTreatment (trt): A positive or negative time-varying coefficient for the treatment group indicates whether the test treatment increases or decreases the hazard of death compared to the standard treatment.\nTime-Varying Effects: If the coefficients for certain covariates change over time, it means their effects on survival are not constant, which might suggest that the effect of these covariates varies depending on how long a patient has survived.",
    "crumbs": [
      "**Risk Regression**",
      "Aalen's Additive Regression Model"
    ]
  },
  {
    "objectID": "02-07-05-04-survival-analysis-aalen-model-r.html#summary-and-conclusions",
    "href": "02-07-05-04-survival-analysis-aalen-model-r.html#summary-and-conclusions",
    "title": "5.4 Aalen’s Additive Regression Model",
    "section": "Summary and Conclusions",
    "text": "Summary and Conclusions\nThe Aalen additive hazards model is a flexible tool in survival analysis, allowing for time-varying effects of covariates on the hazard function. It provides an alternative to the Cox model, particularly when the assumption of proportional hazards does not hold. While the model’s interpretability may be more complex than the Cox model, it is a valuable tool for exploring time-dependent relationships in survival data.",
    "crumbs": [
      "**Risk Regression**",
      "Aalen's Additive Regression Model"
    ]
  },
  {
    "objectID": "02-07-05-04-survival-analysis-aalen-model-r.html#references",
    "href": "02-07-05-04-survival-analysis-aalen-model-r.html#references",
    "title": "5.4 Aalen’s Additive Regression Model",
    "section": "References",
    "text": "References\n\nAalen, O.O. (1989). A linear regression model for the analysis of life times. Statistics in Medicine, 8:907-925.\nAalen, O.O (1993). Further results on the non-parametric linear model in survival analysis. Statistics in Medicine. 12:1569-1588.\nSurvival Analysis with R",
    "crumbs": [
      "**Risk Regression**",
      "Aalen's Additive Regression Model"
    ]
  },
  {
    "objectID": "02-07-05-03-survival-analysis-absolute-risk-regression-r.html#overview",
    "href": "02-07-05-03-survival-analysis-absolute-risk-regression-r.html#overview",
    "title": "5.3 Absolute Risk Regression (Direct CIF Modeling)",
    "section": "Overview",
    "text": "Overview\nIn competing risks survival data, an individual can experience multiple types of events — e.g., death from melanoma vs. death from other causes. The Absolute Risk Regression (ARR) model directly models the Cumulative Incidence Function (CIF) — that is, the absolute probability of a specific event by time t, given covariates. Unlike hazard models (Cox or Fine–Gray), ARR gives direct probabilities, which are easier to interpret in clinical and public health contexts. In competing risks, individuals may fail from one of several mutually exclusive causes (e.g., death from cancer vs. death from heart disease).\n\nWhat is Absolute Risk?\nImagine you’re trying to predict the chance of someone developing a certain disease in the next 10 years.\nRelative Risk: If Group A has a relative risk of 2 compared to Group B, it means people in Group A are twice as likely to develop the disease. However, if the baseline risk in Group B is very low (e.g., 1 in 10,000), then being “twice as likely” still means a very small absolute risk (2 in 10,000).\nAbsolute Risk: This is the actual probability. For example, “There is a 5% chance that this individual will develop the disease in the next 10 years.” This is often more intuitive and directly relevant for patients and clinicians.\nIn contrast to the Cox proportional hazards model, which estimates relative risk (how much more likely one group is to experience an event compared to another), absolute risk models estimate the actual risk (or hazard) on an absolute scale, often expressed as:\n\nInstantaneous event rate (hazard) at time t, or\n\nCumulative incidence: the probability of experiencing the event by time t.\n\nThis makes them especially useful in clinical decision-making, public health planning, and personalized risk prediction, where knowing the actual risk (e.g., “You have a 15% chance of recurrence in 5 years”) is more actionable than a relative measure (e.g., “Your risk is 2× higher”).\nIn survival analysis with competing risks, absolute risk is quantified by the Cumulative Incidence Function (CIF) for event type (k):\n\\[\nF_k(t) = P(T \\leq t, \\text{cause} = k)\n\\]\nDirect CIF modeling regresses \\(F_k(t)\\) directly on covariates \\(\\mathbf{X}\\), bypassing hazard modeling. A common approach uses pseudo-values:\n\nCompute jackknife pseudo-values of the nonparametric CIF at fixed time points \\(t_1, \\dots, t_K\\):\n\n\\[\n   \\hat{\\theta}_{i}(t) = n \\hat{F}_k(t) - (n-1) \\hat{F}_{k,-i}(t)\n\\] where \\(\\hat{F}_{k,-i}(t)\\) is the CIF leaving out subject \\(i\\).\n\nRegress pseudo-values on \\(\\mathbf{X}\\) using generalized estimating equations (GEE):\n\n\\[\n   g\\big( E[\\hat{\\theta}_{i}(t) \\mid \\mathbf{X}_i] \\big) = \\alpha(t) + \\boldsymbol{\\beta}(t)^\\top \\mathbf{X}_i\n\\] where \\(g(\\cdot)\\) is a link function (e.g., identity or logit).\n\n\nAdvantages\n\nDirect clinical interpretation: “This treatment reduces absolute risk by 3% over 5 years.”\nNo proportional hazards assumption (in time-varying versions).\nBetter for heterogeneous populations where baseline risk varies widely.\nNaturally accommodates time-varying effects.\n\n\n\nChallenges\n\nThe hazard \\(\\lambda(t \\mid \\mathbf{X})\\) must remain non-negative—additive models can sometimes predict negative hazards if covariate effects are large (requires careful checking).\nLess familiar to many researchers compared to Cox models.\nSoftware implementation is less standardized (though timereg, riskRegression, and cmprsk in R support them).",
    "crumbs": [
      "**Risk Regression**",
      "Absolute Risk Regression (Direct CIF Modeling)"
    ]
  },
  {
    "objectID": "02-07-05-03-survival-analysis-absolute-risk-regression-r.html#absolute-risk-regression-in-r",
    "href": "02-07-05-03-survival-analysis-absolute-risk-regression-r.html#absolute-risk-regression-in-r",
    "title": "5.3 Absolute Risk Regression (Direct CIF Modeling)",
    "section": "Absolute Risk Regression in R",
    "text": "Absolute Risk Regression in R\nWe’ll use the Melanoma dataset from the riskRegression package. This dataset contains survival data on patients with malignant melanoma, including competing risks of death from melanoma vs. other causes.\n\nInstall Required R Packages\nFollowing R packages are required to run this notebook. If any of these packages are not installed, you can install them using the code below:\n\n\nCode\npackages &lt;-c(\n         'tidyverse',\n         'survival',\n         'survminer',\n         'ggsurvfit',\n         'tidycmprsk',\n         'ggfortify',\n         'timereg',\n         'cmprsk',\n         'condSURV',\n         'riskRegression',\n         'prodlim',\n         'lava',\n         'mstate',\n         'regplot',\n         'cmprskcoxmsm',\n         'timereg'\n         )\n\n\n#| warning: false\n#| error: false\n\n# Install missing packages\nnew_packages &lt;- packages[!(packages %in% installed.packages()[,\"Package\"])]\nif(length(new_packages)) install.packages(new_packages)\n devtools::install_github(\"ItziarI/WeDiBaDis\")\n\n\nVerify installation\n\n\nCode\n# Verify installation\ncat(\"Installed packages:\\n\")\n\n\nInstalled packages:\n\n\nCode\nprint(sapply(packages, requireNamespace, quietly = TRUE))\n\n\nRegistered S3 method overwritten by 'riskRegression':\n  method        from \n  nobs.multinom broom\n\n\n     tidyverse       survival      survminer      ggsurvfit     tidycmprsk \n          TRUE           TRUE           TRUE           TRUE           TRUE \n     ggfortify        timereg         cmprsk       condSURV riskRegression \n          TRUE           TRUE           TRUE           TRUE           TRUE \n       prodlim           lava         mstate        regplot   cmprskcoxmsm \n          TRUE           TRUE           TRUE           TRUE           TRUE \n       timereg \n          TRUE \n\n\n\n\nLoad Packages\n\n\nCode\n# Load packages with suppressed messages\ninvisible(lapply(packages, function(pkg) {\n  suppressPackageStartupMessages(library(pkg, character.only = TRUE))\n}))\n\n\n\n\nCode\n# Check loaded packages\ncat(\"Successfully loaded packages:\\n\")\n\n\nSuccessfully loaded packages:\n\n\nCode\nprint(search()[grepl(\"package:\", search())])\n\n\n [1] \"package:cmprskcoxmsm\"   \"package:regplot\"        \"package:mstate\"        \n [4] \"package:lava\"           \"package:prodlim\"        \"package:riskRegression\"\n [7] \"package:condSURV\"       \"package:cmprsk\"         \"package:timereg\"       \n[10] \"package:ggfortify\"      \"package:tidycmprsk\"     \"package:ggsurvfit\"     \n[13] \"package:survminer\"      \"package:ggpubr\"         \"package:survival\"      \n[16] \"package:lubridate\"      \"package:forcats\"        \"package:stringr\"       \n[19] \"package:dplyr\"          \"package:purrr\"          \"package:readr\"         \n[22] \"package:tidyr\"          \"package:tibble\"         \"package:ggplot2\"       \n[25] \"package:tidyverse\"      \"package:stats\"          \"package:graphics\"      \n[28] \"package:grDevices\"      \"package:utils\"          \"package:datasets\"      \n[31] \"package:methods\"        \"package:base\"          \n\n\n\n\nData Preparation\nn the period 1962-77, 205 patients with malignant melanoma (cancer of the skin) had a radical operation performed at Odense University Hospital, Denmark. All patients were followed until the end of 1977 by which time 134 were still alive while 71 had died (of out whom 57 had died from cancer and 14 from other causes).\ntime: time in days from operation\nstatus: a numeric with values 0=censored 1=death.malignant.melanoma 2=death.other.causes\nevent: a factor with levels censored death.malignant.melanoma death.other.causes\ninvasion: a factor with levels level.0, level.1, level.2\nici: inflammatory cell infiltration (IFI): 0, 1, 2 or 3\nepicel: a factor with levels not present present\nulcer: a factor with levels not present present\nthick: tumour thickness (in 1/100 mm)\nsex: a factor with levels Female Male\nage: age at operation (years)\nlogthick: tumour thickness on log-scale\n\n\nCode\ndata(Melanoma, package = \"riskRegression\")\nhead(Melanoma)\n\n\n  time status                    event invasion ici      epicel       ulcer\n1   10      2       death.other.causes  level.1   2     present     present\n2   30      2       death.other.causes  level.0   0 not present not present\n3   35      0                 censored  level.1   2 not present not present\n4   99      2       death.other.causes  level.0   2 not present not present\n5  185      1 death.malignant.melanoma  level.2   2     present     present\n6  204      1 death.malignant.melanoma  level.2   2 not present     present\n  thick    sex age   logthick\n1  6.76   Male  76  1.9110229\n2  0.65   Male  56 -0.4307829\n3  1.34   Male  41  0.2926696\n4  2.90 Female  71  1.0647107\n5 12.08   Male  52  2.4915512\n6  4.84   Male  28  1.5769147\n\n\n\n\nPrepare Data and Event Definition\n\n\nCode\n# Define event of interest: death from melanoma\nMelanoma$event &lt;- ifelse(Melanoma$status == 1, 1, \n                  ifelse(Melanoma$status == 2, 2, 0))\nMelanoma$event &lt;- factor(Melanoma$event, levels = c(0,1,2),\n                         labels = c(\"censored\", \"melanoma\", \"other\"))",
    "crumbs": [
      "**Risk Regression**",
      "Absolute Risk Regression (Direct CIF Modeling)"
    ]
  },
  {
    "objectID": "02-07-05-03-survival-analysis-absolute-risk-regression-r.html#absolute-risk-regression-with-timereg",
    "href": "02-07-05-03-survival-analysis-absolute-risk-regression-r.html#absolute-risk-regression-with-timereg",
    "title": "5.3 Absolute Risk Regression (Direct CIF Modeling)",
    "section": "Absolute Risk Regression with timereg",
    "text": "Absolute Risk Regression with timereg\nThe timereg package fits semiparametric additive hazard models for survival data, allowing covariate effects to be time-varying (Aalen model) or constant (Lin–Ying model). Unlike Cox regression, it models absolute risk (hazard differences), not relative risk, and does not require proportional hazards. Ideal for flexible, interpretable survival analysis with time-dependent effects.\nInstall: install.packages(\"timereg\").\n\nFit Absolute Risk Model\nUnlike Cox models (which model relative risk), absolute risk models directly model the instantaneous risk (hazard) as a sum of baseline hazard and covariate effects:\n\\[\n\\lambda(t | X) = \\lambda_0(t) + \\beta_1 X_1 + \\beta_2 X_2 + \\dots\n\\]\nThis model assumes the hazard is the sum of a time-varying baseline and linear covariate effects. comp.risk() from timereg fits such models for competing risks data.\nIn the timereg package, the comp.risk() function supports several models for absolute risk (cumulative incidence) regression in competing risks. These are specified via the model = argument.\nBy default, model = \"additive\" (logit-additive), but you can choose:\n\n\n\n\n\n\n\n\n\nModel\nmodel =\nLink Function\nInterpretation\n\n\n\n\nAdditive (logit)\n\"additive\"\nlogit(p) = α(t) + Xᵀβ(t)\nDefault, most common\n\n\nProportional\n\"prop\"\np(t\\|X) = 1 - [1 - p₀(t)]^{exp(Xᵀβ)}\nProportional CIFs\n\n\nRelative cumulative incidence (rcif)\n\"rcif\"\nlog(-log(1-p(t\\|X))) = log(-log(1-p₀(t))) + Xᵀβ\nFine-Gray–like on log-cum hazard scale\n\n\nLogistic\n\"logistic\"\nlogit(p(t\\|X)) = logit(p₀(t)) + Xᵀβ\nProportional odds on CIF\n\n\n\n\n\nmodel = \"additive\" — Additive on Logit Scale (Default)\n\\[\n\\text{logit}(P(T \\leq t, D=1 \\mid \\mathbf{X})) = \\text{logit}(P_0(t)) + \\mathbf{X}^T \\boldsymbol{\\beta}(t)\n\\] - Baseline CIF: \\(P_0(t)\\) is the cumulative incidence at baseline. - Covariates additively shift the logit of CIF over time. - Interpretation: β(t) = change in log-odds of event by time t per unit increase in covariate.\n\n\nCode\n# Fit absolute risk model\nfit_abs &lt;- comp.risk(\n  Event(time, event) ~ const(age) + const(sex) + const(thick),\n  data = Melanoma,\n  cause = 1,                     # event of interest\n  model = \"additive\",            # additive on logit scale\n  n.sim = 1000,                  # for confidence bands\n  resample.iid = 1\n)\n\nsummary(fit_abs)\n\n\nCompeting risks Model \n\nTest for nonparametric terms \n\nTest for non-significant effects \n            Supremum-test of significance p-value H_0: B(t)=0\n(Intercept)                          5.59                   0\n\nTest for time invariant effects \n                  Kolmogorov-Smirnov test p-value H_0:constant effect\n(Intercept)                         0.269                           0\n                    Cramer von Mises test p-value H_0:constant effect\n(Intercept)                          35.6                           0\n\nParametric terms : \n                   Coef.       SE Robust SE     z    P-val lower2.5% upper97.5%\nconst(age)      9.26e-07 7.93e-07  7.93e-07  1.17 2.43e-01 -6.28e-07   2.48e-06\nconst(sex)Male -4.10e-05 2.84e-05  2.84e-05 -1.44 1.50e-01 -9.67e-05   1.47e-05\nconst(thick)   -1.62e-05 2.97e-06  2.97e-06 -5.46 4.90e-08 -2.20e-05  -1.04e-05\n   \n\n\n\n\nCode\n# Coefficient estimates (on logit scale)\nfit_abs$gamma\n\n\n                        [,1]\nconst(age)      9.262402e-07\nconst(sex)Male -4.096830e-05\nconst(thick)   -1.619786e-05\n\n\nCode\n# Standard errors\nfit_abs$se.gamma\n\n\nNULL\n\n\nCode\n# Cumulative baseline (intercept function)\nplot(fit_abs, score = 0, xlab = \"Time (years)\", \n     ylab = \"Logit cumulative baseline\", main = \"Baseline Logit Risk\")\n\n\n\n\n\n\n\n\n\n\n\nmodel = \"prop\" — Proportional Absolute Risk\n\\[\nP(T \\leq t, D=1 \\mid \\mathbf{X}) = 1 - \\left[1 - P_0(t)\\right]^{\\exp(\\mathbf{X}^T \\boldsymbol{\\beta})}\n\\]\n\nBaseline CIF: \\(P_0(t)\\) is the cumulative incidence at baseline.\nCovariates scale the survival probability from the event of interest.\nInterpretation: A positive β means higher absolute risk at all times.\n\n\n\nCode\nfit_prop &lt;- comp.risk(\n  Event(time, event) ~ const(age) + const(sex),\n  data = Melanoma,\n  cause = 1,\n  model = \"prop\",          # &lt;-- proportional model\n  n.sim = 1000\n)\n\nsummary(fit_prop)\n\n\nCompeting risks Model \n\nTest for nonparametric terms \n\nTest for non-significant effects \n            Supremum-test of significance p-value H_0: B(t)=0\n(Intercept)                          6.93                   0\n\nTest for time invariant effects \n                  Kolmogorov-Smirnov test p-value H_0:constant effect\n(Intercept)                          2.23                           0\n                    Cramer von Mises test p-value H_0:constant effect\n(Intercept)                          2530                           0\n\nParametric terms : \n                   Coef.      SE Robust SE      z P-val lower2.5% upper97.5%\nconst(age)      0.000604 0.00557   0.00557  0.108 0.914   -0.0103    0.01150\nconst(sex)Male -0.394000 0.20200   0.20200 -1.950 0.051   -0.7900    0.00191\n   \n\n\n\n\nCode\n# Plot baseline CIF:\nplot(fit_prop, score = 0, xlab = \"Years\", ylab = \"Baseline CIF\", \n     main = \"Proportional Model: Baseline Risk\")\n\n\n\n\n\n\n\n\n\n\nUse when: You assume proportionality of CIFs (like proportional hazards, but for CIF).\n\n\n\nmodel = \"rcif\" — Relative Cumulative Incidence Function\n\\[\n\\log\\left(-\\log(1 - F_1(t|\\mathbf{X}))\\right) = \\log\\left(-\\log(1 - F_{10}(t))\\right) + \\mathbf{X}^T \\boldsymbol{\\beta}\n\\]\n\nEquivalent to Fine-Gray model on the subdistribution hazard scale.\nBut here, directly models CIF.\nInterpretation: exp(β) = subdistribution hazard ratio.\n\n\n\nCode\nfit_rcif &lt;- comp.risk(\n  Event(time, event) ~ const(age) + const(thick),\n  data = Melanoma,\n  cause = 1,\n  model = \"rcif\",          # &lt;-- Fine-Gray–like\n  n.sim = 1000\n)\n\nsummary(fit_rcif)\n\n\nCompeting risks Model \n\nTest for nonparametric terms \n\nTest for non-significant effects \n            Supremum-test of significance p-value H_0: B(t)=0\n(Intercept)                          7.37                   0\n\nTest for time invariant effects \n                  Kolmogorov-Smirnov test p-value H_0:constant effect\n(Intercept)                          2.01                           0\n                    Cramer von Mises test p-value H_0:constant effect\n(Intercept)                          1320                           0\n\nParametric terms : \n                Coef.      SE Robust SE      z    P-val lower2.5% upper97.5%\nconst(age)    0.00177 0.00364   0.00364  0.488 6.26e-01  -0.00536     0.0089\nconst(thick) -0.17100 0.04080   0.04080 -4.190 2.77e-05  -0.25100    -0.0910\n   \n\n\n\nNote: This is very similar to cmprsk::crr(), but allows time-varying effects.\n\n\nUse when: You want Fine-Gray interpretation but with flexible time-varying effects.\n\n\n\nmodel = \"logistic\" — Logistic (Proportional Odds on CIF)\n\\[\n\\text{logit}(F_1(t|\\mathbf{X})) = \\text{logit}(F_{10}(t)) + \\mathbf{X}^T \\boldsymbol{\\beta}\n\\]\n\nProportional odds for the cumulative incidence.\nCovariates shift the logit of CIF by a constant.\n\n\n\nCode\nfit_logistic &lt;- comp.risk(\n  Event(time, event) ~ const(sex) + const(thick),\n  data = Melanoma,\n  cause = 1,\n  model = \"logistic\",      # &lt;-- logistic model\n  n.sim = 1000\n)\n\nsummary(fit_logistic)\n\n\nCompeting risks Model \n\nTest for nonparametric terms \n\nTest for non-significant effects \n            Supremum-test of significance p-value H_0: B(t)=0\n(Intercept)                          6.29                   0\n\nTest for time invariant effects \n                  Kolmogorov-Smirnov test p-value H_0:constant effect\n(Intercept)                           2.5                           0\n                    Cramer von Mises test p-value H_0:constant effect\n(Intercept)                          4230                           0\n\nParametric terms : \n                Coef.     SE Robust SE     z    P-val lower2.5% upper97.5%\nconst(sex)Male -0.250 0.2810    0.2810 -0.89 0.374000    -0.801      0.301\nconst(thick)   -0.274 0.0746    0.0746 -3.67 0.000241    -0.420     -0.128\n   \n\n\n\nUse when: You want proportional odds assumption on the absolute risk.\n\n\n\nWhich Model to Choose?\n\n\n\n\n\n\n\n\n\n\nModel\nmodel =\nAssumption\nInterpretation\nBest For\n\n\n\n\nAdditive\n\"additive\"\nAdditive on logit scale\nFlexible, no proportionality\nPrediction, time-varying effects\n\n\nProportional\n\"prop\"\nCIFs are proportional\nexp(β) = ratio of (1 - S)\nRisk stratification\n\n\nRCIF\n\"rcif\"\nProportional subdistribution hazard\nexp(β) = subdistribution HR\nCompare with Fine-Gray\n\n\nLogistic\n\"logistic\"\nProportional odds on CIF\nOdds ratio for cumulative risk\nClinical interpretation",
    "crumbs": [
      "**Risk Regression**",
      "Absolute Risk Regression (Direct CIF Modeling)"
    ]
  },
  {
    "objectID": "02-07-05-03-survival-analysis-absolute-risk-regression-r.html#absolute-risk-regression-with-riskregression-package",
    "href": "02-07-05-03-survival-analysis-absolute-risk-regression-r.html#absolute-risk-regression-with-riskregression-package",
    "title": "5.3 Absolute Risk Regression (Direct CIF Modeling)",
    "section": "Absolute Risk Regression with ’riskRegression` package",
    "text": "Absolute Risk Regression with ’riskRegression` package\nThe riskRegression package provides functions for fitting various absolute risk regression models, including the Additive Hazards Model and Cumulative Incidence Regression for competing risks data. It allows for flexible modeling of absolute risk (CIF) directly, with options for time-varying effects and different link functions. This is a wrapper for the function comp.risk from the timeregpackage. The main difference is one marks variables in the formula that should have a time-dependent effect whereas in comp.risk one marks variables that should have a time constant (proportional) effect.\nARR function fits absolute risk regression models for competing risks data, directly modeling the cumulative incidence function (CIF) for a specific event type. It allows for flexible covariate effects and provides interpretable estimates of absolute risk.\n\nAbsolute Risk Regression - single binary factor\n\n\nCode\n# absolute risk regression - single binary factor\nfit.arr &lt;- ARR(Hist(time,status)~sex,data=Melanoma,cause=1)\nprint(fit.arr)\n\n\nCompeting risks regression model \n\nIPCW weights: marginal Kaplan-Meier for the censoring distribution.\nLink: 'log' yielding absolute risk ratios\nNo covariates with time-varying coefficient specified.\n\nTime constant regression coefficients:\n Variable Levels Coef Lower Upper  Pvalue\n      sex   Male 1.87  1.17  3.01 0.00927\n\n\nNote: The coefficients (Coef) are absolute risk ratios \n\n\n\n\nCode\n# show predicted cumulative incidences\nplot(fit.arr,col=3:4,newdata=data.frame(sex=c(\"Female\",\"Male\")))\n\n\n\n\n\n\n\n\n\n\n\nAbsolute Risk Regression - single continous variable\n\n\nCode\n# Single continuous factor\n## tumor thickness on the log-scale\nMelanoma$logthick &lt;- log(Melanoma$thick)\n\n## absolute risk regression \nfit2.arr &lt;- ARR(Hist(time,status)~logthick,data=Melanoma,cause=1)\nprint(fit2.arr)\n\n\nCompeting risks regression model \n\nIPCW weights: marginal Kaplan-Meier for the censoring distribution.\nLink: 'log' yielding absolute risk ratios\nNo covariates with time-varying coefficient specified.\n\nTime constant regression coefficients:\n Variable Levels Coef Lower Upper Pvalue\n logthick        1.87  1.49  2.35 &lt;1e-04\n\n\nNote: The coefficients (Coef) are absolute risk ratios \n\n\n\n\nCode\n# show predicted cumulative incidences\nplot(fit2.arr,col=1:5,newdata=data.frame(logthick=quantile(Melanoma$logthick)))\n\n\n\n\n\n\n\n\n\n\n\nAbsolute Risk Regression - multiple covariates\n\n\nCode\n# absolute risk model with multiple covariates\nmulti.arr &lt;- ARR(Hist(time,status)~logthick+sex+age+ulcer,data=Melanoma,cause=1)\nprint(multi.arr)\n\n\nCompeting risks regression model \n\nIPCW weights: marginal Kaplan-Meier for the censoring distribution.\nLink: 'log' yielding absolute risk ratios\nNo covariates with time-varying coefficient specified.\n\nTime constant regression coefficients:\n Variable  Levels Coef Lower Upper  Pvalue\n logthick         1.56 1.156  2.10 0.00358\n      sex    Male 1.28 0.809  2.03 0.28973\n      age         1.00 0.986  1.01 0.99484\n    ulcer present 2.48 1.349  4.57 0.00348\n\n\nNote: The coefficients (Coef) are absolute risk ratios \n\n\n\n\nAbsolute risk regression - stratified model\nStratified model allow different baseline hazards for different levels of a factor variable.\n\n\nCode\n# stratified model allowing different baseline risk for the two gender\nmulti.arr &lt;- ARR(Hist(time,status)~thick+strata(sex)+age+ulcer,data=Melanoma,cause=1)\nprint(multi.arr)\n\n\nCompeting risks regression model \n\nIPCW weights: marginal Kaplan-Meier for the censoring distribution.\nLink: 'log' yielding absolute risk ratios\nCovariates with time-varying effects:\n\n sexMale (numeric)\n\nTime constant regression coefficients:\n Variable  Levels Coef Lower Upper   Pvalue\n    thick         1.08 1.026  1.13 0.002219\n      age         1.00 0.986  1.02 0.956937\n    ulcer present 3.08 1.733  5.49 0.000129\n\n\nNote: The coefficients (Coef) are absolute risk ratios \n\n\n\n\nLogistic Absolute Risk Regression\nLRR function fits logistic absolute risk regression models for competing risks data, directly modeling the cumulative incidence function (CIF) for a specific event type using a logistic link. It provides interpretable estimates of absolute risk in terms of odds ratios.\n\n\nCode\n# logistic absolute risk regression\nfit.lrr &lt;- LRR(Hist(time,status)~thick,data=Melanoma,cause=1)\nsummary(fit.lrr)\n\n\n\nriskRegression: Competing risks regression model \n\nIPCW estimation. The weights are based on\nthe Kaplan-Meier estimate for the censoring distribution.\n\nLink function: 'logistic' yielding odds ratios, see help(riskRegression).\n\nCovariates with time-varying effects:\n\n Intercept (numeric)\n\nThe effects of these variables depend on time.The column 'Intercept' is the baseline risk where all the covariates have value zero\n\n     (Intercept)\n230  \"1.007\"    \n770  \"1.050\"    \n1100 \"1.098\"    \n1700 \"1.158\"    \n3300 \"1.347\"    \n\nShown are selected time points, use\n\nplot.riskRegression\n\nto investigate the full shape.\n\n\nCovariates with time-constant effects:\n\n thick (numeric)\n\nTime constant regression coefficients:\n\n Factor   Coef exp(Coef) StandardError      z         CI_95  Pvalue\n  thick 0.1942    1.2143        0.0477 4.0690 [1.106;1.333] &lt; 1e-04\n\n\nNote: The values exp(Coef) are odds ratios",
    "crumbs": [
      "**Risk Regression**",
      "Absolute Risk Regression (Direct CIF Modeling)"
    ]
  },
  {
    "objectID": "02-07-05-03-survival-analysis-absolute-risk-regression-r.html#summary-and-conclusion",
    "href": "02-07-05-03-survival-analysis-absolute-risk-regression-r.html#summary-and-conclusion",
    "title": "5.3 Absolute Risk Regression (Direct CIF Modeling)",
    "section": "Summary and Conclusion",
    "text": "Summary and Conclusion\nAbsolute Risk Regression (Direct CIF Modeling) directly estimates the probability of event occurrence by a given time under competing risks. It’s especially valuable for clinical decision-making, risk communication, and prediction, where absolute probabilities (not just relative hazards) matter.",
    "crumbs": [
      "**Risk Regression**",
      "Absolute Risk Regression (Direct CIF Modeling)"
    ]
  },
  {
    "objectID": "02-07-05-03-survival-analysis-absolute-risk-regression-r.html#resources",
    "href": "02-07-05-03-survival-analysis-absolute-risk-regression-r.html#resources",
    "title": "5.3 Absolute Risk Regression (Direct CIF Modeling)",
    "section": "Resources",
    "text": "Resources\n\nGerds, T. A., & Andersen, P. K. (2012). Absolute risk regression for competing risks: interpretation, link functions, and prediction. Statistics in Medicine, 31(29), 3921–3930.\nriskRegression package documentation (CRAN): https://cran.r-project.org/package=riskRegression\nMartinussen, T., & Scheike, T. H. (2006). Dynamic Regression Models for Survival Data. Springer.\ntimereg package vignette: vignette(\"timereg\")\nriskRegression package documentation",
    "crumbs": [
      "**Risk Regression**",
      "Absolute Risk Regression (Direct CIF Modeling)"
    ]
  },
  {
    "objectID": "02-07-06-01-survival-analysis-joint-modeling-r.html#overview",
    "href": "02-07-06-01-survival-analysis-joint-modeling-r.html#overview",
    "title": "6.1 Standard (Shared Random Effects) Joint Model",
    "section": "Overview",
    "text": "Overview\nIn a mixed-effects longitudinal model, random effects (e.g., random intercepts/slopes) represent individual deviations from the population average trajectory. In a joint model, these same random effects are included in the survival submodel to explain differences in event risk. This “sharing” induces correlation between the longitudinal and event processes.\nThe standard joint model consists of two submodels:\nLongitudinal Submodel Typically a Linear Mixed-Effects Model)\n\\[\ny_i(t) = m_i(t) + \\varepsilon_i(t) = \\underbrace{x_i^\\top(t) \\beta}_{\\text{Fixed effects}} + \\underbrace{z_i^\\top(t) b_i}_{\\text{Random effects}} + \\varepsilon_i(t)\n\\]\n\n\\(y_i(t)\\): observed longitudinal outcome for subject \\(i\\) at time \\(t\\)\n\\(x_i(t)\\): vector of fixed-effect covariates (e.g., time, treatment)\n\\(\\beta\\): fixed-effect coefficients\n\\(z_i(t)\\): design vector for random effects (often includes intercept and time)\n\\(b_i \\sim \\mathcal{N}(0, D)\\): subject-specific random effects (e.g., random intercept \\(b_{0i}\\), random slope \\(b_{1i}\\)\n\\(\\varepsilon_i(t) \\sim \\mathcal{N}(0, \\sigma^2)\\): measurement error, independent of \\(b_i\\))\n\nThe true (unobserved) longitudinal trajectory is:\n\\[\nm_i(t) = x_i^\\top(t)\\beta + z_i^\\top(t)b_i\n\\]\nSurvival (Event Time) Submodel (Typically a relative risk model with baseline hazard)\n\\[\nh_i(t) = h_0(t) \\exp\\left( w_i^\\top \\gamma + \\alpha \\cdot \\eta\\big(m_i(t), b_i, t\\big) \\right)\n\\]\n\n\\(h_i(t)\\): hazard of the event for subject \\(i\\) at time \\(t\\)\n\\(h_0(t)\\): baseline hazard function (can be parametric like Weibull, or semi-parametric using splines)\n$w_i ): vector of baseline/time-independent covariates (e.g., age, treatment group)\n\\(gamma\\): log-hazard ratios for covariates\n\\(\\alpha\\): association parameter — quantifies how the longitudinal process affects event risk\n\\(\\eta(\\cdot)\\)): association structure linking longitudinal and survival processes\n\n\nCommon Association Structures \\(\\eta(\\cdot)\\)\nThe choice of \\(\\eta\\) defines how the biomarker influences risk:\n\n\n\n\n\n\n\n\nAssociation Type\nForm\nInterpretation\n\n\n\n\nCurrent value\n\\(\\eta = m_i(t)\\)\nInstantaneous biomarker level affects current hazard\n\n\nCurrent slope\n\\(\\eta = m_i'(t) = \\frac{d}{dt}m_i(t)\\)\nRate of change (e.g., rapid decline) affects risk\n\n\nRandom effects\n\\(\\eta = b_i\\)\nSubject’s deviation from population (e.g., low baseline) affects baseline hazard\n\n\nCombined\n\\(\\eta = m_i(t) + m_i'(t)\\)\nBoth level and trend matter\n\n\n\n\nThe current value parameterization is the most common and often serves as the “standard” joint model.\n\n\n\nKey Assumptions\n\nConditional independence: Given the random effects ( b_i ) (and thus ( m_i(t) )), the longitudinal and event processes are independent.\nNormality: Random effects ( b_i (0, D) ); measurement errors ( _i (0, ^2) ).\nCorrect specification of the association structure and baseline hazard.\n\n\n\nEstimation\nTypically done via maximum likelihood: [ L() = _{i=1}^n ] - ( = (_L, _S, D, ^2, ) ): full parameter vector - Integration over ( b_i ) is done numerically (e.g., Gauss-Hermite quadrature or Monte Carlo) - Alternatively, Bayesian MCMC can be used\n\n\nAdvantages\n\nReduces bias from informative dropout (e.g., patients with worsening biomarkers drop out and die)\nAccounts for measurement error in biomarkers\nEnables dynamic predictions: update survival probability as new longitudinal data arrive\nProvides mechanistic insight: quantifies how biomarker trajectories drive risk\n\n\n\nExample\n\nStudy: Prostate cancer patients monitored via repeated PSA (prostate-specific antigen) tests; event = metastasis.\n\nLongitudinal: PSA modeled with random intercept/slope.\nSurvival: Hazard of metastasis depends on current true PSA level.\nResult: Higher PSA trajectory → significantly increased metastasis risk (( &gt; 0 )).\n\n\nThis fits the standard shared random effects joint model with current-value association."
  },
  {
    "objectID": "02-07-06-01-survival-analysis-joint-modeling-r.html#implementation-in-r",
    "href": "02-07-06-01-survival-analysis-joint-modeling-r.html#implementation-in-r",
    "title": "6.1 Standard (Shared Random Effects) Joint Model",
    "section": "Implementation in R",
    "text": "Implementation in R\nStandard joint models can be implemented in R using several packages, including {JM}, {joineR}, and {JMbayes}. Below, we provide an example of how to fit a standard joint model using the {JM} and {joineR} package.\n\nInstall Required R Packages\nFollowing R packages are required to run this notebook. If any of these packages are not installed, you can install them using the code below:\n\n\nCode\npackages &lt;-c(\n         'tidyverse',\n         'survival',\n         'survminer',\n         'ggsurvfit',\n         'tidycmprsk',\n         'ggfortify',\n         'timereg',\n         'cmprsk',\n         'condSURV',\n         'riskRegression',\n         'prodlim',\n         'lava',\n         'mstate',\n         'regplot',\n         'cmprskcoxmsm',\n         'GLMMadaptive',\n         'JM',\n         'nlme',\n         'lattice',\n         'joineR',\n         'joineRML'\n         \n         )\n\n\n```{r\n#| warning: false #| error: false"
  },
  {
    "objectID": "02-07-06-01-survival-analysis-joint-modeling-r.html#the-jm-package",
    "href": "02-07-06-01-survival-analysis-joint-modeling-r.html#the-jm-package",
    "title": "6.1 Standard (Shared Random Effects) Joint Model",
    "section": "The {JM} Package",
    "text": "The {JM} Package\nThe {JM} package is a comprehensive tool for fitting joint models of longitudinal and time-to-event data in R. It provides functions for estimating the parameters of the longitudinal and survival submodels, as well as the association structure linking the two. The package also offers tools for making predictions and assessing the model’s performance. Below, we demonstrate how to use the {JM} package to fit joint models to longitudinal and time-to-event data.\nThe joint model consists of two submodels: a linear mixed model for the longitudinal CD4 cell counts and a Cox proportional hazards model for the survival outcomes. The two submodels are linked by a shared parameter that captures the association between the longitudinal and survival processes.\n\nData\nWe’ll use the aids dataset from the {JM} package to demonstrate how to perform joint modeling of longitudinal and time-to-event data in R. A randomized clinical trial in which both longitudinal and survival data were collected to compare the efficacy and safety of two antiretroviral drugs in treating patients who had failed or were intolerant of zidovudine (AZT) therapy. The dataset contains information on patients’ CD4 cell counts (CD4) and survival status (death) over time, along with other covariates. The goal is to model the relationship between CD4 cell counts and survival outcomes.\nThe dataset used is the same that the one seen with the mixed model, aids. The survival information can be found in aids.id.\nA data frame with 1408 observations on the following 9 variables.\npatiet: patients identifier; in total there are 467 patients.\nTime: the time to death or censoring.\ndeath: a numeric vector with 0 denoting censoring and 1 death.\nCD4: the CD4 cells count.\nobstime: the time points at which the CD4 cells count was recorded.\ndrug: a factor with levels ddC denoting zalcitabine and ddI denoting didanosine.\ngender: a factor with levels female and male.\nprevOI: a factor with levels AIDS denoting previous opportunistic infection (AIDS diagnosis) at study entry, and noAIDS denoting no previous infection.\nAZT: a factor with levels intolerance and failure denoting AZT intolerance and AZT failure, respectively.\n\n\nCode\ndata(\"aids\", package = \"JM\")\nglimpse(aids)\n\n\nRows: 1,405\nColumns: 12\n$ patient &lt;fct&gt; 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 4, 4, 4, 4, 5, 5, 5, 5, 6, 7, 7,…\n$ Time    &lt;dbl&gt; 16.97, 16.97, 16.97, 19.00, 19.00, 19.00, 19.00, 18.53, 18.53,…\n$ death   &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,…\n$ CD4     &lt;dbl&gt; 10.677078, 8.426150, 9.433981, 6.324555, 8.124038, 4.582576, 5…\n$ obstime &lt;int&gt; 0, 6, 12, 0, 6, 12, 18, 0, 2, 6, 0, 2, 6, 12, 0, 2, 6, 12, 0, …\n$ drug    &lt;fct&gt; ddC, ddC, ddC, ddI, ddI, ddI, ddI, ddI, ddI, ddI, ddC, ddC, dd…\n$ gender  &lt;fct&gt; male, male, male, male, male, male, male, female, female, fema…\n$ prevOI  &lt;fct&gt; AIDS, AIDS, AIDS, noAIDS, noAIDS, noAIDS, noAIDS, AIDS, AIDS, …\n$ AZT     &lt;fct&gt; intolerance, intolerance, intolerance, intolerance, intoleranc…\n$ start   &lt;int&gt; 0, 6, 12, 0, 6, 12, 18, 0, 2, 6, 0, 2, 6, 12, 0, 2, 6, 12, 0, …\n$ stop    &lt;dbl&gt; 6.00, 12.00, 16.97, 6.00, 12.00, 18.00, 19.00, 2.00, 6.00, 18.…\n$ event   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,…\n\n\n\n\nCode\ndata(\"aids.id\", package = \"JM\")\nglimpse(aids.id)\n\n\nRows: 467\nColumns: 12\n$ patient &lt;fct&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18,…\n$ Time    &lt;dbl&gt; 16.97, 19.00, 18.53, 12.70, 15.13, 1.90, 14.33, 9.57, 11.57, 1…\n$ death   &lt;int&gt; 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1,…\n$ CD4     &lt;dbl&gt; 10.677078, 6.324555, 3.464102, 3.872983, 7.280110, 4.582576, 6…\n$ obstime &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ drug    &lt;fct&gt; ddC, ddI, ddI, ddC, ddI, ddC, ddC, ddI, ddC, ddI, ddC, ddI, dd…\n$ gender  &lt;fct&gt; male, male, female, male, male, female, male, female, male, ma…\n$ prevOI  &lt;fct&gt; AIDS, noAIDS, AIDS, AIDS, AIDS, AIDS, AIDS, noAIDS, AIDS, AIDS…\n$ AZT     &lt;fct&gt; intolerance, intolerance, intolerance, failure, failure, failu…\n$ start   &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ stop    &lt;dbl&gt; 6.00, 6.00, 2.00, 2.00, 2.00, 1.90, 2.00, 2.00, 2.00, 2.00, 2.…\n$ event   &lt;dbl&gt; 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,…\n\n\n\n\nCode\nhist(aids$CD4)\n\n\n\n\n\n\n\n\n\nThe CD4 cell exhibits right skewed shapes of distribution, and therefore, for the remainder of this analysis we will work with the square root of the CD4 cell values\n\n\nCode\nhist(sqrt(aids$CD4))\n\n\n\n\n\n\n\n\n\n\n\nCode\nlattice::xyplot(sqrt(CD4) ~ obstime | patient, group = patient, \n       data = aids[aids$patient %in% c(1:10),], \n       xlab = \"Months\", ylab = expression(sqrt(\"CD4\")), col = 1, type = \"b\")\n\n\n\n\n\n\n\n\n\n\n\nFit a Longitudinal Model (lmeObject)\nFirst, we fit a linear mixed model for the longitudinal CD4 cell counts using the lme() function from the {nlme} package. This model captures the trajectory of CD4 cell counts over time, accounting for patient-specific variability.\n\n\nCode\n# linear mixed model fit (random intercepts)\nlmeFit_01 &lt;- lme(sqrt(CD4) ~ obstime : drug, random = ~ 1 | patient, data = aids)\nsummary(lmeFit_01)\n\n\nLinear mixed-effects model fit by REML\n  Data: aids \n       AIC      BIC    logLik\n  2730.729 2756.957 -1360.364\n\nRandom effects:\n Formula: ~1 | patient\n        (Intercept)  Residual\nStdDev:   0.8719493 0.4106749\n\nFixed effects:  sqrt(CD4) ~ obstime:drug \n                     Value  Std.Error  DF  t-value p-value\n(Intercept)      2.5087153 0.04306984 936 58.24761       0\nobstime:drugddC -0.0338783 0.00352643 936 -9.60697       0\nobstime:drugddI -0.0282970 0.00360358 936 -7.85247       0\n Correlation: \n                (Intr) obst:C\nobstime:drugddC -0.153       \nobstime:drugddI -0.145  0.022\n\nStandardized Within-Group Residuals:\n        Min          Q1         Med          Q3         Max \n-5.09514877 -0.46243667  0.01324625  0.48993786  5.17139372 \n\nNumber of Observations: 1405\nNumber of Groups: 467 \n\n\n\n\nFit a Survival Model (survObject)\nNext, we fit a Cox proportional hazards model for the survival outcomes using the coxph() function from the survival package. This model estimates the hazard of death based on the CD4 cell counts and other covariates.\nThe survival submodel include: treatment effect (as a time-independent covariate) and the true underlying effect of CD4 cell count as estimated from the longitudinal model (as time-dependent). The baseline risk function is assumed piecewise constant.\n\n\nCode\n# Fit the survival model\nsurvFit &lt;- coxph(Surv(Time, death) ~ drug, data = aids.id, x = TRUE)\nsummary(survFit)\n\n\nCall:\ncoxph(formula = Surv(Time, death) ~ drug, data = aids.id, x = TRUE)\n\n  n= 467, number of events= 188 \n\n          coef exp(coef) se(coef)     z Pr(&gt;|z|)\ndrugddI 0.2102    1.2339   0.1462 1.437    0.151\n\n        exp(coef) exp(-coef) lower .95 upper .95\ndrugddI     1.234     0.8104    0.9264     1.643\n\nConcordance= 0.531  (se = 0.019 )\nLikelihood ratio test= 2.07  on 1 df,   p=0.2\nWald test            = 2.07  on 1 df,   p=0.2\nScore (logrank) test = 2.07  on 1 df,   p=0.1\n\n\n\n\nFit the Joint Model\nThe jointModel() function is used to fit the joint model, specifying the longitudinal and survival submodels, as well as the time variable (obstime) linking the two.\nThis jointModel() function fits shared parameter models for the joint modelling of normal longitudinal responses and time-to-event data under a maximum likelihood approach. Various options for the survival model are available.\njointModel(lmeObject, survObject, timeVar, \n  parameterization = c(\"value\", \"slope\", \"both\"),\n  method = c(\"weibull-PH-aGH\", \"weibull-PH-GH\", \"weibull-AFT-aGH\", \n    \"weibull-AFT-GH\", \"piecewise-PH-aGH\", \"piecewise-PH-GH\", \n    \"Cox-PH-aGH\", \"Cox-PH-GH\", \"spline-PH-aGH\", \"spline-PH-GH\", \n    \"ch-Laplace\"),\n  interFact = NULL, derivForm = NULL, lag = 0, scaleWB = NULL,\n  CompRisk = FALSE, init = NULL, control = list(), ...)\nmethod a character string specifying the type of joint model to fit. Various methods are available for the survival model:\nweibull-AFT-GH: a time-dependent Weibull model under the accelerated failure time formulation is assumed. weibull-PH-GH: a time-dependent relative risk model is postulated with a Weibull baseline risk function. piecewise-PH-GH: a time-dependent relative risk model is postulated with a piecewise constant baseline risk function. spline-PH-GH: a time-dependent relative risk model is assumed in which the log baseline risk function is approximated using B-splines. ch-Laplace: an additive model on the log cumulative hazard scale is assumed (see Rizopoulos et al., 2009 for more info). Cox-PH-GH a time-dependent relative risk model is assumed where the baseline risk function is left unspecified (Wulfsohn and Tsiatis, 1997\nFinally, we fit the joint model that links the longitudinal and survival submodels using the jointModel() function from the JM package. This model provides insights into how the CD4 cell counts influence the risk of death over time.\nWe will use method as piecewise-PH-GH to fit a time-dependent relative risk model with a piecewise constant baseline risk function.\n\n\nCode\n# Fit the joint model with random intercepts\nJMfit_01 &lt;- jointModel(lmeFit_01, survFit, timeVar = \"obstime\", method = \"piecewise-PH-GH\")\n\n\nsummary() retrieves the summary of the joint model summary, variance components, and the association parameter, coefficients of longitudinal and time-to-event processs:\n\n\nCode\nsummary(JMfit_01)\n\n\n\nCall:\njointModel(lmeObject = lmeFit_01, survObject = survFit, timeVar = \"obstime\", \n    method = \"piecewise-PH-GH\")\n\nData Descriptives:\nLongitudinal Process        Event Process\nNumber of Observations: 1405    Number of Events: 188 (40.3%)\nNumber of Groups: 467\n\nJoint Model Summary:\nLongitudinal Process: Linear mixed-effects model\nEvent Process: Relative risk model with piecewise-constant\n        baseline risk function\nParameterization: Time-dependent \n\n   log.Lik      AIC      BIC\n -2135.224 4298.447 4356.496\n\nVariance Components:\n               StdDev\n(Intercept) 0.8797564\nResidual    0.4251247\n\nCoefficients:\nLongitudinal Process\n                  Value Std.Err z-value p-value\n(Intercept)      2.4987  0.0457 54.6228 &lt;0.0001\nobstime:drugddC -0.0361  0.0037 -9.8114 &lt;0.0001\nobstime:drugddI -0.0314  0.0037 -8.4558 &lt;0.0001\n\nEvent Process\n            Value Std.Err z-value p-value\ndrugddI    0.3466  0.1527  2.2697  0.0232\nAssoct    -1.0945  0.1192 -9.1841 &lt;0.0001\nlog(xi.1) -1.6430  0.2543 -6.4603        \nlog(xi.2) -1.2659  0.2517 -5.0283        \nlog(xi.3) -0.9151  0.2991 -3.0597        \nlog(xi.4) -1.4505  0.3839 -3.7782        \nlog(xi.5) -1.3336  0.3597 -3.7072        \nlog(xi.6) -1.3264  0.4310 -3.0774        \nlog(xi.7) -1.3026  0.5386 -2.4186        \n\nIntegration:\nmethod: Gauss-Hermite\nquadrature points: 15 \n\nOptimization:\nConvergence: 0 \n\n\nNow will fit the with random intercepts + random slopes\n\n\nCode\nlmeFit_02 &lt;- lme(sqrt(CD4) ~ obstime : drug, random = ~ obstime | patient, data = aids)\n\n\n\n\nCode\n# Fit the joint model with intercepts + random slopes\nJMfit_02 &lt;- jointModel(lmeFit_02, survFit, timeVar = \"obstime\", method = \"piecewise-PH-GH\")\n\n\n\n\nModel Comparison\nanova() function can be used to compare the two models:\n\n\nCode\nanova(JMfit_01, JMfit_02)\n\n\n\n             AIC     BIC  log.Lik   LRT df p.value\nJMfit_01 4298.45 4356.50 -2135.22                 \nJMfit_02 4247.29 4313.64 -2107.65 55.15  2 &lt;0.0001\n\n\nLongitudinal submodels with random intercepts and random slopes are compared using the anova() function. The results show that the model with random intercepts and slopes is significantly better than the model with random intercepts only.\n\n\nPlotting Marginal Survival Curves\n\n\nCode\nplot(JMfit_02, 3, add.KM = TRUE, col = 2, lwd = 2,\n     main = \"Piecewise-constant Baseline Risk Function\", \n     ylab = \"Survival Probability\")\n\n\n\n\n\n\n\n\n\nsurvfit() function of {survial} pcakage creates survival curves from either a formula (e.g. the Kaplan-Meier), a previously fitted Cox model, or a previously fitted accelerated failure time model.\n\n\nCode\nres.surv &lt;- residuals(JMfit_02, process = \"Event\", type = \"Cox\")\nsfit &lt;- survfit(Surv(res.surv,JMfit_02$y$d) ~ 1)\nplot(sfit, mark.time = FALSE, conf.int = TRUE, lty = 1:2, \n    xlab = \"Cox-Snell Residuals\", ylab = \"Survival Probability\", \n    main = \"Piecewise-constant Baseline Risk Function\")\ncurve(pexp(x, lower.tail = FALSE), from = min(res.surv), to = max(res.surv), \n    add = TRUE, col = \"red\", lwd = 2)\nlegend(\"topright\", c(\"Survival function of unit Exponential\", \n    \"Survival function of Cox-Snell Residuals\"), lty = 1, lwd = 2, col = 2:1, bty = \"n\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nCox-Snell residuals are a diagnostic tool used in survival analysis to assess the fit of a survival model, such as a Cox proportional hazards model or parametric survival models. They transform survival data into residuals that should ideally follow a standard exponential distribution with a mean of 1, under the assumption that the model is correctly specified.\n\n\n\n\nHazard Ratio\nThe cinfint() function calculates the hazard ratio for the treatment effect in the survival model. This ratio quantifies the relative risk of death between the two treatment groups.\n\n\nCode\nexp(confint(JMfit_02, parm = \"Event\"))\n\n\n            2.5 %      est.    97.5 %\ndrugddI 1.0510576 1.4206752 1.9202736\nAssoct  0.2637343 0.3323346 0.4187786\n\n\nwe also compare with the time-dependent Cox model\n\n\nCode\nexp(confint(survFit))\n\n\n            2.5 %   97.5 %\ndrugddI 0.9264493 1.643414\n\n\n\n\nExpected survival probabilities\nsurvfitJM() function calculates the expected survival probabilities for each patient at specific time points based on the joint model.\nHere we compute the expected survival probabilities for two patients (5, 141) in the data set who has not died by the time of loss to follow-up. The function assumes that the patient has survived up to the last time point \\(t\\) in newdata for which a CD4 measurement was recorded, and will produce survival probabilities for a set of predefined \\(u &gt; t\\) values\n\n\nCode\nset.seed(300716) # it uses Monte Carlo samples\npred &lt;- survfitJM(JMfit_02, newdata = aids[aids$patient %in% c(\"5\", \"141\"), ],\n          idVar = \"patient\")  # last.time = \"Time\"\n\n\n\n\nCode\npar(mfrow=c(1,2))\nplot(pred, which = \"5\", conf.int = TRUE)\nplot(pred, which = \"141\", conf.int = TRUE, \n     fun = function (x) -log(x), ylab = \"Cumulative Risk\")\n\n\n\n\n\n\n\n\n\n\n\nPrediction\n\n\nCode\nDF &lt;- with(aids, expand.grid(drug = levels(drug),\n    obstime = seq(min(obstime), max(obstime), len = 100)))\nPs &lt;- predict(JMfit_02, DF, interval = \"confidence\", return = TRUE)\nrequire(lattice)\nxyplot(pred + low + upp ~ obstime | drug, data = Ps,\n    type = \"l\", col = c(2,1,1), lty = c(1,2,2), lwd = 2,\n    ylab = \"Average log serum Bilirubin\")"
  },
  {
    "objectID": "02-07-06-01-survival-analysis-joint-modeling-r.html#the-joiner-package",
    "href": "02-07-06-01-survival-analysis-joint-modeling-r.html#the-joiner-package",
    "title": "6.1 Standard (Shared Random Effects) Joint Model",
    "section": "The {joineR} package",
    "text": "The {joineR} package\nThe {joineR} package is designed to facilitate the analysis of data from longitudinal studies, which involve collecting multiple measurements over time from the same subjects. Each subject’s data consists of a series of repeated measurements as well as a potentially censored time-to-event outcome, such as the time until a particular event occurs. To model the repeated measurements, the package utilizes a linear model that incorporates random effects, allowing for the analysis of individual variations over time. Additionally, a correlated error structure can be included to account for any dependencies among the measurements. For the time-to-event outcomes, the modeling framework employs a Cox proportional hazards model augmented with log-Gaussian frailty. This approach enables the analysis of the effects of covariates on the timing of events while accounting for unobserved heterogeneity among subjects. A key feature of this framework is the ability to capture stochastic dependence between the two components of the model. Specifically, it allows the Gaussian random effects from the linear model—representing subject-specific variations—to be correlated with the frailty term in the Cox proportional hazards model. This correlation can provide deeper insights into the relationships between the repeated measurements and the timing of events, leading to more comprehensive and nuanced interpretations of the\n\nData\nThe mental data-set relates to a study in which 150 patients suffering from chronic mental illness were randomised amongst three different drug treatments: placebo and two active drugs. A questionnaire instrument was used to assess each patient’s mental state at weeks 0, 1, 2, 4, 6 and 8 post-randomization. The data can be loaded with the command:\n\n\nCode\nlibrary(joineR)\ndata(\"mental\")\n\n\nA total of 150 subjects were enrolled in the study, but only 68 of them completed all measurements at weeks 0, 1, 2, 4, 6, and 8. The remaining participants left the study early for various reasons, some of which were believed to be related to their mental state. As a result, the dropout rate may provide informative insights. The data from the first five subjects can be accessed as usual.\n\n\nCode\nmental[1:5, ]\n\n\n  id Y.t0 Y.t1 Y.t2 Y.t4 Y.t6 Y.t8 treat n.obs surv.time cens.ind\n1  1   55   NA   NA   NA   NA   NA     2     1     0.704        0\n2  2   44   NA   NA   NA   NA   NA     1     1     0.740        0\n3  3   65   67   NA   NA   NA   NA     2     2     1.121        1\n4  4   64   56   NA   NA   NA   NA     2     2     1.224        1\n5  5   47   48   NA   NA   NA   NA     2     2     1.303        0\n\n\nThe data is stored in a balanced format, consisting of 150 rows (one for each subject) and 11 columns. These columns include a subject identifier, the measured values from the questionnaire taken at each of the six scheduled follow-up times, the treatment allocation, the count of non-missing measured values, an imputed dropout time, and a censoring indicator. The censoring indicator is coded as 1 for subjects who dropped out for reasons believed to be related to their mental health state and as 0 for all others. It’s important to note the distinction made between potentially informative dropout and censoring, with the latter being regarded as uninformative. Thus, the command:\n\n\nCode\ntable(mental$cens[is.na(mental$Y.t8)])\n\n\n\n 0  1 \n21 61 \n\n\nshows that 21 of the 82 dropouts did so for reasons unrelated to their mental health.\n\n\nConverting between balanced and unbalanced data-formats\nTwo functions are provided for converting objects from one format to another. The code below demonstrates how to convert the mental dataset from a balanced format to an unbalanced format. It also includes a mnemonic renaming of the column that now contains all the repeated measurements.\n\n\nCode\nmental.unbalanced &lt;- to.unbalanced(mental, id.col = 1,\n                                   times = c(0, 1, 2, 4, 6, 8),\n                                   Y.col = 2:7, other.col = 8:11)\nnames(mental.unbalanced)\n\n\n[1] \"id\"        \"time\"      \"Y.t0\"      \"treat\"     \"n.obs\"     \"surv.time\"\n[7] \"cens.ind\" \n\n\n\n\nCode\nnames(mental.unbalanced)[3] &lt;- \"Y\"\n\n\nThe following code converts the new object back to the balanced format:\n\n\nCode\nmental.balanced &lt;- to.balanced(mental.unbalanced, id.col = 1,\n                               time.col = 2,\n                               Y.col = 3, other.col = 4:7)\ndim(mental.balanced)\n\n\n[1] 150  11\n\n\nOnce a data-set is in the balanced format (if it is unbalanced then to.balanced can be applied) then the mean, variance and correlation matrix of the responses can be extracted using summarybal(). An example is given below using the mental data-set:\n\n\nCode\nsummarybal(mental, Y.col = 2:7, times = c(0, 1, 2, 4, 6, 8), \n           na.rm = TRUE)\n\n\n$mean.vect\n     x        y\nY.t0 0 55.77333\nY.t1 1 53.03378\nY.t2 2 50.37008\nY.t4 4 48.62963\nY.t6 6 46.94048\nY.t8 8 46.02941\n\n$variance\n    Y.t0     Y.t1     Y.t2     Y.t4     Y.t6     Y.t8 \n132.1228 152.7403 167.5366 168.6653 228.8759 181.4917 \n\n$cor.mtx\n          [,1]      [,2]      [,3]      [,4]      [,5]      [,6]\n[1,] 1.0000000 0.5939982 0.4537496 0.4186798 0.3432355 0.2997679\n[2,] 0.5939982 1.0000000 0.7793311 0.6805021 0.6631543 0.6149448\n[3,] 0.4537496 0.7793311 1.0000000 0.7992470 0.7077238 0.6202277\n[4,] 0.4186798 0.6805021 0.7992470 1.0000000 0.8108585 0.7402240\n[5,] 0.3432355 0.6631543 0.7077238 0.8108585 1.0000000 0.8681933\n[6,] 0.2997679 0.6149448 0.6202277 0.7402240 0.8681933 1.0000000\n\n\n\n\nExploring covariance structure\n\n\nCode\ny &lt;- as.matrix(mental[, 2:7]) \n# converts mental from list format to numeric matrix format\nmeans &lt;- matrix(0, 3, 6)\n\nfor (trt in 1:3) {\n   ysub &lt;- y[mental$treat == trt, ]\n   means[trt,] &lt;- apply(ysub, 2, mean, na.rm = TRUE)\n}\n\nresiduals &lt;- matrix(0, 150, 6)\n\nfor (i in 1:150) {\n   residuals[i,] &lt;- y[i,] - means[mental$treat[i], ]\n}\n\nV &lt;- cov(residuals, use = \"pairwise\")\nR &lt;- cor(residuals, use = \"pairwise\")\nround(cbind(diag(V), R), 3)\n\n\n        [,1]  [,2]  [,3]  [,4]  [,5]  [,6]  [,7]\n[1,] 131.774 1.000 0.612 0.472 0.464 0.391 0.321\n[2,] 142.171 0.612 1.000 0.766 0.663 0.650 0.603\n[3,] 159.711 0.472 0.766 1.000 0.792 0.712 0.624\n[4,] 153.364 0.464 0.663 0.792 1.000 0.799 0.738\n[5,] 198.350 0.391 0.650 0.712 0.799 1.000 0.861\n[6,] 167.886 0.321 0.603 0.624 0.738 0.861 1.000\n\n\n\n\nCreating a jointdata object\nA jointdata() object is a list that can contain up to three data frames, including repeated measurement data, time-to-event data, and baseline covariate data. The repeated measurement data must be stored in an unbalanced format, while the time-to-event and baseline covariate data should be stored in a balanced format, meaning there should be one line for each subject. Each data frame must include a column for the subject ID, and these subject ID columns across all three data frames must match. The UniqueVariables function offers a convenient way to extract the time-to-event and baseline covariate data from an unbalanced data frame, as demonstrated in the following example.\n\n\nCode\nmental.long &lt;- mental.unbalanced[, 1:3]\nmental.surv &lt;- UniqueVariables(mental.unbalanced, \n                               6:7, id.col = 1)\nmental.baseline &lt;- UniqueVariables(mental.unbalanced,\n                                   4, id.col = 1)\nmental.jd &lt;- jointdata(mental.long, \n                       mental.surv,\n                       mental.baseline,\n                       id.col = \"id\", \n                       time.col = \"time\")\n\n\n\n\nModel Fitting\nThe package includes functions for fitting two different classes of model using maximum likelihood estimation. The first is an extended version of the random effects model proposed by Wulfsohn and Tsiatis (1997). See Henderson et al. (2000). The second is the transformation model described by Diggle et al. (2008).\nThe random effects model is fitted using the joint() function. This generic function fits a joint model with random latent association, building on the formulation described in Wulfsohn and Tsiatis (1997) while allowing for the presence of longitudinal and survival covariates, and three choices for the latent process. The link between the longitudinal and survival processes can be proportional or separate. When failure is attributable to 2 separate causes, a competing risks joint model is fitted as per Williamson et al. (2008).\n\n\nCode\nmodel.jointrandom &lt;- joint(mental.jd, Y ~ 1 + time + treat, \n                           Surv(surv.time, cens.ind) ~ treat,\n                           model = \"int\")\nnames(model.jointrandom)\n\n\n [1] \"coefficients\" \"sigma.z\"      \"sigma.u\"      \"hazard\"       \"loglik\"      \n [6] \"numIter\"      \"convergence\"  \"model\"        \"sepassoc\"     \"sepests\"     \n[11] \"compRisk\"     \"sep.loglik\"   \"formulae\"     \"data\"         \"call\"        \n\n\nThere is a summary() method for a model fitted using joint. This produces a summarized version of the model fit, omitting some of the information contained within the fitted object itself. The summary method for a joint object is demonstrated below:\n\n\nCode\nsummary(model.jointrandom)\n\n\n\nCall:\njoint(data = mental.jd, long.formula = Y ~ 1 + time + treat, \n    surv.formula = Surv(surv.time, cens.ind) ~ treat, model = \"int\")\n\nRandom effects joint model\n Data: mental.jd \n Log-likelihood: -2884.045 \n\nLongitudinal sub-model fixed effects: Y ~ 1 + time + treat                      \n(Intercept) 61.9221415\ntime        -0.5969329\ntreat       -3.8544659\n\nSurvival sub-model fixed effects: Surv(surv.time, cens.ind) ~ treat                \ntreat -0.6001792\n\nLatent association:                  \ngamma_0 0.08654259\n\nVariance components:\n     U_0 Residual \n99.84760 64.34334 \n\nConvergence at iteration: 32 \n\nNumber of observations: 900 \nNumber of groups: 150 \n\n\nThe function jointSE() provides standard errors and confidence intervals for the parameters defining the mean response profiles in a random effects joint model. Currently, the calculation of standard errors for the random effect parameters is not implemented. However, approximate standard errors can be obtained through a parametric bootstrap, which involves re-estimating the model parameters from simulated realizations of the fitted model. The first two arguments for jointSE() are the results from calling the joint() function, with the corresponding jointdata object automatically stored as part of the model fit. The second argument allows you to specify the number of bootstrap samples to be taken. The remaining arguments are included for completeness; they mirror the last four arguments of the joint() function and provide an additional option to control the level of output displayed in the R terminal. Be aware that with a sufficiently large number of bootstrap samples, this function can be slow to execute. If you run the function with only 100 bootstrap samples, the output will reflect this number, and the confidence limits will default to zero unless at least 100 bootstrap samples are taken.\n\n\nCode\nmodel.jointrandom.se &lt;- jointSE(model.jointrandom, n.boot = 100)\nmodel.jointrandom.se\n\n\n     Component   Parameter Estimate      SE 95%Lower 95%Upper\n1 Longitudinal (Intercept)  61.9221  2.2932  56.8931  66.0140\n2                     time  -0.5969  0.1993  -0.9304  -0.2398\n3                    treat  -3.8545  1.0423  -5.8642  -1.7523\n4      Failure       treat  -0.6002  0.2176  -1.1641  -0.2548\n5  Association     gamma_0   0.0865  0.0175   0.0608   0.1313\n6     Variance         U_0  99.8476 11.9674  76.9695 121.3286\n7                 Residual  64.3433  7.0281  52.1935  79.0215"
  },
  {
    "objectID": "02-07-06-01-survival-analysis-joint-modeling-r.html#summary-and-conclusions",
    "href": "02-07-06-01-survival-analysis-joint-modeling-r.html#summary-and-conclusions",
    "title": "6.1 Standard (Shared Random Effects) Joint Model",
    "section": "Summary and Conclusions",
    "text": "Summary and Conclusions\nThis tutorial covered the essential concepts and practical applications of Standard (Shared Random Effects) Joint model of longitudinal and time-to-event data in R. We began by introducing the theoretical background of joint models and their relevance in clinical research. We then demonstrated how to fit univariate and multivariate joint models using {JM}, and {joineR} packages. We also discussed the importance of functional forms in joint models and how to specify them in the model fitting process. Additionally, we explored dynamic predictions and predictive accuracy metrics, such as ROC curves, calibration plots, and Brier scores. Finally, we introduced the concept of competing risks and illustrated how to fit joint models for competing risks data.\nThis tutorial helps researchers and practitioners understand the principles of joint modeling and provides practical guidance on implementing these models in R. By following the step-by-step instructions and examples provided in this tutorial, users can gain a comprehensive understanding of joint models and apply them to their own longitudinal and time-to-event data."
  },
  {
    "objectID": "02-07-06-01-survival-analysis-joint-modeling-r.html#resources",
    "href": "02-07-06-01-survival-analysis-joint-modeling-r.html#resources",
    "title": "6.1 Standard (Shared Random Effects) Joint Model",
    "section": "Resources",
    "text": "Resources\n\nA Tutorial for Joint Modeling of Longitudinal and Time-to-Event Data in R\nJoint Models for Longitudinal and Time-to-Event Data with Applications in R by Dimitris Rizopoulos\nJoint Models for Longitudinal and Time-to-Event Data\nChapter 4 Joint Models for Longitudinal and Time-to-Event Data\njoineR\nMultivariate Joint Models\nJMbayes2: Extended Joint Models for Longitudinal and Time-to-Event Data"
  },
  {
    "objectID": "02-07-06-00-survival-analysis-joint-modeling-introduction-r.html#ovberview",
    "href": "02-07-06-00-survival-analysis-joint-modeling-introduction-r.html#ovberview",
    "title": "6. Joint Modeling in Survival Analysis",
    "section": "Ovberview",
    "text": "Ovberview\nJoint Modeling (JM) is a statistical framework that simultaneously analyzes two or more linked outcome processes that are measured repeatedly over time and potentially influenced by shared underlying factors. It is particularly powerful when one outcome is longitudinal (e.g., a biomarker measured repeatedly) and another is time-to-event (e.g., survival time until death, disease progression, or dropout).\nThe key idea is that these processes are not independent — the longitudinal trajectory may inform the risk of the event, and the event may affect the longitudinal process (e.g., via informative dropout). Joint models account for this association using shared random effects.",
    "crumbs": [
      "**Joint Modeling**",
      "Introduction to Joint Modeling in Survival Analysis"
    ]
  },
  {
    "objectID": "02-07-06-00-survival-analysis-joint-modeling-introduction-r.html#core-components-of-a-joint-model",
    "href": "02-07-06-00-survival-analysis-joint-modeling-introduction-r.html#core-components-of-a-joint-model",
    "title": "6. Joint Modeling in Survival Analysis",
    "section": "Core Components of a Joint Model",
    "text": "Core Components of a Joint Model\nA standard joint model for longitudinal and survival data consists of two sub-models:\n\nLongitudinal Sub-model (for the repeated measurements):\n\n\\[\n   y_i(t) = \\mathbf{x}_i(t)^\\top \\boldsymbol{\\beta} + \\mathbf{z}_i(t)^\\top \\mathbf{b}_i + \\epsilon_i(t)\n\\] - \\(y_i(t)\\): observed biomarker at time \\(t\\) for subject \\(i\\) - \\(\\mathbf{x}_i(t)\\): fixed-effect covariates (e.g., treatment, age) - \\(\\mathbf{z}_i(t)\\): random-effect design (e.g., intercept, slope) - \\(\\mathbf{b}_i\\): subject-specific random effects ~ \\(N(0, \\Sigma)\\) - \\(\\epsilon_i(t)\\): measurement error ~ \\(N(0, \\sigma^2)\\)\n\nSurvival (Time-to-Event) Sub-model:\n\n\\[\n   h_i(t) = h_0(t) \\exp\\left( \\mathbf{w}_i^\\top \\boldsymbol{\\gamma} + \\alpha \\cdot m_i(t) \\right)\n\\] - \\(h_i(t)\\): hazard of event at time \\(t\\) - \\(h_0(t)\\): baseline hazard (often flexible, e.g., splines) - \\(\\mathbf{w}_i\\): baseline covariates - \\(m_i(t)\\): true (unobserved) longitudinal trajectory at time ( t ), linked via:\n\\[\n     m_i(t) = \\mathbf{z}_i^*(t)^\\top \\mathbf{b}_i\n\\] - \\(\\alpha\\): association parameter (key — measures how the biomarker affects risk)\n\nThe random effects \\(\\mathbf{b}_i\\) are shared between both sub-models → this induces dependence.\n\nThe link between sub-models comes through \\(m_i(t)\\) — the true, unobserved value of the biomarker.\n\nAdvantages of Joint Models\n\nHandle informative dropout (e.g., subjects leaving the study due to disease progression).\nImprove dynamic prediction of event risk given ongoing biomarker measurements.\nBetter inference about longitudinal-survival relationships.\n\n\n\nWhy Use Joint Models?\n\nDependency Between Data Types:\n\nLongitudinal markers (e.g., biomarkers, repeated measurements) often influence or are associated with survival outcomes. Ignoring this dependency can lead to biased results.\nFor example, in a cancer study, a biomarker like PSA (prostate-specific antigen) measured over time can be predictive of time to disease progression or death.\n\nSeparate Models are Insufficient:\n\n\nLongitudinal models (e.g., linear mixed models) focus only on repeated measurements and ignore survival times.\nSurvival models (e.g., Cox proportional hazards) treat longitudinal measures as static covariates, losing information about their dynamic nature.\n\n\nPrediction:\n\nJoint models allow dynamic predictions of survival probabilities based on the evolving trajectory of the longitudinal marker.\n\n\n\n\nApplications\n\nMedicine:\n\nMonitoring disease biomarkers (e.g., tumor size, glucose levels) and predicting survival outcomes.\n\nDrug Development:\n\nUnderstanding the relationship between pharmacokinetics/pharmacodynamics and patient outcomes.\n\nEngineering:\n\nModeling degradation of machinery (longitudinal) and time to failure (event data).",
    "crumbs": [
      "**Joint Modeling**",
      "Introduction to Joint Modeling in Survival Analysis"
    ]
  },
  {
    "objectID": "02-07-06-00-survival-analysis-joint-modeling-introduction-r.html#types-of-jpoint-models",
    "href": "02-07-06-00-survival-analysis-joint-modeling-introduction-r.html#types-of-jpoint-models",
    "title": "6. Joint Modeling in Survival Analysis",
    "section": "Types of Jpoint Models",
    "text": "Types of Jpoint Models\nBelow is a concise, systematic classification of the main types of Joint Models (JM) used in Survival Analysis.\nAll share the idea of linking a longitudinal sub-model with a time-to-event sub-model via shared random effects, but they differ in:\n\nLongitudinal process\n\nSurvival process\nAssociation (link) structure\n\n\nBy Longitudinal Sub-model\n\n\n\n\n\n\n\n\nType\nFormula (simplified)\nWhen to use\n\n\n\n\nLinear Mixed-Effects JM\n\\(y_i(t) = X_i(t)\\beta + Z_i(t)b_i + \\varepsilon_i(t)\\)\nContinuous biomarker with linear trajectory (e.g., eGFR, CD4)\n\n\nNon-linear Mixed-Effects JM\n\\(y_i(t) = f(t;b_i) + \\varepsilon_i(t)\\) (splines, NLME)\nCurvilinear trajectories (e.g., tumor growth, PSA)\n\n\nGeneralized Linear Mixed JM (GLMM-JM)\n\\(g(\\mu_i(t)) = X_i(t)\\beta + Z_i(t)b_i\\)\nBinary, count, ordinal longitudinal outcomes\n\n\nLatent-Class / Mixture JM\nSubjects grouped into latent trajectories\nHeterogeneous populations (e.g., fast vs slow decliners)\n\n\nMultivariate JM\nSeveral longitudinal outcomes jointly\nMultiple biomarkers (e.g., CD4 + viral load)\n\n\n\n\n\nBy Survival Sub-model\n\n\n\n\n\n\n\n\nType\nHazard form\nUse case\n\n\n\n\nRelative Risk (Multiplicative) JM (most common)\n\\(h_i(t) = h_0(t) \\exp(\\mathbf{w}_i\\gamma + \\alpha m_i(t))\\)\nStandard Cox-type with time-varying effect\n\n\nAdditive Hazard JM\n\\(h_i(t) = h_0(t) + \\mathbf{w}_i\\gamma + \\alpha m_i(t)\\)\nWhen absolute risk difference is of interest\n\n\nAccelerated Failure Time (AFT) JM\nLog-time = linear in covariates + random effects\nWhen acceleration of time is the focus\n\n\nCompeting Risks JM\nMultiple cause-specific hazards share random effects\nMultiple event types (death vs transplant)\n\n\nRecurrent Events JM\nGap-time or counting process with frailty\nRepeated hospitalizations\n\n\n\n\n\nBy Association (Link) Structure\n\n\n\n\n\n\n\n\nLink Type\nFunctional form\nInterpretation\n\n\n\n\nCurrent Value\n\\(\\alpha \\cdot m_i(t)\\)\nRisk depends on current true level\n\n\nCurrent Slope\n\\(\\alpha \\cdot \\frac{d}{dt}m_i(t)\\)\nRisk tied to rate of change\n\n\nArea Under Curve (AUC)\n\\(\\alpha \\cdot \\int_0^t m_i(s)ds\\)\nCumulative exposure\n\n\nRandom Intercept Only\n\\(\\alpha b_{i0}\\)\nBaseline heterogeneity drives risk\n\n\nRandom Slope Only\n\\(\\alpha b_{i1}\\)\nTrajectory speed drives risk\n\n\nBoth Intercept & Slope\n\\(\\alpha_1 b_{i0} + \\alpha_2 b_{i1}\\)\nFull trajectory shape\n\n\nTime-Lagged Value\n\\(\\alpha m_i(t-\\tau)\\)\nDelayed effect (e.g., treatment lag)\n\n\nInteraction Form\n\\(\\alpha(t) m_i(t)\\)\nTime-varying association\n\n\n\n\n\nPopular Named/Standardized JMs\n\n\n\n\n\n\n\n\nName\nSoftware\nKey Feature\n\n\n\n\nStandard JM (Rizopoulos)\nJM, JMbayes2 (R)\nRelative risk + LMM + flexible link\n\n\nDynamic JM\nJMbayes2\nReal-time personalized predictions\n\n\nMultivariate JM\njoineRML (R)\nMultiple longitudinal outcomes\n\n\nCompeting Risks JM\nJMbayes2, frailtypack\nCause-specific hazards\n\n\nShared Parameter JM (SPJM)\nGeneral term\nAny shared random effect model\n\n\nLatent Class JM\nlcmm + JMbayes2\nTrajectory classes\n\n\n\n\n\nQuick Decision Tree\nIs longitudinal outcome:\n├── Continuous & linear? → Linear Mixed JM (Current value/slope)\n├── Continuous & nonlinear? → NLME JM (splines)\n├── Binary/count? → GLMM-JM\n└── Multiple biomarkers? → Multivariate JM\n\nIs survival:\n├── Single event? → Relative Risk JM\n├── Competing risks? → CR-JM\n└── Recurrent? → Recurrent Events JM\n\nWhat drives risk?\n├── Level now? → Current value\n├── Rate of change? → Slope\n└── Cumulative? → AUC",
    "crumbs": [
      "**Joint Modeling**",
      "Introduction to Joint Modeling in Survival Analysis"
    ]
  },
  {
    "objectID": "02-07-06-00-survival-analysis-joint-modeling-introduction-r.html#summary-and-conclusion",
    "href": "02-07-06-00-survival-analysis-joint-modeling-introduction-r.html#summary-and-conclusion",
    "title": "6. Joint Modeling in Survival Analysis",
    "section": "Summary and Conclusion",
    "text": "Summary and Conclusion\nJoint models provide a robust framework for analyzing complex datasets where longitudinal measurements and time-to-event outcomes are interrelated. By capturing the dependency between these two data types, joint models enhance our understanding of disease progression, improve prediction accuracy, and facilitate better decision-making in clinical and research settings. As computational methods and software continue to advance, joint modeling is becoming increasingly accessible and widely used across various fields.",
    "crumbs": [
      "**Joint Modeling**",
      "Introduction to Joint Modeling in Survival Analysis"
    ]
  },
  {
    "objectID": "02-07-06-00-survival-analysis-joint-modeling-introduction-r.html#resources",
    "href": "02-07-06-00-survival-analysis-joint-modeling-introduction-r.html#resources",
    "title": "6. Joint Modeling in Survival Analysis",
    "section": "Resources",
    "text": "Resources\n\nRizopoulos, D. (2012). Joint Models for Longitudinal and Time-to-Event Data. Chapman & Hall.\nJMbayes2 package vignette: https://drizopoulos.github.io/JMbayes2/",
    "crumbs": [
      "**Joint Modeling**",
      "Introduction to Joint Modeling in Survival Analysis"
    ]
  },
  {
    "objectID": "02-07-06-01-survival-analysis-standard-joint-model-r.html#overview",
    "href": "02-07-06-01-survival-analysis-standard-joint-model-r.html#overview",
    "title": "6.1 Standard (Shared Random Effects) Joint Model",
    "section": "Overview",
    "text": "Overview\nIn a mixed-effects longitudinal model, random effects (e.g., random intercepts/slopes) represent individual deviations from the population average trajectory. In a joint model, these same random effects are included in the survival submodel to explain differences in event risk. This “sharing” induces correlation between the longitudinal and event processes.\nThe standard joint model consists of two submodels:\nLongitudinal Submodel Typically a Linear Mixed-Effects Model)\n\\[\ny_i(t) = m_i(t) + \\varepsilon_i(t) = \\underbrace{x_i^\\top(t) \\beta}_{\\text{Fixed effects}} + \\underbrace{z_i^\\top(t) b_i}_{\\text{Random effects}} + \\varepsilon_i(t)\n\\]\n\n\\(y_i(t)\\): observed longitudinal outcome for subject \\(i\\) at time \\(t\\)\n\\(x_i(t)\\): vector of fixed-effect covariates (e.g., time, treatment)\n\\(\\beta\\): fixed-effect coefficients\n\\(z_i(t)\\): design vector for random effects (often includes intercept and time)\n\\(b_i \\sim \\mathcal{N}(0, D)\\): subject-specific random effects (e.g., random intercept \\(b_{0i}\\), random slope \\(b_{1i}\\)\n\\(\\varepsilon_i(t) \\sim \\mathcal{N}(0, \\sigma^2)\\): measurement error, independent of \\(b_i\\))\n\nThe true (unobserved) longitudinal trajectory is:\n\\[\nm_i(t) = x_i^\\top(t)\\beta + z_i^\\top(t)b_i\n\\]\nSurvival (Event Time) Submodel (Typically a relative risk model with baseline hazard)\n\\[\nh_i(t) = h_0(t) \\exp\\left( w_i^\\top \\gamma + \\alpha \\cdot \\eta\\big(m_i(t), b_i, t\\big) \\right)\n\\]\n\n\\(h_i(t)\\): hazard of the event for subject \\(i\\) at time \\(t\\)\n\\(h_0(t)\\): baseline hazard function (can be parametric like Weibull, or semi-parametric using splines)\n$w_i ): vector of baseline/time-independent covariates (e.g., age, treatment group)\n\\(gamma\\): log-hazard ratios for covariates\n\\(\\alpha\\): association parameter — quantifies how the longitudinal process affects event risk\n\\(\\eta(\\cdot)\\)): association structure linking longitudinal and survival processes\n\n\nCommon Association Structures \\(\\eta(\\cdot)\\)\nThe choice of \\(\\eta\\) defines how the biomarker influences risk:\n\n\n\n\n\n\n\n\nAssociation Type\nForm\nInterpretation\n\n\n\n\nCurrent value\n\\(\\eta = m_i(t)\\)\nInstantaneous biomarker level affects current hazard\n\n\nCurrent slope\n\\(\\eta = m_i'(t) = \\frac{d}{dt}m_i(t)\\)\nRate of change (e.g., rapid decline) affects risk\n\n\nRandom effects\n\\(\\eta = b_i\\)\nSubject’s deviation from population (e.g., low baseline) affects baseline hazard\n\n\nCombined\n\\(\\eta = m_i(t) + m_i'(t)\\)\nBoth level and trend matter\n\n\n\n\nThe current value parameterization is the most common and often serves as the “standard” joint model.\n\n\n\nKey Assumptions\n\nConditional independence: Given the random effects ( b_i ) (and thus ( m_i(t) )), the longitudinal and event processes are independent.\nNormality: Random effects ( b_i (0, D) ); measurement errors ( _i (0, ^2) ).\nCorrect specification of the association structure and baseline hazard.\n\n\n\nEstimation\nTypically done via maximum likelihood:\n\\[\nL(\\theta) = \\prod_{i=1}^n \\left[ \\int f(y_i \\mid b_i; \\theta_L) \\cdot f(T_i, \\delta_i \\mid b_i; \\theta_S) \\cdot f(b_i; D)  \\, db_i \\right]\n\\] - \\(\\theta = (\\theta_L, \\theta_S, D, \\sigma^2, \\alpha)\\): full parameter vector - Integration over ( b_i ) is done numerically (e.g., Gauss-Hermite quadrature or Monte Carlo) - Alternatively, Bayesian MCMC can be used\n\n\nAdvantages\n\nReduces bias from informative dropout (e.g., patients with worsening biomarkers drop out and die)\nAccounts for measurement error in biomarkers\nEnables dynamic predictions: update survival probability as new longitudinal data arrive\nProvides mechanistic insight: quantifies how biomarker trajectories drive risk\n\n\n\nExample\n\nStudy: Prostate cancer patients monitored via repeated PSA (prostate-specific antigen) tests; event = metastasis.\n\nLongitudinal: PSA modeled with random intercept/slope.\nSurvival: Hazard of metastasis depends on current true PSA level.\nResult: Higher PSA trajectory → significantly increased metastasis risk (( &gt; 0 )).\n\n\nThis fits the standard shared random effects joint model with current-value association.",
    "crumbs": [
      "**Joint Modeling**",
      "Standard (Shared Random Effects) Joint Model"
    ]
  },
  {
    "objectID": "02-07-06-01-survival-analysis-standard-joint-model-r.html#implementation-in-r",
    "href": "02-07-06-01-survival-analysis-standard-joint-model-r.html#implementation-in-r",
    "title": "6.1 Standard (Shared Random Effects) Joint Model",
    "section": "Implementation in R",
    "text": "Implementation in R\nStandard joint models can be implemented in R using several packages, including {JM}, {joineR}, and {JMbayes}. Below, we provide an example of how to fit a standard joint model using the {JM} and {joineR} package.\n\nInstall Required R Packages\nFollowing R packages are required to run this notebook. If any of these packages are not installed, you can install them using the code below:\n\n\nCode\npackages &lt;-c(\n         'tidyverse',\n         'survival',\n         'survminer',\n         'ggsurvfit',\n         'tidycmprsk',\n         'ggfortify',\n         'timereg',\n         'cmprsk',\n         'condSURV',\n         'riskRegression',\n         'prodlim',\n         'lava',\n         'mstate',\n         'regplot',\n         'cmprskcoxmsm',\n         'GLMMadaptive',\n         'nlme',\n         'lme4',\n         'lattice',\n         'JM',\n         'joineR',\n         'joineRML',\n         'JMbayes2'\n         \n         )\n\n\n```{r\n#| warning: false #| error: false",
    "crumbs": [
      "**Joint Modeling**",
      "Standard (Shared Random Effects) Joint Model"
    ]
  },
  {
    "objectID": "02-07-06-01-survival-analysis-standard-joint-model-r.html#the-jm-package",
    "href": "02-07-06-01-survival-analysis-standard-joint-model-r.html#the-jm-package",
    "title": "6.1 Standard (Shared Random Effects) Joint Model",
    "section": "The {JM} Package",
    "text": "The {JM} Package\nThe {JM} package is a comprehensive tool for fitting joint models of longitudinal and time-to-event data in R. It provides functions for estimating the parameters of the longitudinal and survival submodels, as well as the association structure linking the two. The package also offers tools for making predictions and assessing the model’s performance. Below, we demonstrate how to use the {JM} package to fit joint models to longitudinal and time-to-event data.\nThe joint model consists of two submodels: a linear mixed model for the longitudinal CD4 cell counts and a Cox proportional hazards model for the survival outcomes. The two submodels are linked by a shared parameter that captures the association between the longitudinal and survival processes.\n\nData\nWe’ll use the aids dataset from the {JM} package to demonstrate how to perform joint modeling of longitudinal and time-to-event data in R. A randomized clinical trial in which both longitudinal and survival data were collected to compare the efficacy and safety of two antiretroviral drugs in treating patients who had failed or were intolerant of zidovudine (AZT) therapy. The dataset contains information on patients’ CD4 cell counts (CD4) and survival status (death) over time, along with other covariates. The goal is to model the relationship between CD4 cell counts and survival outcomes.\nThe dataset used is the same that the one seen with the mixed model, aids. The survival information can be found in aids.id.\nA data frame with 1408 observations on the following 9 variables.\npatiet: patients identifier; in total there are 467 patients.\nTime: the time to death or censoring.\ndeath: a numeric vector with 0 denoting censoring and 1 death.\nCD4: the CD4 cells count.\nobstime: the time points at which the CD4 cells count was recorded.\ndrug: a factor with levels ddC denoting zalcitabine and ddI denoting didanosine.\ngender: a factor with levels female and male.\nprevOI: a factor with levels AIDS denoting previous opportunistic infection (AIDS diagnosis) at study entry, and noAIDS denoting no previous infection.\nAZT: a factor with levels intolerance and failure denoting AZT intolerance and AZT failure, respectively.\n\n\nCode\ndata(\"aids\", package = \"JM\")\nglimpse(aids)\n\n\nRows: 1,405\nColumns: 12\n$ patient &lt;fct&gt; 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 4, 4, 4, 4, 5, 5, 5, 5, 6, 7, 7,…\n$ Time    &lt;dbl&gt; 16.97, 16.97, 16.97, 19.00, 19.00, 19.00, 19.00, 18.53, 18.53,…\n$ death   &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,…\n$ CD4     &lt;dbl&gt; 10.677078, 8.426150, 9.433981, 6.324555, 8.124038, 4.582576, 5…\n$ obstime &lt;int&gt; 0, 6, 12, 0, 6, 12, 18, 0, 2, 6, 0, 2, 6, 12, 0, 2, 6, 12, 0, …\n$ drug    &lt;fct&gt; ddC, ddC, ddC, ddI, ddI, ddI, ddI, ddI, ddI, ddI, ddC, ddC, dd…\n$ gender  &lt;fct&gt; male, male, male, male, male, male, male, female, female, fema…\n$ prevOI  &lt;fct&gt; AIDS, AIDS, AIDS, noAIDS, noAIDS, noAIDS, noAIDS, AIDS, AIDS, …\n$ AZT     &lt;fct&gt; intolerance, intolerance, intolerance, intolerance, intoleranc…\n$ start   &lt;int&gt; 0, 6, 12, 0, 6, 12, 18, 0, 2, 6, 0, 2, 6, 12, 0, 2, 6, 12, 0, …\n$ stop    &lt;dbl&gt; 6.00, 12.00, 16.97, 6.00, 12.00, 18.00, 19.00, 2.00, 6.00, 18.…\n$ event   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,…\n\n\n\n\nCode\ndata(\"aids.id\", package = \"JM\")\nglimpse(aids.id)\n\n\nRows: 467\nColumns: 12\n$ patient &lt;fct&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18,…\n$ Time    &lt;dbl&gt; 16.97, 19.00, 18.53, 12.70, 15.13, 1.90, 14.33, 9.57, 11.57, 1…\n$ death   &lt;int&gt; 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1,…\n$ CD4     &lt;dbl&gt; 10.677078, 6.324555, 3.464102, 3.872983, 7.280110, 4.582576, 6…\n$ obstime &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ drug    &lt;fct&gt; ddC, ddI, ddI, ddC, ddI, ddC, ddC, ddI, ddC, ddI, ddC, ddI, dd…\n$ gender  &lt;fct&gt; male, male, female, male, male, female, male, female, male, ma…\n$ prevOI  &lt;fct&gt; AIDS, noAIDS, AIDS, AIDS, AIDS, AIDS, AIDS, noAIDS, AIDS, AIDS…\n$ AZT     &lt;fct&gt; intolerance, intolerance, intolerance, failure, failure, failu…\n$ start   &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ stop    &lt;dbl&gt; 6.00, 6.00, 2.00, 2.00, 2.00, 1.90, 2.00, 2.00, 2.00, 2.00, 2.…\n$ event   &lt;dbl&gt; 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,…\n\n\n\n\nCode\nhist(aids$CD4)\n\n\n\n\n\n\n\n\n\nThe CD4 cell exhibits right skewed shapes of distribution, and therefore, for the remainder of this analysis we will work with the square root of the CD4 cell values\n\n\nCode\nhist(sqrt(aids$CD4))\n\n\n\n\n\n\n\n\n\n\n\nCode\nlattice::xyplot(sqrt(CD4) ~ obstime | patient, group = patient, \n       data = aids[aids$patient %in% c(1:10),], \n       xlab = \"Months\", ylab = expression(sqrt(\"CD4\")), col = 1, type = \"b\")\n\n\n\n\n\n\n\n\n\n\n\nFit a Longitudinal Model (lmeObject)\nFirst, we fit a linear mixed model for the longitudinal CD4 cell counts using the lme() function from the {nlme} package. This model captures the trajectory of CD4 cell counts over time, accounting for patient-specific variability.\n\n\nCode\n# linear mixed model fit (random intercepts)\nlmeFit_01 &lt;- lme(sqrt(CD4) ~ obstime : drug, random = ~ 1 | patient, data = aids)\nsummary(lmeFit_01)\n\n\nLinear mixed-effects model fit by REML\n  Data: aids \n       AIC      BIC    logLik\n  2730.729 2756.957 -1360.364\n\nRandom effects:\n Formula: ~1 | patient\n        (Intercept)  Residual\nStdDev:   0.8719493 0.4106749\n\nFixed effects:  sqrt(CD4) ~ obstime:drug \n                     Value  Std.Error  DF  t-value p-value\n(Intercept)      2.5087153 0.04306984 936 58.24761       0\nobstime:drugddC -0.0338783 0.00352643 936 -9.60697       0\nobstime:drugddI -0.0282970 0.00360358 936 -7.85247       0\n Correlation: \n                (Intr) obst:C\nobstime:drugddC -0.153       \nobstime:drugddI -0.145  0.022\n\nStandardized Within-Group Residuals:\n        Min          Q1         Med          Q3         Max \n-5.09514877 -0.46243667  0.01324625  0.48993786  5.17139372 \n\nNumber of Observations: 1405\nNumber of Groups: 467 \n\n\n\n\nFit a Survival Model (survObject)\nNext, we fit a Cox proportional hazards model for the survival outcomes using the coxph() function from the survival package. This model estimates the hazard of death based on the CD4 cell counts and other covariates.\nThe survival submodel include: treatment effect (as a time-independent covariate) and the true underlying effect of CD4 cell count as estimated from the longitudinal model (as time-dependent). The baseline risk function is assumed piecewise constant.\n\n\nCode\n# Fit the survival model\nsurvFit &lt;- coxph(Surv(Time, death) ~ drug, data = aids.id, x = TRUE)\nsummary(survFit)\n\n\nCall:\ncoxph(formula = Surv(Time, death) ~ drug, data = aids.id, x = TRUE)\n\n  n= 467, number of events= 188 \n\n          coef exp(coef) se(coef)     z Pr(&gt;|z|)\ndrugddI 0.2102    1.2339   0.1462 1.437    0.151\n\n        exp(coef) exp(-coef) lower .95 upper .95\ndrugddI     1.234     0.8104    0.9264     1.643\n\nConcordance= 0.531  (se = 0.019 )\nLikelihood ratio test= 2.07  on 1 df,   p=0.2\nWald test            = 2.07  on 1 df,   p=0.2\nScore (logrank) test = 2.07  on 1 df,   p=0.1\n\n\n\n\nFit the Joint Model\nThe jointModel() function is used to fit the joint model, specifying the longitudinal and survival submodels, as well as the time variable (obstime) linking the two.\nThis jointModel() function fits shared parameter models for the joint modelling of normal longitudinal responses and time-to-event data under a maximum likelihood approach. Various options for the survival model are available.\njointModel(lmeObject, survObject, timeVar, \n  parameterization = c(\"value\", \"slope\", \"both\"),\n  method = c(\"weibull-PH-aGH\", \"weibull-PH-GH\", \"weibull-AFT-aGH\", \n    \"weibull-AFT-GH\", \"piecewise-PH-aGH\", \"piecewise-PH-GH\", \n    \"Cox-PH-aGH\", \"Cox-PH-GH\", \"spline-PH-aGH\", \"spline-PH-GH\", \n    \"ch-Laplace\"),\n  interFact = NULL, derivForm = NULL, lag = 0, scaleWB = NULL,\n  CompRisk = FALSE, init = NULL, control = list(), ...)\nmethod a character string specifying the type of joint model to fit. Various methods are available for the survival model:\nweibull-AFT-GH: a time-dependent Weibull model under the accelerated failure time formulation is assumed. weibull-PH-GH: a time-dependent relative risk model is postulated with a Weibull baseline risk function. piecewise-PH-GH: a time-dependent relative risk model is postulated with a piecewise constant baseline risk function. spline-PH-GH: a time-dependent relative risk model is assumed in which the log baseline risk function is approximated using B-splines. ch-Laplace: an additive model on the log cumulative hazard scale is assumed (see Rizopoulos et al., 2009 for more info). Cox-PH-GH a time-dependent relative risk model is assumed where the baseline risk function is left unspecified (Wulfsohn and Tsiatis, 1997\nFinally, we fit the joint model that links the longitudinal and survival submodels using the jointModel() function from the JM package. This model provides insights into how the CD4 cell counts influence the risk of death over time.\nWe will use method as piecewise-PH-GH to fit a time-dependent relative risk model with a piecewise constant baseline risk function.\n\n\nCode\n# Fit the joint model with random intercepts\nJMfit_01 &lt;- jointModel(lmeFit_01, survFit, timeVar = \"obstime\", method = \"piecewise-PH-GH\")\n\n\nsummary() retrieves the summary of the joint model summary, variance components, and the association parameter, coefficients of longitudinal and time-to-event processs:\n\n\nCode\nsummary(JMfit_01)\n\n\n\nCall:\njointModel(lmeObject = lmeFit_01, survObject = survFit, timeVar = \"obstime\", \n    method = \"piecewise-PH-GH\")\n\nData Descriptives:\nLongitudinal Process        Event Process\nNumber of Observations: 1405    Number of Events: 188 (40.3%)\nNumber of Groups: 467\n\nJoint Model Summary:\nLongitudinal Process: Linear mixed-effects model\nEvent Process: Relative risk model with piecewise-constant\n        baseline risk function\nParameterization: Time-dependent \n\n   log.Lik      AIC      BIC\n -2135.224 4298.447 4356.496\n\nVariance Components:\n               StdDev\n(Intercept) 0.8797564\nResidual    0.4251247\n\nCoefficients:\nLongitudinal Process\n                  Value Std.Err z-value p-value\n(Intercept)      2.4987  0.0457 54.6228 &lt;0.0001\nobstime:drugddC -0.0361  0.0037 -9.8114 &lt;0.0001\nobstime:drugddI -0.0314  0.0037 -8.4558 &lt;0.0001\n\nEvent Process\n            Value Std.Err z-value p-value\ndrugddI    0.3466  0.1527  2.2697  0.0232\nAssoct    -1.0945  0.1192 -9.1841 &lt;0.0001\nlog(xi.1) -1.6430  0.2543 -6.4603        \nlog(xi.2) -1.2659  0.2517 -5.0283        \nlog(xi.3) -0.9151  0.2991 -3.0597        \nlog(xi.4) -1.4505  0.3839 -3.7782        \nlog(xi.5) -1.3336  0.3597 -3.7072        \nlog(xi.6) -1.3264  0.4310 -3.0774        \nlog(xi.7) -1.3026  0.5386 -2.4186        \n\nIntegration:\nmethod: Gauss-Hermite\nquadrature points: 15 \n\nOptimization:\nConvergence: 0 \n\n\nNow will fit the with random intercepts + random slopes\n\n\nCode\nlmeFit_02 &lt;- lme(sqrt(CD4) ~ obstime : drug, random = ~ obstime | patient, data = aids)\n\n\n\n\nCode\n# Fit the joint model with intercepts + random slopes\nJMfit_02 &lt;- jointModel(lmeFit_02, survFit, timeVar = \"obstime\", method = \"piecewise-PH-GH\")\n\n\n\n\nModel Comparison\nanova() function can be used to compare the two models:\n\n\nCode\nanova(JMfit_01, JMfit_02)\n\n\n\n             AIC     BIC  log.Lik   LRT df p.value\nJMfit_01 4298.45 4356.50 -2135.22                 \nJMfit_02 4247.29 4313.64 -2107.65 55.15  2 &lt;0.0001\n\n\nLongitudinal submodels with random intercepts and random slopes are compared using the anova() function. The results show that the model with random intercepts and slopes is significantly better than the model with random intercepts only.\n\n\nPlotting Marginal Survival Curves\n\n\nCode\nplot(JMfit_02, 3, add.KM = TRUE, col = 2, lwd = 2,\n     main = \"Piecewise-constant Baseline Risk Function\", \n     ylab = \"Survival Probability\")\n\n\n\n\n\n\n\n\n\nsurvfit() function of {survial} pcakage creates survival curves from either a formula (e.g. the Kaplan-Meier), a previously fitted Cox model, or a previously fitted accelerated failure time model.\n\n\nCode\nres.surv &lt;- residuals(JMfit_02, process = \"Event\", type = \"Cox\")\nsfit &lt;- survfit(Surv(res.surv,JMfit_02$y$d) ~ 1)\nplot(sfit, mark.time = FALSE, conf.int = TRUE, lty = 1:2, \n    xlab = \"Cox-Snell Residuals\", ylab = \"Survival Probability\", \n    main = \"Piecewise-constant Baseline Risk Function\")\ncurve(pexp(x, lower.tail = FALSE), from = min(res.surv), to = max(res.surv), \n    add = TRUE, col = \"red\", lwd = 2)\nlegend(\"topright\", c(\"Survival function of unit Exponential\", \n    \"Survival function of Cox-Snell Residuals\"), lty = 1, lwd = 2, col = 2:1, bty = \"n\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nCox-Snell residuals are a diagnostic tool used in survival analysis to assess the fit of a survival model, such as a Cox proportional hazards model or parametric survival models. They transform survival data into residuals that should ideally follow a standard exponential distribution with a mean of 1, under the assumption that the model is correctly specified.\n\n\n\n\nHazard Ratio\nThe cinfint() function calculates the hazard ratio for the treatment effect in the survival model. This ratio quantifies the relative risk of death between the two treatment groups.\n\n\nCode\nexp(confint(JMfit_02, parm = \"Event\"))\n\n\n            2.5 %      est.    97.5 %\ndrugddI 1.0510576 1.4206752 1.9202736\nAssoct  0.2637343 0.3323346 0.4187786\n\n\nwe also compare with the time-dependent Cox model\n\n\nCode\nexp(confint(survFit))\n\n\n            2.5 %   97.5 %\ndrugddI 0.9264493 1.643414\n\n\n\n\nExpected survival probabilities\nsurvfitJM() function calculates the expected survival probabilities for each patient at specific time points based on the joint model.\nHere we compute the expected survival probabilities for two patients (5, 141) in the data set who has not died by the time of loss to follow-up. The function assumes that the patient has survived up to the last time point \\(t\\) in newdata for which a CD4 measurement was recorded, and will produce survival probabilities for a set of predefined \\(u &gt; t\\) values\n\n\nCode\nset.seed(300716) # it uses Monte Carlo samples\npred &lt;- survfitJM(JMfit_02, newdata = aids[aids$patient %in% c(\"5\", \"141\"), ],\n          idVar = \"patient\")  # last.time = \"Time\"\n\n\n\n\nCode\npar(mfrow=c(1,2))\nplot(pred, which = \"5\", conf.int = TRUE)\nplot(pred, which = \"141\", conf.int = TRUE, \n     fun = function (x) -log(x), ylab = \"Cumulative Risk\")\n\n\n\n\n\n\n\n\n\n\n\nPrediction\n\n\nCode\nDF &lt;- with(aids, expand.grid(drug = levels(drug),\n    obstime = seq(min(obstime), max(obstime), len = 100)))\nPs &lt;- predict(JMfit_02, DF, interval = \"confidence\", return = TRUE)\nrequire(lattice)\nxyplot(pred + low + upp ~ obstime | drug, data = Ps,\n    type = \"l\", col = c(2,1,1), lty = c(1,2,2), lwd = 2,\n    ylab = \"Average log serum Bilirubin\")",
    "crumbs": [
      "**Joint Modeling**",
      "Standard (Shared Random Effects) Joint Model"
    ]
  },
  {
    "objectID": "02-07-06-01-survival-analysis-standard-joint-model-r.html#the-joiner-package",
    "href": "02-07-06-01-survival-analysis-standard-joint-model-r.html#the-joiner-package",
    "title": "6.1 Standard (Shared Random Effects) Joint Model",
    "section": "The {joineR} package",
    "text": "The {joineR} package\nThe {joineR} package is designed to facilitate the analysis of data from longitudinal studies, which involve collecting multiple measurements over time from the same subjects. Each subject’s data consists of a series of repeated measurements as well as a potentially censored time-to-event outcome, such as the time until a particular event occurs. To model the repeated measurements, the package utilizes a linear model that incorporates random effects, allowing for the analysis of individual variations over time. Additionally, a correlated error structure can be included to account for any dependencies among the measurements. For the time-to-event outcomes, the modeling framework employs a Cox proportional hazards model augmented with log-Gaussian frailty. This approach enables the analysis of the effects of covariates on the timing of events while accounting for unobserved heterogeneity among subjects. A key feature of this framework is the ability to capture stochastic dependence between the two components of the model. Specifically, it allows the Gaussian random effects from the linear model—representing subject-specific variations—to be correlated with the frailty term in the Cox proportional hazards model. This correlation can provide deeper insights into the relationships between the repeated measurements and the timing of events, leading to more comprehensive and nuanced interpretations of the\n\nData\nThe mental data-set relates to a study in which 150 patients suffering from chronic mental illness were randomised amongst three different drug treatments: placebo and two active drugs. A questionnaire instrument was used to assess each patient’s mental state at weeks 0, 1, 2, 4, 6 and 8 post-randomization. The data can be loaded with the command:\n\n\nCode\nlibrary(joineR)\ndata(\"mental\")\n\n\nA total of 150 subjects were enrolled in the study, but only 68 of them completed all measurements at weeks 0, 1, 2, 4, 6, and 8. The remaining participants left the study early for various reasons, some of which were believed to be related to their mental state. As a result, the dropout rate may provide informative insights. The data from the first five subjects can be accessed as usual.\n\n\nCode\nmental[1:5, ]\n\n\n  id Y.t0 Y.t1 Y.t2 Y.t4 Y.t6 Y.t8 treat n.obs surv.time cens.ind\n1  1   55   NA   NA   NA   NA   NA     2     1     0.704        0\n2  2   44   NA   NA   NA   NA   NA     1     1     0.740        0\n3  3   65   67   NA   NA   NA   NA     2     2     1.121        1\n4  4   64   56   NA   NA   NA   NA     2     2     1.224        1\n5  5   47   48   NA   NA   NA   NA     2     2     1.303        0\n\n\nThe data is stored in a balanced format, consisting of 150 rows (one for each subject) and 11 columns. These columns include a subject identifier, the measured values from the questionnaire taken at each of the six scheduled follow-up times, the treatment allocation, the count of non-missing measured values, an imputed dropout time, and a censoring indicator. The censoring indicator is coded as 1 for subjects who dropped out for reasons believed to be related to their mental health state and as 0 for all others. It’s important to note the distinction made between potentially informative dropout and censoring, with the latter being regarded as uninformative. Thus, the command:\n\n\nCode\ntable(mental$cens[is.na(mental$Y.t8)])\n\n\n\n 0  1 \n21 61 \n\n\nshows that 21 of the 82 dropouts did so for reasons unrelated to their mental health.\n\n\nConverting between balanced and unbalanced data-formats\nTwo functions are provided for converting objects from one format to another. The code below demonstrates how to convert the mental dataset from a balanced format to an unbalanced format. It also includes a mnemonic renaming of the column that now contains all the repeated measurements.\n\n\nCode\nmental.unbalanced &lt;- to.unbalanced(mental, id.col = 1,\n                                   times = c(0, 1, 2, 4, 6, 8),\n                                   Y.col = 2:7, other.col = 8:11)\nnames(mental.unbalanced)\n\n\n[1] \"id\"        \"time\"      \"Y.t0\"      \"treat\"     \"n.obs\"     \"surv.time\"\n[7] \"cens.ind\" \n\n\n\n\nCode\nnames(mental.unbalanced)[3] &lt;- \"Y\"\n\n\nThe following code converts the new object back to the balanced format:\n\n\nCode\nmental.balanced &lt;- to.balanced(mental.unbalanced, id.col = 1,\n                               time.col = 2,\n                               Y.col = 3, other.col = 4:7)\ndim(mental.balanced)\n\n\n[1] 150  11\n\n\nOnce a data-set is in the balanced format (if it is unbalanced then to.balanced can be applied) then the mean, variance and correlation matrix of the responses can be extracted using summarybal(). An example is given below using the mental data-set:\n\n\nCode\nsummarybal(mental, Y.col = 2:7, times = c(0, 1, 2, 4, 6, 8), \n           na.rm = TRUE)\n\n\n$mean.vect\n     x        y\nY.t0 0 55.77333\nY.t1 1 53.03378\nY.t2 2 50.37008\nY.t4 4 48.62963\nY.t6 6 46.94048\nY.t8 8 46.02941\n\n$variance\n    Y.t0     Y.t1     Y.t2     Y.t4     Y.t6     Y.t8 \n132.1228 152.7403 167.5366 168.6653 228.8759 181.4917 \n\n$cor.mtx\n          [,1]      [,2]      [,3]      [,4]      [,5]      [,6]\n[1,] 1.0000000 0.5939982 0.4537496 0.4186798 0.3432355 0.2997679\n[2,] 0.5939982 1.0000000 0.7793311 0.6805021 0.6631543 0.6149448\n[3,] 0.4537496 0.7793311 1.0000000 0.7992470 0.7077238 0.6202277\n[4,] 0.4186798 0.6805021 0.7992470 1.0000000 0.8108585 0.7402240\n[5,] 0.3432355 0.6631543 0.7077238 0.8108585 1.0000000 0.8681933\n[6,] 0.2997679 0.6149448 0.6202277 0.7402240 0.8681933 1.0000000\n\n\n\n\nExploring covariance structure\n\n\nCode\ny &lt;- as.matrix(mental[, 2:7]) \n# converts mental from list format to numeric matrix format\nmeans &lt;- matrix(0, 3, 6)\n\nfor (trt in 1:3) {\n   ysub &lt;- y[mental$treat == trt, ]\n   means[trt,] &lt;- apply(ysub, 2, mean, na.rm = TRUE)\n}\n\nresiduals &lt;- matrix(0, 150, 6)\n\nfor (i in 1:150) {\n   residuals[i,] &lt;- y[i,] - means[mental$treat[i], ]\n}\n\nV &lt;- cov(residuals, use = \"pairwise\")\nR &lt;- cor(residuals, use = \"pairwise\")\nround(cbind(diag(V), R), 3)\n\n\n        [,1]  [,2]  [,3]  [,4]  [,5]  [,6]  [,7]\n[1,] 131.774 1.000 0.612 0.472 0.464 0.391 0.321\n[2,] 142.171 0.612 1.000 0.766 0.663 0.650 0.603\n[3,] 159.711 0.472 0.766 1.000 0.792 0.712 0.624\n[4,] 153.364 0.464 0.663 0.792 1.000 0.799 0.738\n[5,] 198.350 0.391 0.650 0.712 0.799 1.000 0.861\n[6,] 167.886 0.321 0.603 0.624 0.738 0.861 1.000\n\n\n\n\nCreating a jointdata object\nA jointdata() object is a list that can contain up to three data frames, including repeated measurement data, time-to-event data, and baseline covariate data. The repeated measurement data must be stored in an unbalanced format, while the time-to-event and baseline covariate data should be stored in a balanced format, meaning there should be one line for each subject. Each data frame must include a column for the subject ID, and these subject ID columns across all three data frames must match. The UniqueVariables function offers a convenient way to extract the time-to-event and baseline covariate data from an unbalanced data frame, as demonstrated in the following example.\n\n\nCode\nmental.long &lt;- mental.unbalanced[, 1:3]\nmental.surv &lt;- UniqueVariables(mental.unbalanced, \n                               6:7, id.col = 1)\nmental.baseline &lt;- UniqueVariables(mental.unbalanced,\n                                   4, id.col = 1)\nmental.jd &lt;- jointdata(mental.long, \n                       mental.surv,\n                       mental.baseline,\n                       id.col = \"id\", \n                       time.col = \"time\")\n\n\n\n\nModel Fitting\nThe package includes functions for fitting two different classes of model using maximum likelihood estimation. The first is an extended version of the random effects model proposed by Wulfsohn and Tsiatis (1997). See Henderson et al. (2000). The second is the transformation model described by Diggle et al. (2008).\nThe random effects model is fitted using the joint() function. This generic function fits a joint model with random latent association, building on the formulation described in Wulfsohn and Tsiatis (1997) while allowing for the presence of longitudinal and survival covariates, and three choices for the latent process. The link between the longitudinal and survival processes can be proportional or separate. When failure is attributable to 2 separate causes, a competing risks joint model is fitted as per Williamson et al. (2008).\n\n\nCode\nmodel.jointrandom &lt;- joint(mental.jd, Y ~ 1 + time + treat, \n                           Surv(surv.time, cens.ind) ~ treat,\n                           model = \"int\")\nnames(model.jointrandom)\n\n\n [1] \"coefficients\" \"sigma.z\"      \"sigma.u\"      \"hazard\"       \"loglik\"      \n [6] \"numIter\"      \"convergence\"  \"model\"        \"sepassoc\"     \"sepests\"     \n[11] \"compRisk\"     \"sep.loglik\"   \"formulae\"     \"data\"         \"call\"        \n\n\nThere is a summary() method for a model fitted using joint. This produces a summarized version of the model fit, omitting some of the information contained within the fitted object itself. The summary method for a joint object is demonstrated below:\n\n\nCode\nsummary(model.jointrandom)\n\n\n\nCall:\njoint(data = mental.jd, long.formula = Y ~ 1 + time + treat, \n    surv.formula = Surv(surv.time, cens.ind) ~ treat, model = \"int\")\n\nRandom effects joint model\n Data: mental.jd \n Log-likelihood: -2884.045 \n\nLongitudinal sub-model fixed effects: Y ~ 1 + time + treat                      \n(Intercept) 61.9221415\ntime        -0.5969329\ntreat       -3.8544659\n\nSurvival sub-model fixed effects: Surv(surv.time, cens.ind) ~ treat                \ntreat -0.6001792\n\nLatent association:                  \ngamma_0 0.08654259\n\nVariance components:\n     U_0 Residual \n99.84760 64.34334 \n\nConvergence at iteration: 32 \n\nNumber of observations: 900 \nNumber of groups: 150 \n\n\nThe function jointSE() provides standard errors and confidence intervals for the parameters defining the mean response profiles in a random effects joint model. Currently, the calculation of standard errors for the random effect parameters is not implemented. However, approximate standard errors can be obtained through a parametric bootstrap, which involves re-estimating the model parameters from simulated realizations of the fitted model. The first two arguments for jointSE() are the results from calling the joint() function, with the corresponding jointdata object automatically stored as part of the model fit. The second argument allows you to specify the number of bootstrap samples to be taken. The remaining arguments are included for completeness; they mirror the last four arguments of the joint() function and provide an additional option to control the level of output displayed in the R terminal. Be aware that with a sufficiently large number of bootstrap samples, this function can be slow to execute. If you run the function with only 100 bootstrap samples, the output will reflect this number, and the confidence limits will default to zero unless at least 100 bootstrap samples are taken.\n\n\nCode\nmodel.jointrandom.se &lt;- jointSE(model.jointrandom, n.boot = 100)\nmodel.jointrandom.se\n\n\n     Component   Parameter Estimate      SE 95%Lower 95%Upper\n1 Longitudinal (Intercept)  61.9221  2.6208  56.7265  66.5462\n2                     time  -0.5969  0.1830  -0.9838  -0.1780\n3                    treat  -3.8545  1.1133  -6.0900  -1.6931\n4      Failure       treat  -0.6002  0.2125  -1.0297  -0.2766\n5  Association     gamma_0   0.0865  0.0173   0.0488   0.1255\n6     Variance         U_0  99.8476 11.6411  74.1943 117.1204\n7                 Residual  64.3433  6.6245  51.0678  76.9221",
    "crumbs": [
      "**Joint Modeling**",
      "Standard (Shared Random Effects) Joint Model"
    ]
  },
  {
    "objectID": "02-07-06-01-survival-analysis-standard-joint-model-r.html#summary-and-conclusions",
    "href": "02-07-06-01-survival-analysis-standard-joint-model-r.html#summary-and-conclusions",
    "title": "6.1 Standard (Shared Random Effects) Joint Model",
    "section": "Summary and Conclusions",
    "text": "Summary and Conclusions\nThis tutorial covered the essential concepts and practical applications of Standard (Shared Random Effects) Joint model of longitudinal and time-to-event data in R. We began by introducing the theoretical background of joint models and their relevance in clinical research. We then demonstrated how to fit univariate and multivariate joint models using {JM}, and {joineR} packages. We also discussed the importance of functional forms in joint models and how to specify them in the model fitting process. Additionally, we explored dynamic predictions and predictive accuracy metrics, such as ROC curves, calibration plots, and Brier scores. Finally, we introduced the concept of competing risks and illustrated how to fit joint models for competing risks data.\nThis tutorial helps researchers and practitioners understand the principles of joint modeling and provides practical guidance on implementing these models in R. By following the step-by-step instructions and examples provided in this tutorial, users can gain a comprehensive understanding of joint models and apply them to their own longitudinal and time-to-event data.",
    "crumbs": [
      "**Joint Modeling**",
      "Standard (Shared Random Effects) Joint Model"
    ]
  },
  {
    "objectID": "02-07-06-01-survival-analysis-standard-joint-model-r.html#resources",
    "href": "02-07-06-01-survival-analysis-standard-joint-model-r.html#resources",
    "title": "6.1 Standard (Shared Random Effects) Joint Model",
    "section": "Resources",
    "text": "Resources\n\nA Tutorial for Joint Modeling of Longitudinal and Time-to-Event Data in R\nJoint Models for Longitudinal and Time-to-Event Data with Applications in R by Dimitris Rizopoulos\nJoint Models for Longitudinal and Time-to-Event Data\nChapter 4 Joint Models for Longitudinal and Time-to-Event Data\njoineR\nMultivariate Joint Models\nJMbayes2: Extended Joint Models for Longitudinal and Time-to-Event Data",
    "crumbs": [
      "**Joint Modeling**",
      "Standard (Shared Random Effects) Joint Model"
    ]
  },
  {
    "objectID": "02-07-06-02-survival-analysis-baseline-hazard-function-r.html#implementing-different-baseline-hazard-specifications-in-r",
    "href": "02-07-06-02-survival-analysis-baseline-hazard-function-r.html#implementing-different-baseline-hazard-specifications-in-r",
    "title": "6.2 Baseline Hazard Function of Joint Models",
    "section": "Implementing Different Baseline Hazard Specifications in R",
    "text": "Implementing Different Baseline Hazard Specifications in R\nBaseline Hazard can be implemented using the {JMbayes2} package in R for Bayesian joint modeling of longitudinal and time-to-event (survival) data. It enables simultaneous analysis of repeated longitudinal measurements (e.g., biomarker trajectories) and event outcomes (e.g., death or disease progression) by linking them through shared random effects or other association structures. Built on Stan for efficient Hamiltonian Monte Carlo sampling, {JMbayes2} supports flexible model specifications, including linear, generalized linear, or nonlinear mixed-effects models for the longitudinal submodel and Cox or accelerated failure time models for the survival submodel.\n\nWe’ll use the AIDS dataset included in JMbayes2, which contains:\n\nLongitudinal CD4 cell counts (square-root transformed),\nTime-to-death with censoring,\nTreatment group (drug: ddC or ddI).\n\n\nInstall Required R Packages\nFollowing R packages are required to run this notebook. If any of these packages are not installed, you can install them using the code below:\n\n\nCode\npackages &lt;-c(\n         'tidyverse',\n         'survival',\n         'survminer',\n         'ggsurvfit',\n         'tidycmprsk',\n         'ggfortify',\n         'timereg',\n         'cmprsk',\n         'condSURV',\n         'riskRegression',\n         'prodlim',\n         'lava',\n         'mstate',\n         'regplot',\n         'cmprskcoxmsm',\n         'GLMMadaptive',\n         'nlme',\n         'lme4',\n         'lattice',\n         'JM',\n         'joineR',\n         'joineRML',\n         'JMbayes2'\n         \n         )\n\n\n```{r\n#| warning: false #| error: false",
    "crumbs": [
      "**Joint Modeling**",
      "Baseline Hazard Function of Joint Models"
    ]
  },
  {
    "objectID": "02-07-06-02-survival-analysis-baseline-hazard-function-r.html#summary-and-conclusion",
    "href": "02-07-06-02-survival-analysis-baseline-hazard-function-r.html#summary-and-conclusion",
    "title": "6.2 Baseline Hazard Function of Joint Models",
    "section": "Summary and Conclusion",
    "text": "Summary and Conclusion\nIn joint modeling of longitudinal and time-to-event data, the baseline hazard function $ h_0(t) $ plays a crucial role in defining the risk of an event over time. The {JMbayes2} package in R provides flexible options to specify and estimate $ h_0(t) $, allowing researchers to tailor the model to their data and research questions. By choosing appropriate baseline hazard specifications—ranging from penalized B-splines to parametric forms like Weibull—analysts can capture the underlying hazard dynamics effectively, leading to more accurate inferences and predictions. Careful consideration of the baseline hazard structure, along with thorough model diagnostics, is essential for robust joint modeling analyses.",
    "crumbs": [
      "**Joint Modeling**",
      "Baseline Hazard Function of Joint Models"
    ]
  },
  {
    "objectID": "02-07-06-02-survival-analysis-baseline-hazard-function-r.html#resources",
    "href": "02-07-06-02-survival-analysis-baseline-hazard-function-r.html#resources",
    "title": "6.2 Baseline Hazard Function of Joint Models",
    "section": "Resources",
    "text": "Resources\n\nRizopoulos, D. (2025). Baseline Hazard Function. JMbayes2 Vignette.\nhttps://drizopoulos.github.io/JMbayes2/articles/Baseline_Hazard.html\nRizopoulos, D. (2012). Joint Models for Longitudinal and Time-to-Event Data. Chapman & Hall/CRC.",
    "crumbs": [
      "**Joint Modeling**",
      "Baseline Hazard Function of Joint Models"
    ]
  },
  {
    "objectID": "02-07-06-03-survival-analysis-causal-effects-r.html#overview",
    "href": "02-07-06-03-survival-analysis-causal-effects-r.html#overview",
    "title": "6.3 Causal Effects from Joint Models",
    "section": "Overview",
    "text": "Overview\nIn longitudinal studies, time-varying confounding and feedback between biomarkers and outcomes complicate causal inference. Joint models simultaneously model:\n\nA longitudinal submodel (e.g., biomarker trajectory)\nA survival submodel (e.g., time to death/transplant)\n\nBy linking these via shared random effects, joint models can be used to simulate counterfactual outcomes under hypothetical treatment scenarios—enabling estimation of causal effects.\n\nKey idea: Compare expected survival under two treatment regimes (e.g., always treated vs. never treated), adjusting for time-varying confounders via the longitudinal process.\n\n\nInstall Required R Packages\nFollowing R packages are required to run this notebook. If any of these packages are not installed, you can install them using the code below:\n\n\nCode\npackages &lt;-c(\n         'tidyverse',\n         'survival',\n         'survminer',\n         'ggsurvfit',\n         'tidycmprsk',\n         'ggfortify',\n         'timereg',\n         'cmprsk',\n         'condSURV',\n         'riskRegression',\n         'prodlim',\n         'lava',\n         'mstate',\n         'regplot',\n         'cmprskcoxmsm',\n         'GLMMadaptive',\n         'nlme',\n         'lme4',\n         'lattice',\n         'JM',\n         'joineR',\n         'joineRML',\n         'JMbayes2'\n         )\n\n\n```{r\n#| warning: false #| error: false",
    "crumbs": [
      "**Joint Modeling**",
      "Causal Effects from Joint Models"
    ]
  },
  {
    "objectID": "02-07-06-03-survival-analysis-causal-effects-r.html#summary-and-conclusion",
    "href": "02-07-06-03-survival-analysis-causal-effects-r.html#summary-and-conclusion",
    "title": "6.3 Causal Effects from Joint Models",
    "section": "Summary and Conclusion",
    "text": "Summary and Conclusion\nThis tutorial demonstrated how to estimate conditional causal effects using joint models fitted with the {JMbayes2} R package. By comparing predicted survival probabilities under different treatment regimes for a specific patient, we can assess the impact of treatments while accounting for time-varying confounders through the longitudinal process. This approach provides a powerful framework for causal inference in longitudinal studies with complex data structures.",
    "crumbs": [
      "**Joint Modeling**",
      "Causal Effects from Joint Models"
    ]
  },
  {
    "objectID": "02-07-06-03-survival-analysis-causal-effects-r.html#resources",
    "href": "02-07-06-03-survival-analysis-causal-effects-r.html#resources",
    "title": "6.3 Causal Effects from Joint Models",
    "section": "Resources",
    "text": "Resources\n\nOfficial Vignette:\nCausal Effects with JMbayes2\nSource Code Example:\ncausal_effects.R\nKey References:\n\nRizopoulos, D. (2012). Joint Models for Longitudinal and Time-to-Event Data. Chapman & Hall.\nRizopoulos, D. (2021). JMbayes2: Extended Joint Models for Longitudinal and Time-to-Event Data. R package.\nKeogh, R. H., & Morris, R. W. (2019). Causal inference with joint models. Statistical Methods in Medical Research.",
    "crumbs": [
      "**Joint Modeling**",
      "Causal Effects from Joint Models"
    ]
  },
  {
    "objectID": "02-07-06-04-survival-analysis-competing-risks-r.html#competing-risks-of-joint-models-in-r",
    "href": "02-07-06-04-survival-analysis-competing-risks-r.html#competing-risks-of-joint-models-in-r",
    "title": "6.4 Competing Risks of Joint Models",
    "section": "Competing Risks of Joint Models in R",
    "text": "Competing Risks of Joint Models in R\nWe use the pbc2 dataset from the {JMbayes2} package, which contains longitudinal measurements and survival data for patients with primary biliary cirrhosis (PBC). In this example, we will model the longitudinal biomarker serBilir (serum bilirubin levels) and the competing risks of death due to PBC and death due to other causes.\n\nInstall Required R Packages\nFollowing R packages are required to run this notebook. If any of these packages are not installed, you can install them using the code below:\n\n\nCode\npackages &lt;-c(\n         'tidyverse',\n         'survival',\n         'survminer',\n         'ggsurvfit',\n         'tidycmprsk',\n         'ggfortify',\n         'timereg',\n         'cmprsk',\n         'condSURV',\n         'riskRegression',\n         'prodlim',\n         'lava',\n         'mstate',\n         'regplot',\n         'cmprskcoxmsm',\n         'GLMMadaptive',\n         'nlme',\n         'lme4',\n         'lattice',\n         'JM',\n         'joineR',\n         'joineRML',\n         'JMbayes2'\n         )\n\n\n```{r\n#| warning: false #| error: false",
    "crumbs": [
      "**Joint Modeling**",
      "Competing Risks of Joint Models"
    ]
  },
  {
    "objectID": "02-07-06-04-survival-analysis-competing-risks-r.html#summary-and-conclusion",
    "href": "02-07-06-04-survival-analysis-competing-risks-r.html#summary-and-conclusion",
    "title": "6.4 Competing Risks of Joint Models",
    "section": "Summary and Conclusion",
    "text": "Summary and Conclusion\nIn this notebook, we explored the application of joint models for competing risks using the {JMbayes2} package in R. We demonstrated how to prepare the data for competing risks analysis, fit cause-specific Cox models, and specify the association structure between longitudinal biomarkers and competing event types. Finally, we illustrated how to obtain dynamic predictions for individual patients based on their longitudinal data and event history. Joint models for competing risks provide a powerful framework for understanding the complex relationships between longitudinal processes and multiple event types in survival analysis.",
    "crumbs": [
      "**Joint Modeling**",
      "Competing Risks of Joint Models"
    ]
  },
  {
    "objectID": "02-07-06-04-survival-analysis-competing-risks-r.html#resources",
    "href": "02-07-06-04-survival-analysis-competing-risks-r.html#resources",
    "title": "6.4 Competing Risks of Joint Models",
    "section": "Resources",
    "text": "Resources\n\nRizopoulos, D. (2012). Joint Models for Longitudinal and Time-to-Event Data: With Applications in R. Chapman and Hall/CRC.\nRizopoulos, D. (2024). JMbayes2: Joint Models for Longitudinal and Time-to-Event Data under a Bayesian Approach. R package version 0.9.6. https://CRAN.R-project.org/package=JMbayes2\nCompeting Risks",
    "crumbs": [
      "**Joint Modeling**",
      "Competing Risks of Joint Models"
    ]
  },
  {
    "objectID": "02-07-06-05-survival-analysis-dynamic-joint-model-r.html#dynamic-joint-model-in-r",
    "href": "02-07-06-05-survival-analysis-dynamic-joint-model-r.html#dynamic-joint-model-in-r",
    "title": "6.5 Dynamic Joint Model",
    "section": "Dynamic Joint Model in R",
    "text": "Dynamic Joint Model in R\nThis tutorial explains how to fit, analyze, and interpret a Dynamic Joint Model — that is, a joint model where the association between a longitudinal biomarker and survival outcome changes over time.\n\nInstall Required R Packages\nFollowing R packages are required to run this notebook. If any of these packages are not installed, you can install them using the code below:\n\n\nCode\npackages &lt;-c(\n         'tidyverse',\n         'survival',\n         'survminer',\n         'ggsurvfit',\n         'tidycmprsk',\n         'ggfortify',\n         'timereg',\n         'cmprsk',\n         'condSURV',\n         'riskRegression',\n         'prodlim',\n         'lava',\n         'mstate',\n         'regplot',\n         'cmprskcoxmsm',\n         'GLMMadaptive',\n         'nlme',\n         'lme4',\n         'lattice',\n         'JM',\n         'joineR',\n         'joineRML',\n         'JMbayes2'\n         \n         )\n\n\n```{r #| warning: false #| error: false",
    "crumbs": [
      "**Joint Models**",
      "Dynamic Joint Model"
    ]
  },
  {
    "objectID": "02-07-06-05-survival-analysis-dynamic-joint-model-r.html#fit-a-dynamic-joint-model-time-varying-association",
    "href": "02-07-06-05-survival-analysis-dynamic-joint-model-r.html#fit-a-dynamic-joint-model-time-varying-association",
    "title": "6.5 Dynamic Joint Model",
    "section": "Fit a Dynamic Joint Model (Time-Varying Association)",
    "text": "Fit a Dynamic Joint Model (Time-Varying Association)\nNow we allow the association parameter α(t) to vary with time.\nThis can be done by specifying a time-varying functional form using B-splines:\n\n\nCode\n# Dynamic (time-varying) association joint model\njmFit_dynamic &lt;- jm(CoxFit, lmeFit, time_var = \"obstime\",\n# Functional form for time-varying association\n  functional_forms = ~ value(CD4, form = \"splines\")\n)\nsummary(jmFit_dynamic)\n\n\n\nCall:\njm(Surv_object = CoxFit, Mixed_objects = lmeFit, time_var = \"obstime\", \n    functional_forms = ~value(CD4, form = \"splines\"))\n\nData Descriptives:\nNumber of Groups: 467       Number of events: 188 (40.3%)\nNumber of Observations:\n  sqrt(CD4): 1405\n\n                 DIC     WAIC      LPML\nmarginal    3995.836 4189.375 -2094.226\nconditional 5947.087 5766.982 -3372.603\n\nRandom-effects covariance matrix:\n                           \n       StdDev   Corr       \n(Intr) 0.8736 (Intr) n(,2)1\nn(,2)1 0.6458              \nn(,2)2 0.2382              \n\nSurvival Outcome:\n                    Mean  StDev    2.5%   97.5%      P   Rhat\ndrugddI           0.3599 0.1956 -0.0212  0.7373 0.0656 1.0030\ngendermale       -0.2506 0.2775 -0.7693  0.2943 0.3707 1.0016\nvalue(sqrt(CD4)) -1.0791 0.1137 -1.3077 -0.8617 0.0000 1.0106\n\nLongitudinal Outcome: sqrt(CD4) (family = gaussian, link = identity)\n                   Mean  StDev    2.5%   97.5%      P   Rhat\n(Intercept)      2.4981 0.0609  2.3790  2.6170 0.0000 1.0063\nns(obstime, 2)1 -0.7529 0.0879 -0.9208 -0.5750 0.0000 1.0385\nns(obstime, 2)2 -0.3964 0.1003 -0.6010 -0.2111 0.0000 1.2061\ndrugddI          0.0465 0.0872 -0.1280  0.2181 0.5787 1.0011\nn(,2)1           0.1905 0.1254 -0.0673  0.4259 0.1347 1.0047\nn(,2)2          -0.1435 0.1165 -0.3788  0.0638 0.2164 1.3814\nsigma            0.3766 0.0112  0.3561  0.3986 0.0000 1.1669\n\nMCMC summary:\nchains: 3 \niterations per chain: 3500 \nburn-in per chain: 500 \nthinning: 1 \ntime: 21 sec\n\n\n\nInterpretation:\n\n\nIn the standard JM, α is constant → one global effect of CD4 on survival.\nIn the dynamic JM, α(t) is modeled as a smooth spline → it can change over follow-up time.\n\n\nDynamic (Real-Time) Predictions\nOnce fitted, you can make personalized survival predictions that update as new data arrives.\nExample for a specific patient:\n\n\nCode\n# Select one patient’s data\npatientData &lt;- aids[aids$patient == \"100\", ]\n\n# Predict survival probability beyond 10 years, given data up to time = 5\npred_dynamic &lt;- predict(\n  object = jmFit_dynamic,\n  newdata = patientData,\n  process = \"event\",\n  Times = seq(5, 10, by = 0.5),  # times at which to predict survival\n  return_newdata = TRUE\n)\n\n\nYou can visualize the predicted survival curves:\n\n\nCode\nplot(pred_dynamic)\n\n\n\n\n\n\n\n\n\nThis curve updates as the patient’s CD4 history is extended — illustrating dynamic (real-time) prediction.\n\n\nDynamic predictive accuracy\nUsing the available longitudinal information up to a starting time point, tvROV fuctions compute estimates of the ROC curve and the AUC, the Brier score and expected predictive cross-entropy at a horizon time point based on joint models.\n\n\nCode\n# Evaluate Time-dependent ROC curve\nroc &lt;- tvROC(\n  jmFit_static,\n  newdata = aids,\n  Tstart = 5,   # Landmark time (e.g., year 5)\n  Dt = 3,       # Prediction horizon (5 → 8 years)\n  cores = 1L\n)\n\n\n\n\nCode\n# Print results\nroc\n\n\n\n    Time-dependent Sensitivity and Specificity for the Joint Model jmFit_static\n\nAt time: 8\nUsing information up to time: 5 (414 subjects still at risk)\nAccounting for censoring using model-based weights\n\n   cut-off      SN       SP  \n1     0.00 0.00000 1.000000  \n2     0.53 0.02857 1.000000  \n3     0.54 0.05714 1.000000  \n4     0.56 0.05714 0.997361  \n5     0.60 0.08571 0.997361  \n6     0.61 0.14285 0.997361  \n7     0.63 0.14285 0.994723  \n8     0.65 0.17142 0.994723  \n9     0.68 0.17142 0.992084  \n10    0.69 0.17142 0.989446  \n11    0.71 0.17142 0.986807  \n12    0.74 0.17142 0.981530  \n13    0.76 0.17142 0.978892  \n14    0.77 0.17142 0.976253  \n15    0.78 0.20000 0.968338  \n16    0.79 0.22857 0.947229  \n17    0.80 0.25714 0.944591  \n18    0.81 0.31428 0.934037  \n19    0.82 0.34285 0.928760  \n20    0.83 0.34285 0.907652  \n21    0.84 0.34285 0.881266  \n22    0.85 0.39999 0.852242  \n23    0.86 0.39999 0.828496  \n24    0.87 0.45713 0.786279  \n25    0.88 0.51427 0.749340  \n26    0.89 0.59999 0.715039  \n27    0.90 0.65713 0.667545  \n28    0.91 0.71427 0.630606  \n29    0.92 0.79998 0.583113  \n30    0.93 0.85712 0.530342 *\n31    0.94 0.85712 0.435355  \n32    0.95 0.85712 0.385223  \n33    0.96 0.94284 0.308706  \n34    0.97 0.97141 0.237465  \n35    0.98 1.00000 0.131926  \n36    0.99 1.00000 0.002639  \n37    1.00 1.00000 0.000000  \n\n\nCode\ntvAUC(roc)\n\n\n\n    Time-dependent AUC for the Joint Model jmFit_static\n\nEstimated AUC:  0.739\nAt time: 8\nUsing information up to time: 5 (414 subjects still at risk)\nAccounting for censoring using model-based weights\n\n\n\n\nCode\n# Plot ROC curve\nplot(roc, legend = TRUE, optimal_cutoff = \"Youden\")\n\n\n\n\n\n\n\n\n\n\n\nConcept Recap\n\n\n\n\n\n\n\n\nModel Type\nAssociation\nPurpose\n\n\n\n\nStandard JM\nConstant ( \\(\\alpha\\) )\nSimple, assumes biomarker effect constant\n\n\nDynamic JM\n( \\(\\alpha(t)\\)) varies with time\nCaptures changing biomarker–risk relationship\n\n\nDynamic Prediction\nUpdated survival probability using recent biomarker history\nPersonalized forecasts",
    "crumbs": [
      "**Joint Models**",
      "Dynamic Joint Model"
    ]
  },
  {
    "objectID": "02-07-06-05-survival-analysis-dynamic-joint-model-r.html#summary-and-conclusion",
    "href": "02-07-06-05-survival-analysis-dynamic-joint-model-r.html#summary-and-conclusion",
    "title": "6.5 Dynamic Joint Model",
    "section": "Summary and Conclusion",
    "text": "Summary and Conclusion\nThe Dynamic Joint Model extends the standard joint modeling framework by allowing the association between longitudinal biomarkers and survival risk to vary over time. This flexibility enables more accurate and personalized risk predictions that adapt as new biomarker data are collected. This tutorial demonstrated how to fit and interpret a Dynamic Joint Model in R using the JMbayes2 package. At the end of tutorial you should be able to:\n\nUnderstand the concept of dynamic joint modeling and its advantages over standard joint models.\nFit a dynamic joint model with time-varying association using R.\nGenerate and interpret dynamic survival predictions that update with new longitudinal data.\nEvaluate the predictive accuracy of dynamic joint models using time-dependent ROC curves and AUC.\nAppreciate the clinical utility of dynamic joint models for personalized medicine.",
    "crumbs": [
      "**Joint Models**",
      "Dynamic Joint Model"
    ]
  },
  {
    "objectID": "02-07-06-05-survival-analysis-dynamic-joint-model-r.html#resources",
    "href": "02-07-06-05-survival-analysis-dynamic-joint-model-r.html#resources",
    "title": "6.5 Dynamic Joint Model",
    "section": "Resources",
    "text": "Resources\n\nRizopoulos D. (2012). Joint Models for Longitudinal and Time-to-Event Data. Chapman & Hall/CRC.\nRizopoulos D. (2023). JMbayes2: Joint Models for Longitudinal and Survival Data using Bayesian Methods.\nProust-Lima, C., et al. (2014). Joint modelling of multivariate longitudinal outcomes and time-to-event: a review. Statistical Methods in Medical Research. 8 JMbayes2: Extended Joint Models for Longitudinal and Time-to-Event Data",
    "crumbs": [
      "**Joint Models**",
      "Dynamic Joint Model"
    ]
  },
  {
    "objectID": "02-07-01-01-survival-analysis-kaplan-meier-r.html#overview",
    "href": "02-07-01-01-survival-analysis-kaplan-meier-r.html#overview",
    "title": "1.1 Kaplan-Meier Estimator",
    "section": "Overview",
    "text": "Overview\nThe Kaplan-Meier estimator is a non-parametric statistic used to estimate the survival function from time-to-event data. It is particularly valuable when some data points are censored, meaning the event of interest (e.g., death, equipment failure) has not occurred for some subjects at the end of the study period.\nThe survival function \\(S(t)\\) estimates the probability that an individual survives beyond time \\(t\\). The Kaplan-Meier estimator provides a step function that drops at each event time (e.g., death, failure). The survival probability at a given time is calculated as:\n\\[ \\hat{S}(t) = \\prod_{t_i \\leq t} \\left( \\frac{n_i - d_i}{n_i} \\right) \\]\nWhere: - \\(t_i\\) is the time of the \\(i\\)-th event (e.g., death). - \\(n_i\\) is the number of subjects still at risk just before time \\(t_i\\). - \\(d_i\\) is the number of events (e.g., deaths) at time \\(t_i\\). - The product of the ratios \\(\\frac{n_i - d_i}{n_i}\\) across all event times up to \\(t\\) gives the estimated survival probability at time ( t ).\n\nFeatures of Kaplan-Meier Estimation\n\nNon-parametric: The Kaplan-Meier estimator does not assume any specific distribution of the survival times.\nStepwise Survival Curve: The survival curve is a step function that drops at each event time and stays flat between events.\nCensoring: The Kaplan-Meier method can handle right-censored data, where we do not observe the event of interest for some subjects by the time the study ends. Censoring is denoted by ticks on the survival curve, showing the points where individuals were lost to follow-up or their event had not occurred by the study’s end.\n\n\n\nKaplan-Meier Survival Curve Interpretation\n\nThe curve starts at 1 (100% survival probability) and decreases over time as events occur. At each event time (e.g., death or failure), the curve steps down.\nThe Y-axis shows the estimated survival probability, and the X-axis represents time.\nThe rate of decline in the curve reflects the frequency of events (e.g., if there is a rapid decline, the event is occurring more frequently).\nCensored data points are marked with tick marks or other indicators along the curve. These represent individuals who did not experience the event during the study but were lost to follow-up or had not yet experienced the event at the end of the study.",
    "crumbs": [
      "**Non-Parametric Methods**",
      "Kaplan-Meier Estimator"
    ]
  },
  {
    "objectID": "02-07-01-01-survival-analysis-kaplan-meier-r.html#kaplan-meier-esimator-from-scratch",
    "href": "02-07-01-01-survival-analysis-kaplan-meier-r.html#kaplan-meier-esimator-from-scratch",
    "title": "1.1 Kaplan-Meier Estimator",
    "section": "Kaplan-Meier Esimator from Scratch",
    "text": "Kaplan-Meier Esimator from Scratch\nThis is a manual calculation of the Kaplan-Meier estimator, without using any external R packages. The steps involve create a dataset, determining the number of individuals at risk, calculating the survival probability at each event time, and then plotting the results. You can compare these manually calculated results with those obtained from {estimation} analysis packages to verify accuracy. This exercise help you to understand theory behind Kaplan-Meier estimation.\nSteps:\n\nCreate a dataset: We’ll created a simulated dataset similar to the lung dataset in the {survival} package.\nSort Data by Time: Organize the dataset based on event times.\nCalculate Risk Set and Survival Probabilities: For each unique event time, manually calculate the Kaplan-Meier survival probabilities.\nPlot the Kaplan-Meier Survival Curve.\n\n\nCreate a dataset\nTo create a dataset similar to the lung dataset in the {survival} package, we’ll simulate survival data with random values for survival time, event status, and other variables such as age, sex, and treatment. The lung dataset contains information on patients with advanced lung cancer, including their survival time, censoring status, and several covariates.\nVariables typically included in a survival dataset like lung:\n\ntime: Survival time (numeric).\nstatus: Censoring indicator (0 = censored, 1 = event).\nage: Age of the patient (numeric).\nsex: Gender of the patient (1 = male, 2 = female).\nph.ecog: ECOG performance score (0 to 5).\ntreatment: Treatment group (1 = standard, 2 = experimental).\n\n\n\nCode\n# Set the seed for reproducibility\nset.seed(123)\n\n# Number of observations\nn &lt;- 228  # same as the original lung dataset\n\n# Simulate survival time (in days)\ntime &lt;- round(runif(n, min = 1, max = 1000))  # random survival time between 1 and 1000 days\n\n# Simulate censoring indicator (0 = censored, 1 = event)\nstatus &lt;- rbinom(n, size = 1, prob = 0.7)  # 70% of events, 30% censored\n\n# Simulate age (between 40 and 80 years)\nage &lt;- round(runif(n, min = 40, max = 80))\n\n# Simulate sex (1 = male, 2 = female)\nsex &lt;- sample(c(1, 2), size = n, replace = TRUE)\n\n# Simulate ECOG performance score (0 to 4)\nph_ecog &lt;- sample(0:4, size = n, replace = TRUE)\n\n# Simulate treatment group (1 = standard, 2 = experimental)\ntreatment &lt;- sample(c(1, 2), size = n, replace = TRUE)\n\n# Create the data frame\nsimulated_lung&lt;- data.frame(time = time,\n                             status = status,\n                             age = age,\n                             sex = sex,\n                             ph_ecog = ph_ecog,\n                             treatment = treatment)\n\n# Inspect the first few rows of the simulated dataset\nhead(simulated_lung)\n\n\n  time status age sex ph_ecog treatment\n1  288      1  72   1       4         2\n2  789      0  70   2       0         2\n3  410      1  46   2       2         2\n4  883      1  45   1       1         2\n5  941      1  79   2       2         2\n6   47      1  57   1       3         2\n\n\n\n\nSort data by time and event status\nWe need to sort the dataset by survival time\n\n\nCode\n# Sort data by 'time'\nsimulated_lung &lt;- simulated_lung[order(simulated_lung$time), ]\nhead(simulated_lung)\n\n\n    time status age sex ph_ecog treatment\n74     2      1  52   1       1         1\n143   11      1  76   2       0         1\n35    26      1  79   1       2         1\n18    43      0  70   1       4         1\n6     47      1  57   1       3         2\n51    47      1  61   1       1         1\n\n\n\n\nCalculate Kaplan-Meier estimates\nTo calculate the Kaplan-Meier estimates manually, you can follow these steps:\n\nRisk Set: At each unique event time, count how many individuals are still at risk.\nEvent Occurrence: At each unique event time, count how many events (deaths) occur.\nSurvival Probability: Compute the survival probability at each step using the formula:\n\n\\[  S(t_i) = S(t_{i-1}) \\times \\left(1 - \\frac{d_i}{n_i}\\right) \\]\nwhere: - \\(d\\) is the number of events at time \\(t_i\\), - \\(n_i\\) is the number of individuals at risk at time \\(i\\).\n\n\nCode\n# Initialize survival probability\nn &lt;- nrow(simulated_lung)  # total number of individuals\nS_t &lt;- 1              # start with a survival probability of 1 (i.e., 100% survival at t = 0)\nkm_table &lt;- data.frame(time = numeric(0), n_risk = numeric(0), events = numeric(0), survival = numeric(0))\n\n# Loop through each unique event time\nunique_times &lt;- unique(simulated_lung$time[simulated_lung$status == 1])  # only times where events occurred\n\nfor (t in unique_times) {\n  # Calculate the number of individuals at risk at this time point\n  n_risk &lt;- sum(simulated_lung$time &gt;= t)\n  \n  # Calculate the number of events at this time point\n  d_i &lt;- sum(simulated_lung$time == t & simulated_lung$status == 1)\n  \n  # Update the survival probability\n  S_t &lt;- S_t * (1 - d_i / n_risk)\n  \n  # Add this to the Kaplan-Meier table\n  km_table &lt;- rbind(km_table, data.frame(time = t, n_risk = n_risk, events = d_i, survival = S_t))\n}\n\n# Print the resulting Kaplan-Meier estimates\nhead(km_table)\n\n\n  time n_risk events  survival\n1    2    228      1 0.9956140\n2   11    227      1 0.9912281\n3   26    226      1 0.9868421\n4   47    224      2 0.9780310\n5   54    221      1 0.9736055\n6   62    220      1 0.9691801\n\n\n\n\nPlot the Kaplan-Meier survival curve\nOnce you have manually computed the Kaplan-Meier estimates, you can plot the survival curve using base R plotting functions:\n\n\nCode\n# Plot the survival curve\nplot(km_table$time, km_table$survival, type = \"s\", col = \"blue\", lwd = 2,\n     xlab = \"Time (days)\", ylab = \"Survival Probability\",\n     main = \"Manual Kaplan-Meier Estimate - Lung Dataset\")",
    "crumbs": [
      "**Non-Parametric Methods**",
      "Kaplan-Meier Estimator"
    ]
  },
  {
    "objectID": "02-07-01-01-survival-analysis-kaplan-meier-r.html#kaplan-meier-estimator-in-r",
    "href": "02-07-01-01-survival-analysis-kaplan-meier-r.html#kaplan-meier-estimator-in-r",
    "title": "1.1 Kaplan-Meier Estimator",
    "section": "Kaplan-Meier Estimator in R",
    "text": "Kaplan-Meier Estimator in R\nThis tutorial introduces survival analysis and how to conduct it in R. It primarily follows the Survival Analysis in R tutorials by Emily C. Zabor and Joseph Rickert’s Survival Analysis in R tutorials.\nThis tutorial is mostly used two R packages {survival} and {ggsurvfit}. The survival package in R is a powerful and widely used tool for survival analysis, which deals with the analysis of time-to-event data. It provides a suite of functions to handle various types of survival data, fit survival models, and visualize survival curves. This package is widely applied in medical research, reliability engineering, and many other fields where time-to-event data is of interest.\n\nThe {ggsurvfit} package eases the creation of time-to-event (aka survival) summary figures with {ggplot2}. The concise and modular code creates images that are ready for publication or sharing. Competing risks cumulative incidence is also supported via ggcuminc().\n\nAdditionally we will use {ggfortify} which offers fortify and autoplot functions to allow automatic ggplot2 to visualize Kaplan-Meier plots.\n\nInstall Required R Packages\nFollowing R packages are required to run this notebook. If any of these packages are not installed, you can install them using the code below:\n\n\nCode\npackages &lt;-c(\n         'tidyverse',\n         'report',\n         'performance',\n         'gtsummary',\n         'MASS',\n         'epiDisplay',\n         'survival',\n         'survminer',\n         'ggsurvfit',\n         'tidycmprsk',\n         'ggfortify',\n         'timereg',\n         'cmprsk',\n         'riskRegression'\n         )\n\n\n#| warning: false\n#| error: false\n\n# Install missing packages\nnew_packages &lt;- packages[!(packages %in% installed.packages()[,\"Package\"])]\nif(length(new_packages)) install.packages(new_packages)\n devtools::install_github(\"ItziarI/WeDiBaDis\")\n\n\nVwerify Installation\n\n\nCode\n# Verify installation\ncat(\"Installed packages:\\n\")\n\n\nInstalled packages:\n\n\nCode\nprint(sapply(packages, requireNamespace, quietly = TRUE))\n\n\nRegistered S3 method overwritten by 'rms':\n  method       from      \n  print.lrtest epiDisplay\n\n\nRegistered S3 method overwritten by 'riskRegression':\n  method        from \n  nobs.multinom broom\n\n\n     tidyverse         report    performance      gtsummary           MASS \n          TRUE           TRUE           TRUE           TRUE           TRUE \n    epiDisplay       survival      survminer      ggsurvfit     tidycmprsk \n          TRUE           TRUE           TRUE           TRUE           TRUE \n     ggfortify        timereg         cmprsk riskRegression \n          TRUE           TRUE           TRUE           TRUE \n\n\n\n\nLoad Packages\n\n\nCode\n# Load packages with suppressed messages\ninvisible(lapply(packages, function(pkg) {\n  suppressPackageStartupMessages(library(pkg, character.only = TRUE))\n}))\n\n\n\n\nCode\n# Check loaded packages\ncat(\"Successfully loaded packages:\\n\")\n\n\nSuccessfully loaded packages:\n\n\nCode\nprint(search()[grepl(\"package:\", search())])\n\n\n [1] \"package:riskRegression\" \"package:cmprsk\"         \"package:timereg\"       \n [4] \"package:ggfortify\"      \"package:tidycmprsk\"     \"package:ggsurvfit\"     \n [7] \"package:survminer\"      \"package:ggpubr\"         \"package:epiDisplay\"    \n[10] \"package:nnet\"           \"package:survival\"       \"package:foreign\"       \n[13] \"package:MASS\"           \"package:gtsummary\"      \"package:performance\"   \n[16] \"package:report\"         \"package:lubridate\"      \"package:forcats\"       \n[19] \"package:stringr\"        \"package:dplyr\"          \"package:purrr\"         \n[22] \"package:readr\"          \"package:tidyr\"          \"package:tibble\"        \n[25] \"package:ggplot2\"        \"package:tidyverse\"      \"package:stats\"         \n[28] \"package:graphics\"       \"package:grDevices\"      \"package:utils\"         \n[31] \"package:datasets\"       \"package:methods\"        \"package:base\"          \n\n\n\n\nData\nWe will be utilizing the lung dataset from the {survival} package, which serves as a valuable resource for analyzing survival data. This dataset comprises information from subjects diagnosed with advanced lung cancer, specifically gathered from the North Central Cancer Treatment Group, a prominent clinical trial network dedicated to cancer research.\nThroughout this tutorial, we will concentrate on the following key variables that provide insight into the patients’ demographics and clinical outcomes:\n\nTime: This variable denotes the observed survival time in days, measuring the duration from the start of treatment until the event of interest occurs, whether that be death or censoring.\nStatus: This variable represents the censoring status of each patient. A value of 1 indicates that the patient is censored, meaning they either withdrew from the study or were still alive at the end of the observation period. A value of 2 signifies that the patient has died, marking the occurrence of the event being studied.\nSex: This variable captures the gender of the subjects, providing crucial demographic information. A value of 1 corresponds to Male and a value of 2 corresponds to Female. This distinction may be important for understanding potential differences in survival rates between genders.\n\n\n\nCode\n# Load lung cancer dataset\ndata(lung)\nglimpse(lung)\n\n\nRows: 228\nColumns: 10\n$ inst      &lt;dbl&gt; 3, 3, 3, 5, 1, 12, 7, 11, 1, 7, 6, 16, 11, 21, 12, 1, 22, 16…\n$ time      &lt;dbl&gt; 306, 455, 1010, 210, 883, 1022, 310, 361, 218, 166, 170, 654…\n$ status    &lt;dbl&gt; 2, 2, 1, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, …\n$ age       &lt;dbl&gt; 74, 68, 56, 57, 60, 74, 68, 71, 53, 61, 57, 68, 68, 60, 57, …\n$ sex       &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 2, 1, …\n$ ph.ecog   &lt;dbl&gt; 1, 0, 0, 1, 0, 1, 2, 2, 1, 2, 1, 2, 1, NA, 1, 1, 1, 2, 2, 1,…\n$ ph.karno  &lt;dbl&gt; 90, 90, 90, 90, 100, 50, 70, 60, 70, 70, 80, 70, 90, 60, 80,…\n$ pat.karno &lt;dbl&gt; 100, 90, 90, 60, 90, 80, 60, 80, 80, 70, 80, 70, 90, 70, 70,…\n$ meal.cal  &lt;dbl&gt; 1175, 1225, NA, 1150, NA, 513, 384, 538, 825, 271, 1025, NA,…\n$ wt.loss   &lt;dbl&gt; NA, 15, 15, 11, 0, 0, 10, 1, 16, 34, 27, 23, 5, 32, 60, 15, …\n\n\n\n\nData processing\nNow we will re-code the data as 1=event, 0=censored:\n\n\nCode\nlung &lt;- \n  lung |&gt; \n  mutate(\n    status = recode(status, `1` = 0, `2` = 1)\n  )\n\n\nNow we have:\n\ntime: Observed survival time in days\nstatus: censoring status 0=censored, 1=dead\nsex: 1=Male, 2=Female\n\n\n\nCode\nhead(lung[, c(\"time\", \"status\", \"sex\")])\n\n\n  time status sex\n1  306      1   1\n2  455      1   1\n3 1010      0   1\n4  210      1   1\n5  883      1   1\n6 1022      0   1\n\n\n\n\nKaplan Meier Analysis\nThe Kaplan-Meier method is the most common technique for estimating survival times and probabilities. It is a non-parametric approach that produces a step function, with a drop occurring each time an event takes place.\nThe {survival} package is fundamental to the entire R survival analysis framework. It is not only feature-rich but also essential for creating the object produced by the Surv() function, which includes data on failure times and censoring. Each subject will have a single entry representing their survival time, followed by a “+” sign if the subject was censored. Let’s examine the first 15 observations:\n\n\nCode\nSurv(lung$time, lung$status)[1:15]\n\n\n [1]  306   455  1010+  210   883  1022+  310   361   218   166   170   654 \n[13]  728    71   567 \n\n\nSubject 1 experienced a notable event at 306 days of observation, indicating a significant outcome in their progression. Subject 2 had a similar event occur later, at 455 days, suggesting varying timelines in their responses. Meanwhile, subject 3 was censored at 1010 days, meaning that their outcome could not be fully assessed due to the study’s conclusion or loss to follow-up. This variation among subjects highlights the differences in individual experiences and response times within the study.\n\n\n\n\n\n\nNote\n\n\n\nthe Surv() function in the {survival} package accepts by default TRUE/FALSE, where TRUE is event and FALSE is censored; 1/0 where 1 is event and 0 is censored; or 2/1 where 2 is event and 1 is censored.\n\n\nThe survfit() function plays a crucial role in survival analysis by generating survival curves based on the Kaplan-Meier method. This method is particularly useful for estimating the probability of survival over a specified time period, especially when dealing with censored data, which is common in medical research.\nTo initiate our analysis, we start with the formula Surv(futime, status) ~ 1, where time represents the follow-up time until the event of interest or censoring occurs, and status indicates whether the event happened (usually coded as 1) or if the data is censored (coded as 0). The survfit() function utilizes this formula to calculate the Kaplan-Meier estimates, providing a stepwise approximation of the survival function. Additionally, the summary() function complements this analysis by offering a detailed summary of the survival estimates. Within this function, the times parameter allows researchers to specify particular time points at which they wish to view the survival probabilities. This feature is beneficial for focusing on time intervals of interest, enhancing the interpretability of the survival curves. Overall, these tools are essential for understanding patient survival trajectories and can inform clinical decision-making and research developments.\n\n\nCode\nkm.fit&lt;- survfit(Surv(time, status) ~ 1, data = lung)\nstr(km.fit)\n\n\nList of 17\n $ n        : int 228\n $ time     : num [1:186] 5 11 12 13 15 26 30 31 53 54 ...\n $ n.risk   : num [1:186] 228 227 224 223 221 220 219 218 217 215 ...\n $ n.event  : num [1:186] 1 3 1 2 1 1 1 1 2 1 ...\n $ n.censor : num [1:186] 0 0 0 0 0 0 0 0 0 0 ...\n $ surv     : num [1:186] 0.996 0.982 0.978 0.969 0.965 ...\n $ std.err  : num [1:186] 0.0044 0.00885 0.00992 0.01179 0.01263 ...\n $ cumhaz   : num [1:186] 0.00439 0.0176 0.02207 0.03103 0.03556 ...\n $ std.chaz : num [1:186] 0.00439 0.0088 0.00987 0.01173 0.01257 ...\n $ type     : chr \"right\"\n $ logse    : logi TRUE\n $ conf.int : num 0.95\n $ conf.type: chr \"log\"\n $ lower    : num [1:186] 0.987 0.966 0.959 0.947 0.941 ...\n $ upper    : num [1:186] 1 1 0.997 0.992 0.989 ...\n $ t0       : num 0\n $ call     : language survfit(formula = Surv(time, status) ~ 1, data = lung)\n - attr(*, \"class\")= chr \"survfit\"\n\n\nThis model is designed to generate estimates for 1, 30, 60, and 90 days, and then every 90 days thereafter. It is the simplest possible model, requiring just three lines of R code to fit and produce both numerical and graphical summaries.\n\n\nCode\nsummary(km.fit, times = c(1,30,60,90*(1:10)))\n\n\nCall: survfit(formula = Surv(time, status) ~ 1, data = lung)\n\n time n.risk n.event survival std.err lower 95% CI upper 95% CI\n    1    228       0   1.0000  0.0000       1.0000        1.000\n   30    219      10   0.9561  0.0136       0.9299        0.983\n   60    213       7   0.9254  0.0174       0.8920        0.960\n   90    201      10   0.8816  0.0214       0.8406        0.925\n  180    160      36   0.7217  0.0298       0.6655        0.783\n  270    108      30   0.5753  0.0338       0.5128        0.645\n  360     70      24   0.4340  0.0358       0.3693        0.510\n  450     48      16   0.3287  0.0356       0.2659        0.406\n  540     33      10   0.2554  0.0344       0.1962        0.333\n  630     22       7   0.1958  0.0330       0.1407        0.272\n  720     14       8   0.1246  0.0290       0.0789        0.197\n  810      7       5   0.0783  0.0246       0.0423        0.145\n  900      3       2   0.0503  0.0228       0.0207        0.123\n\n\n\n\nKaplan-Meier plots\nWe will use the {ggsurvfit} package to generate Kaplan-Meier plots. This package simplifies the plotting of time-to-event endpoints by leveraging the capabilities of the {ggplot2} package. The {ggsurvfit} package works best when you create the survival object using the included ggsurvfit::survfit2() function. This function uses the same syntax as survival::survfit(), but ggsurvfit::survfit2() tracks the environment from the function call, which helps the plot have better default values for labeling and p-value reporting.\n\n\nCode\nsurvfit2(Surv(time, status) ~ 1, data = lung) |&gt; \n  ggsurvfit() +\n  labs(\n    x = \"Days\",\n    y = \"Overall survival probability\"\n  )\n\n\n\n\n\n\n\n\n\nThe default plot in ggsurvfit() shows the step function only. We can add the confidence interval using add_confidence_interval():\n\n\nCode\nsurvfit2(Surv(time, status) ~ 1, data = lung) |&gt; \n  ggsurvfit() +\n  labs(\n    x = \"Days\",\n    y = \"Overall survival probability\"\n  ) +\n add_confidence_interval()\n\n\n\n\n\n\n\n\n\nIf we want to display the numbers at risk below the x-axis, we can achieve this by using add_risktable() :\n\n\nCode\nsurvfit2(Surv(time, status) ~ 1, data = lung)  |&gt;  \n  ggsurvfit() +\n  labs(\n    x = \"Days\",\n    y = \"Overall survival probability\"\n  ) +\n add_confidence_interval() +\n    add_risktable() \n\n\n\n\n\n\n\n\n\nAlternatively, the ggfortify package allows you to create a simple survival plot with CI using the autoplot() function.\n\n\nCode\nautoplot(km.fit)\n\n\nWarning: `aes_string()` was deprecated in ggplot2 3.0.0.\nℹ Please use tidy evaluation idioms with `aes()`.\nℹ See also `vignette(\"ggplot2-in-packages\")` for more information.\nℹ The deprecated feature was likely used in the ggfortify package.\n  Please report the issue at &lt;https://github.com/sinhrks/ggfortify/issues&gt;.\n\n\n\n\n\n\n\n\n\n\n\nEstimating \\(x\\)-year survival\nIn survival analysis, one key point of interest is the probability of surviving beyond a specific number of years. For instance, to estimate the probability of surviving up to one year, you can use the summary function with the times argument. Note that the time variable in the lung dataset is measured in days, so you should set times to 365.25.\n\n\nCode\nsummary(survfit(Surv(time, status) ~ 1, data = lung), times = 365.25)\n\n\nCall: survfit(formula = Surv(time, status) ~ 1, data = lung)\n\n time n.risk n.event survival std.err lower 95% CI upper 95% CI\n  365     65     121    0.409  0.0358        0.345        0.486\n\n\n\n\nCode\nsurvfit2(Surv(time, status) ~ 1, data = lung)  |&gt;  \n  ggsurvfit() +\n  labs(\n    x = \"Days\",\n    y = \"Overall survival probability\"\n  ) +\n add_confidence_interval() + \n   add_risktable() +\n  geom_segment(x = 365.25, xend = 365.25, y = -0.05, yend = 0.4092416, \n               linewidth = 1.0) +\n  geom_segment(x = 365.25, xend = -40, y = 0.4092416, yend = 0.4092416,\n               linewidth = 1.0, \n               arrow = arrow(length = unit(0.2, \"inches\"))) \n\n\n\n\n\n\n\n\n\nBy one year, 121 of the 228 lung patients had died, and if we ignore censoring, the one year probability of survival:\n\n\nCode\n(1-121/228)*100\n\n\n[1] 46.92982\n\n\nIgnoring the fact that 42 patients were censored before the one-year mark can lead to an inaccurate estimate of the one-year probability of survival (47%). The correct estimate, which takes censoring into account using the Kaplan-Meier method, is 41%. Ignoring censoring results in an overestimation of the overall survival probability. Consider two studies, each with 228 subjects, where both have 165 deaths. In one study (represented by the blue line), censoring is ignored, while in the other (shown by the yellow line), censoring is taken into account. Censored subjects only provide information for part of the follow-up period and then drop out of the risk set, which leads to a lower cumulative probability of survival. Ignoring censoring incorrectly assumes that censored patients remain part of the risk set for the entire follow-up period.\n\n\nCode\nlung2 &lt;- \n  lung |&gt; \n  mutate(\n    time = ifelse(status == 1, time, 1022), \n    group = \"Ignoring censoring\") |&gt;  \n  full_join(mutate(lung, group = \"With censoring\"))\n\nsurvfit2(Surv(time, status) ~ group, data = lung2) |&gt; \n  ggsurvfit() +\n  labs(\n    x = \"Days\",\n    y = \"Overall survival probability\"\n    ) + \n  add_confidence_interval() +\n  add_risktable(risktable_stats = \"n.risk\")\n\n\n\n\n\n\n\n\n\nWe can produce nice tables of \\(x\\)-time survival probability estimates using the tbl_survfit() function from the {gtsummary} package:\n\n\nCode\ntbl_survfit(\n  survfit(Surv(time, status) ~ 1, data = lung),\n  times = 365.25,\n  label_header = \"**1-year survival (95% CI)**\"\n)\n\n\n\n\n\n\n\n\nCharacteristic\n1-year survival (95% CI)\n\n\n\n\nOverall\n41% (34%, 49%)\n\n\n\n\n\n\n\n\n\nMedian survival time\nIn survival analysis, one important point to consider is the average survival time, which we typically measure using the median. Since survival times are usually not normally distributed, using the mean is not an appropriate way to summarize this data.\nThe median survival time can be directly obtained from the survfit() object.\n\n\nCode\nsurvfit(Surv(time, status) ~ 1, data = lung)\n\n\nCall: survfit(formula = Surv(time, status) ~ 1, data = lung)\n\n       n events median 0.95LCL 0.95UCL\n[1,] 228    165    310     285     363\n\n\nWe can create appealing tables displaying the median survival time estimates using the tbl_survfit() function from the {gtsummary} package.\n\n\nCode\nsurvfit(Surv(time, status) ~ 1, data = lung) %&gt;% \n  tbl_survfit(\n    probs = 0.5,\n    label_header = \"**Median survival (95% CI)**\"\n  )\n\n\n\n\n\n\n\n\nCharacteristic\nMedian survival (95% CI)\n\n\n\n\nOverall\n310 (285, 363)\n\n\n\n\n\n\n\n\n\nSurvival times between groups (male/female) using log-rank test\nThe log-rank test is a non-parametric statistical method used to compare the survival distributions of two or more groups. It assesses whether there is a significant difference between the survival curves by giving equal weight to observations throughout the entire follow-up period.\nTo calculate the log-rank p-value, we can use the survdiff() function. For example, we can test for differences in survival time based on sex using the lung dataset.\n\n\nCode\nsurvdiff(Surv(time, status) ~ sex, data = lung)\n\n\nCall:\nsurvdiff(formula = Surv(time, status) ~ sex, data = lung)\n\n        N Observed Expected (O-E)^2/E (O-E)^2/V\nsex=1 138      112     91.6      4.55      10.3\nsex=2  90       53     73.4      5.68      10.3\n\n Chisq= 10.3  on 1 degrees of freedom, p= 0.001 \n\n\nThere is a significant difference in overall survival by sex in the lung data, with a p-value of 0.001.\nNext, we look at survival curves by sex.\n\n\nCode\nsurvfit2(Surv(time, status) ~ sex, data = lung) |&gt; \n  ggsurvfit() +\n  labs(\n    x = \"Days\",\n    y = \"Overall survival probability\"\n  ) +\n  add_confidence_interval() +\n  add_risktable()",
    "crumbs": [
      "**Non-Parametric Methods**",
      "Kaplan-Meier Estimator"
    ]
  },
  {
    "objectID": "02-07-01-01-survival-analysis-kaplan-meier-r.html#summary-and-conclusion",
    "href": "02-07-01-01-survival-analysis-kaplan-meier-r.html#summary-and-conclusion",
    "title": "1.1 Kaplan-Meier Estimator",
    "section": "Summary and Conclusion",
    "text": "Summary and Conclusion\nNonparametric survival analysis, especially using the Kaplan-Meier estimator and log-rank test, is a powerful tool for analyzing time-to-event data, particularly in situations where the distribution of survival times is unknown and when censoring is present. These techniques allow researchers to estimate and compare survival probabilities between groups, providing a foundation for more advanced survival analysis methods.\nThese methods are fundamental in fields like clinical research, reliability engineering, and economics, where time-to-event outcomes and censored data are prevalent.",
    "crumbs": [
      "**Non-Parametric Methods**",
      "Kaplan-Meier Estimator"
    ]
  },
  {
    "objectID": "02-07-01-01-survival-analysis-kaplan-meier-r.html#references",
    "href": "02-07-01-01-survival-analysis-kaplan-meier-r.html#references",
    "title": "1.1 Kaplan-Meier Estimator",
    "section": "References",
    "text": "References\n\nSurvival Analysis with R\nSurvival data analysis\nSurvival Analysis in R\nSurvival Analysis with R\nSurvival Analysis in R Companion\nSurvival Analysis in R",
    "crumbs": [
      "**Non-Parametric Methods**",
      "Kaplan-Meier Estimator"
    ]
  },
  {
    "objectID": "02-07-01-02-survival-analysis-nelson-aalen-r.html#overview",
    "href": "02-07-01-02-survival-analysis-nelson-aalen-r.html#overview",
    "title": "1.2 The Nelson-Aalen Estimator",
    "section": "Overview",
    "text": "Overview\nSurvival analysis studies the time until an event of interest occurs, such as patient death or disease recurrence. A challenge is handling censoring, where the event is not observed for all subjects (e.g., patients still alive at study end). Non-parametric methods like the Kaplan-Meier estimator for survival probability and the Nelson-Aalen estimator for the cumulative hazard function are foundational tools.\nIn this tutorial, we’ll focus on the Nelson-Aalen estimator, which estimates the cumulative hazard function \\(H(t)\\), representing the accumulated risk up to time \\(t\\). The survival probability \\(S(t)\\) relates to it via \\(S(t) = \\exp(-H(t))\\). We’ll apply it to the lung dataset:\n\nDataset Overview: 228 observations from a lung cancer trial (1960s). Key variables:\n\ntime: Survival time in days.\nstatus: Event indicator (1 = death, 0 = censored).\nsex: Patient sex (1 = male, 2 = female).\nOther covariates: age, ph.ecog (performance status), etc.\n\n\nWe’ll compute estimates, plot curves, find medians, and compare male vs. female survival using the log-rank test.\nThe Nelson-Aalen estimator is a non-parametric estimate of the cumulative hazard function \\(H(t) = \\int_0^t h(u) \\, du\\), where \\(h(u)\\) is the hazard rate (instantaneous risk of event at time \\(u\\), given survival to \\(u\\).\nUnder the assumption of independent censoring, the estimator is:\n\\[\n\\hat{H}(t) = \\sum_{t_i \\leq t} \\frac{d_i}{n_i}\n\\]\n\n\\(t_i\\): Distinct event times (ordered).\n\\(d_i\\): Number of events (deaths) at \\(t_i\\).\n\\(n_i\\): Number of individuals at risk just before \\(t_i\\) (those who haven’t experienced the event or been censored yet).\n\nThis is a step function that jumps at each event time by \\(d_i / n_i\\). The variance estimate is $^2(t) = _{t_i t} $, useful for confidence intervals.\nUnlike the Kaplan-Meier (which estimates \\((t)\\) directly and handles ties differently), Nelson-Aalen focuses on the hazard and is more straightforward for cumulative risk. It’s implemented in R’s survival package via survfit().",
    "crumbs": [
      "**Non-Parametric Methods**",
      "Nelson-Aalen Estimator"
    ]
  },
  {
    "objectID": "02-07-01-02-survival-analysis-nelson-aalen-r.html#nelson-aalen-estimator-from-scratch",
    "href": "02-07-01-02-survival-analysis-nelson-aalen-r.html#nelson-aalen-estimator-from-scratch",
    "title": "1.2 The Nelson-Aalen Estimator",
    "section": "Nelson-Aalen estimator from Scratch",
    "text": "Nelson-Aalen estimator from Scratch\nIn this tutorial, we’ll implement the Nelson-Aalen Estimator from scratch in R using the lung dataset from the survival package. We’ll avoid directly using survfit(..., type = \"aalen\") for the core computation, instead manually calculating the cumulative hazard function $ (t) = _{t_i t} $. We’ll then derive the survival function, compute confidence intervals, estimate x-year survival probabilities, plot the survival curve, calculate the median survival time, and compare survival between groups (male vs. female) using the log-rank test. The survival package will be used for data handling and the log-rank test, but the Nelson-Aalen computation will be coded manually.\n\nData\n\n\nCode\nlibrary(survival)\n# Load the lung dataset\ndata(lung)\n# Create a data frame with time, status, and sex\ndata &lt;- data.frame(\n  time = lung$time,\n  status = lung$status,  # 1 = event (death), 0 = censored\n  sex = lung$sex         # 1 = male, 2 = female\n)\n\n\n\n\nFunction to compute Nelson-Aalen estimator from scratch\n\n\nCode\n# Function to compute Nelson-Aalen estimator from scratch\nnelson_aalen &lt;- function(time, status) {\n  # Ensure data is sorted by time\n  ord &lt;- order(time)\n  time &lt;- time[ord]\n  status &lt;- status[ord]\n  \n  # Identify unique event times (where status == 1)\n  event_times &lt;- unique(time[status == 1])\n  event_times &lt;- sort(event_times)\n  \n  # Initialize vectors\n  n_risk &lt;- numeric(length(event_times))  # Number at risk\n  n_event &lt;- numeric(length(event_times)) # Number of events\n  cumhaz &lt;- numeric(length(event_times))  # Cumulative hazard\n  variance &lt;- numeric(length(event_times))# Variance of cumhaz\n  \n  # Compute n_i (at risk) and d_i (events) at each event time\n  for (i in seq_along(event_times)) {\n    t_i &lt;- event_times[i]\n    n_risk[i] &lt;- sum(time &gt;= t_i)  # Subjects still at risk\n    n_event[i] &lt;- sum(time == t_i & status == 1)  # Events at t_i\n    cumhaz[i] &lt;- ifelse(i == 1, 0, cumhaz[i-1]) + n_event[i] / n_risk[i]\n    variance[i] &lt;- ifelse(i == 1, 0, variance[i-1]) + n_event[i] / (n_risk[i] * (n_risk[i] - n_event[i]))\n  }\n  \n  return(list(\n    time = event_times,\n    n.risk = n_risk,\n    n.event = n_event,\n    cumhaz = cumhaz,\n    variance = variance\n  ))\n}\n\n\n\n\nCompute Nelson-Aalen for the entire dataset\n\n\nCode\n# Compute Nelson-Aalen for the entire dataset\nna_fit &lt;- nelson_aalen(data$time, data$status)\nsummary(na_fit)\n\n\n         Length Class  Mode   \ntime     60     -none- numeric\nn.risk   60     -none- numeric\nn.event  60     -none- numeric\ncumhaz   60     -none- numeric\nvariance 60     -none- numeric\n\n\n\n\nDerive survival function S(t) = exp(-H(t))\n\n\nCode\n# Derive survival function S(t) = exp(-H(t))\nsurvival &lt;- exp(-na_fit$cumhaz)\n\n# Confidence intervals (95%) for cumulative hazard\nz &lt;- qnorm(0.975)  # Z-score for 95% CI\nlower_haz &lt;- na_fit$cumhaz - z * sqrt(na_fit$variance)\nupper_haz &lt;- na_fit$cumhaz + z * sqrt(na_fit$variance)\nlower_surv &lt;- exp(-upper_haz)  # Survival CI: exp(-upper_haz) is lower bound\nupper_surv &lt;- exp(-lower_haz)  # exp(-lower_haz) is upper bound\n\n# Estimating x-Year Survival\n# For 1-year (365 days) and 5-year (1825 days) survival\nt_1yr &lt;- 365\nt_5yr &lt;- 1825\n\n# Find closest time points\nidx_1yr &lt;- max(which(na_fit$time &lt;= t_1yr))\nidx_5yr &lt;- max(which(na_fit$time &lt;= t_5yr))\n\ns_1yr &lt;- survival[idx_1yr]\ns_5yr &lt;- survival[idx_5yr]\n\ncat(\"1-Year Survival Probability:\", round(s_1yr, 3), \"\\n\")\n\n\n1-Year Survival Probability: 0.699 \n\n\nCode\ncat(\"5-Year Survival Probability:\", round(s_5yr, 3), \"\\n\")\n\n\n5-Year Survival Probability: 0.044 \n\n\n\n\nPlot Survival Curve\n\n\nCode\n# Plot Survival Curve\nplot(na_fit$time, survival, type = \"s\", col = \"red\", lwd = 2,\n     xlab = \"Time (days)\", ylab = \"Survival Probability S(t)\",\n     main = \"Nelson-Aalen Survival Curve (Lung Dataset)\")\nlines(na_fit$time, lower_surv, col = \"red\", lty = 2)\nlines(na_fit$time, upper_surv, col = \"red\", lty = 2)\nlegend(\"topright\", c(\"S(t)\", \"95% CI\"), col = \"red\", lty = c(1, 2), lwd = c(2, 1))\n\n\n\n\n\n\n\n\n\n\n\nMedian Survival Time\n\n\nCode\n# Median Survival Time\n# Find time where S(t) &lt;= 0.5\nmedian_idx &lt;- min(which(survival &lt;= 0.5))\nmedian_time &lt;- na_fit$time[median_idx]\ncat(\"Median Survival Time:\", median_time, \"days\\n\")\n\n\nMedian Survival Time: 588 days\n\n\n\n\nSurvival Times Between Groups (Male vs. Female)\n\n\nCode\n# Survival Times Between Groups (Male vs. Female)\n# Subset data by sex\ndata_male &lt;- subset(data, sex == 1)\ndata_female &lt;- subset(data, sex == 2)\n\n# Compute Nelson-Aalen for each group\nna_male &lt;- nelson_aalen(data_male$time, data_male$status)\nna_female &lt;- nelson_aalen(data_female$time, data_female$status)\n\n# Derive survival curves\nsurv_male &lt;- exp(-na_male$cumhaz)\nsurv_female &lt;- exp(-na_female$cumhaz)\n\n\n\n\nCode\n# Survival Times Between Groups (Male vs. Female)\n# Plot survival curves by group\nplot(na_male$time, surv_male, type = \"s\", col = \"blue\", lwd = 2,\n     xlab = \"Time (days)\", ylab = \"Survival Probability\",\n     main = \"Survival Curves by Sex (Male: Blue, Female: Red)\")\nlines(na_female$time, surv_female, col = \"red\", lwd = 2)\nlegend(\"topright\", c(\"Male\", \"Female\"), col = c(\"blue\", \"red\"), lwd = 2)\n\n\n\n\n\n\n\n\n\n\n\nLog-rank test for group comparison\n\n\nCode\n# Log-rank test for group comparison\nsurv_obj &lt;- Surv(time = data$time, event = data$status)\nlogrank_test &lt;- survdiff(surv_obj ~ data$sex)\nprint(logrank_test)\n\n\nCall:\nsurvdiff(formula = surv_obj ~ data$sex)\n\n             N Observed Expected (O-E)^2/E (O-E)^2/V\ndata$sex=1 138      112     91.6      4.55      10.3\ndata$sex=2  90       53     73.4      5.68      10.3\n\n Chisq= 10.3  on 1 degrees of freedom, p= 0.001 \n\n\n\n\nCode\n# remove all objects\nrm(list=ls())",
    "crumbs": [
      "**Non-Parametric Methods**",
      "Nelson-Aalen Estimator"
    ]
  },
  {
    "objectID": "02-07-01-02-survival-analysis-nelson-aalen-r.html#nelson-aalen-estimato-in-r",
    "href": "02-07-01-02-survival-analysis-nelson-aalen-r.html#nelson-aalen-estimato-in-r",
    "title": "1.2 The Nelson-Aalen Estimator",
    "section": "Nelson-Aalen estimato in R",
    "text": "Nelson-Aalen estimato in R\nIn this section, we will demonstrate how to compute the Nelson-Aalen estimator using R’s survival package. We’ll cover loading the data, calculating the estimator, estimating x-year survival probabilities, plotting survival curves, calculating median survival time, and comparing survival between groups using the log-rank test.\n\nInstall Required R Packages\nFollowing R packages are required to run this notebook. If any of these packages are not installed, you can install them using the code below:\n\n\nCode\npackages &lt;-c(\n         'tidyverse',\n         'gtsummary',\n         'survival',\n         'survminer',\n         'ggsurvfit',\n         'tidycmprsk',\n         'ggfortify',\n         'timereg',\n         'cmprsk',\n         'riskRegression'\n         )\n\n\n#| warning: false\n#| error: false\n\n# Install missing packages\nnew_packages &lt;- packages[!(packages %in% installed.packages()[,\"Package\"])]\nif(length(new_packages)) install.packages(new_packages)\n devtools::install_github(\"ItziarI/WeDiBaDis\")\n\n\nCode\n# Verify installation\ncat(\"Installed packages:\\n\")\n\n\nInstalled packages:\n\n\nCode\nprint(sapply(packages, requireNamespace, quietly = TRUE))\n\n\nRegistered S3 method overwritten by 'riskRegression':\n  method        from \n  nobs.multinom broom\n\n\n     tidyverse      gtsummary       survival      survminer      ggsurvfit \n          TRUE           TRUE           TRUE           TRUE           TRUE \n    tidycmprsk      ggfortify        timereg         cmprsk riskRegression \n          TRUE           TRUE           TRUE           TRUE           TRUE \n\n\n\n\nLoad Packages\n\n\nCode\n# Load packages with suppressed messages\ninvisible(lapply(packages, function(pkg) {\n  suppressPackageStartupMessages(library(pkg, character.only = TRUE))\n}))\n\n\n\n\nCode\n# Check loaded packages\ncat(\"Successfully loaded packages:\\n\")\n\n\nSuccessfully loaded packages:\n\n\nCode\nprint(search()[grepl(\"package:\", search())])\n\n\n [1] \"package:riskRegression\" \"package:cmprsk\"         \"package:timereg\"       \n [4] \"package:ggfortify\"      \"package:tidycmprsk\"     \"package:ggsurvfit\"     \n [7] \"package:survminer\"      \"package:ggpubr\"         \"package:gtsummary\"     \n[10] \"package:lubridate\"      \"package:forcats\"        \"package:stringr\"       \n[13] \"package:dplyr\"          \"package:purrr\"          \"package:readr\"         \n[16] \"package:tidyr\"          \"package:tibble\"         \"package:ggplot2\"       \n[19] \"package:tidyverse\"      \"package:survival\"       \"package:stats\"         \n[22] \"package:graphics\"       \"package:grDevices\"      \"package:utils\"         \n[25] \"package:datasets\"       \"package:methods\"        \"package:base\"          \n\n\n\n\nData\nFirst, load the necessary package and dataset. We’ll inspect the data and create a survival object.\n\n\nCode\n# Load the lung dataset\ndata(lung)\n# Create a data frame with relevant variables\ndata &lt;- data.frame(\n  time = lung$time,\n  status = lung$status,  # 1 = event (death), 0 = censored\n  sex = lung$sex         # 1 = male, 2 = female\n)\n\n\n\n\nCtrate a survival object\n\n\nCode\n# Create a survival object\nsurv_obj &lt;- Surv(time = data$time, event = data$status)\n\n\n\n\nCalculating the Nelson-Aalen Estimator\n\n\nCode\n# Calculating the Nelson-Aalen Estimator\n# Use survfit to compute the cumulative hazard (Nelson-Aalen estimator)\nfit_na &lt;- survfit(surv_obj ~ 1, type = \"fh\")  # 'fh' specifies Fleming-Harrington (Nelson-Aalen)\n\n\n\n\nExtract key components\n\n\nCode\nna_summary &lt;- summary(fit_na)\ntimes &lt;- na_summary$time\nn_risk &lt;- na_summary$n.risk\nn_event &lt;- na_summary$n.event\ncumhaz &lt;- na_summary$cumhaz\nstd_err &lt;- na_summary$std.chaz  # Standard error of cumhaz\n\n# Derive survival function S(t) = exp(-H(t))\nsurvival &lt;- exp(-cumhaz)\n\n# Compute 95% confidence intervals for cumulative hazard and survival\nz &lt;- qnorm(0.975)  # Z-score for 95% CI\nlower_haz &lt;- cumhaz - z * std_err\nupper_haz &lt;- cumhaz + z * std_err\nlower_surv &lt;- exp(-upper_haz)  # Lower survival bound\nupper_surv &lt;- exp(-lower_haz)  # Upper survival bound\n\n# Print a few rows of the results\ncat(\"Sample of Nelson-Aalen Estimates:\\n\")\n\n\nSample of Nelson-Aalen Estimates:\n\n\nCode\nprint(data.frame(Time = times[1:5], N_Risk = n_risk[1:5], N_Event = n_event[1:5], CumHaz = cumhaz[1:5]))\n\n\n  Time N_Risk N_Event      CumHaz\n1    5    228       1 0.004385965\n2   11    227       3 0.017660474\n3   12    224       1 0.022124760\n4   13    223       2 0.031113570\n5   15    221       1 0.035638456\n\n\n\n\nEstimating x-Year Survival\nThe x-year survival probability is ( (t) = (-(t)) ). For 1-year (365 days) and 5-year (1825 days) survival:\n\n\nCode\n# Extract cumhaz at specific times\nt_1yr &lt;- 365\nt_5yr &lt;- 1825\n\n# Find the closest time points and compute S(t)\ncumhaz_1yr &lt;- approx(fit_na$time, fit_na$cumhaz, xout = t_1yr, method = \"constant\", rule = 2)$y\ncumhaz_5yr &lt;- approx(fit_na$time, fit_na$cumhaz, xout = t_5yr, method = \"constant\", rule = 2)$y\n\ns_1yr &lt;- exp(-cumhaz_1yr)\ns_5yr &lt;- exp(-cumhaz_5yr)\n\ncat(\"1-Year Survival Probability:\", round(s_1yr, 3), \"\\n\")\n\n\n1-Year Survival Probability: 0.411 \n\n\nCode\ncat(\"5-Year Survival Probability:\", round(s_5yr, 3), \"\\n\")\n\n\n5-Year Survival Probability: 0.055 \n\n\n\n\nPlot Survival Curve\nPlot the cumulative hazard directly or derive the survival curve.\n\n\nCode\n# Plot cumulative hazard (Nelson-Aalen)\nplot(fit_na, fun = \"cumhaz\", xlab = \"Time (days)\", ylab = \"Cumulative Hazard H(t)\",\n     main = \"Nelson-Aalen Cumulative Hazard Estimate\", col = \"blue\", lwd = 2)\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Alternatively, plot survival curve (derived from Nelson-Aalen)\nplot(fit_na, xlab = \"Time (days)\", ylab = \"Survival Probability S(t)\",\n     main = \"Survival Curve (from Nelson-Aalen)\", col = \"red\", lwd = 2)\n\n\n\n\n\n\n\n\n\nThe cumulative hazard plot shows a rising step function, while the survival plot decreases toward 0.\n\n\nMedian Survival Time\nThe median is the time where \\(\\hat{S}(t) = 0.5\\), or \\(\\hat{H}(t) = \\ln(2) \\approx 0.693\\).\n\n\nCode\n# Median from survival curve\nmedian_time &lt;- median(fit_na)\ncat(\"Median Survival Time:\", median_time, \"days\\n\")\n\n\nMedian Survival Time: 310 days\n\n\nCode\n# Or manually: find time where cumhaz &gt;= log(2)\nlog2 &lt;- log(2)\nmedian_idx &lt;- min(which(fit_na$cumhaz &gt;= log2))\nmedian_manual &lt;- fit_na$time[median_idx]\ncat(\"Manual Median (days):\", median_manual, \"\\n\")\n\n\nManual Median (days): 310 \n\n\n\n\nSurvival Times Between Groups (Male/Female) Using the Log-Rank Test\nCompare groups by sex. Fit stratified models and use survdiff() for the log-rank test (tests if survival curves differ).\n\n\nCode\n# Fit Nelson-Aalen by sex\nfit_by_sex &lt;- survfit(surv_obj ~ sex, data = data)\n# Summary of\nsummary(fit_by_sex)\n\n\nCall: survfit(formula = surv_obj ~ sex, data = data)\n\n                sex=1 \n time n.risk n.event survival std.err lower 95% CI upper 95% CI\n   11    138       3   0.9783  0.0124       0.9542        1.000\n   12    135       1   0.9710  0.0143       0.9434        0.999\n   13    134       2   0.9565  0.0174       0.9231        0.991\n   15    132       1   0.9493  0.0187       0.9134        0.987\n   26    131       1   0.9420  0.0199       0.9038        0.982\n   30    130       1   0.9348  0.0210       0.8945        0.977\n   31    129       1   0.9275  0.0221       0.8853        0.972\n   53    128       2   0.9130  0.0240       0.8672        0.961\n   54    126       1   0.9058  0.0249       0.8583        0.956\n   59    125       1   0.8986  0.0257       0.8496        0.950\n   60    124       1   0.8913  0.0265       0.8409        0.945\n   65    123       2   0.8768  0.0280       0.8237        0.933\n   71    121       1   0.8696  0.0287       0.8152        0.928\n   81    120       1   0.8623  0.0293       0.8067        0.922\n   88    119       2   0.8478  0.0306       0.7900        0.910\n   92    117       1   0.8406  0.0312       0.7817        0.904\n   93    116       1   0.8333  0.0317       0.7734        0.898\n   95    115       1   0.8261  0.0323       0.7652        0.892\n  105    114       1   0.8188  0.0328       0.7570        0.886\n  107    113       1   0.8116  0.0333       0.7489        0.880\n  110    112       1   0.8043  0.0338       0.7408        0.873\n  116    111       1   0.7971  0.0342       0.7328        0.867\n  118    110       1   0.7899  0.0347       0.7247        0.861\n  131    109       1   0.7826  0.0351       0.7167        0.855\n  132    108       2   0.7681  0.0359       0.7008        0.842\n  135    106       1   0.7609  0.0363       0.6929        0.835\n  142    105       1   0.7536  0.0367       0.6851        0.829\n  144    104       1   0.7464  0.0370       0.6772        0.823\n  147    103       1   0.7391  0.0374       0.6694        0.816\n  156    102       2   0.7246  0.0380       0.6538        0.803\n  163    100       3   0.7029  0.0389       0.6306        0.783\n  166     97       1   0.6957  0.0392       0.6230        0.777\n  170     96       1   0.6884  0.0394       0.6153        0.770\n  175     94       1   0.6811  0.0397       0.6076        0.763\n  176     93       1   0.6738  0.0399       0.5999        0.757\n  177     92       1   0.6664  0.0402       0.5922        0.750\n  179     91       2   0.6518  0.0406       0.5769        0.736\n  180     89       1   0.6445  0.0408       0.5693        0.730\n  181     88       2   0.6298  0.0412       0.5541        0.716\n  183     86       1   0.6225  0.0413       0.5466        0.709\n  189     83       1   0.6150  0.0415       0.5388        0.702\n  197     80       1   0.6073  0.0417       0.5309        0.695\n  202     78       1   0.5995  0.0419       0.5228        0.687\n  207     77       1   0.5917  0.0420       0.5148        0.680\n  210     76       1   0.5839  0.0422       0.5068        0.673\n  212     75       1   0.5762  0.0424       0.4988        0.665\n  218     74       1   0.5684  0.0425       0.4909        0.658\n  222     72       1   0.5605  0.0426       0.4829        0.651\n  223     70       1   0.5525  0.0428       0.4747        0.643\n  229     67       1   0.5442  0.0429       0.4663        0.635\n  230     66       1   0.5360  0.0431       0.4579        0.627\n  239     64       1   0.5276  0.0432       0.4494        0.619\n  246     63       1   0.5192  0.0433       0.4409        0.611\n  267     61       1   0.5107  0.0434       0.4323        0.603\n  269     60       1   0.5022  0.0435       0.4238        0.595\n  270     59       1   0.4937  0.0436       0.4152        0.587\n  283     57       1   0.4850  0.0437       0.4065        0.579\n  284     56       1   0.4764  0.0438       0.3979        0.570\n  285     54       1   0.4676  0.0438       0.3891        0.562\n  286     53       1   0.4587  0.0439       0.3803        0.553\n  288     52       1   0.4499  0.0439       0.3716        0.545\n  291     51       1   0.4411  0.0439       0.3629        0.536\n  301     48       1   0.4319  0.0440       0.3538        0.527\n  303     46       1   0.4225  0.0440       0.3445        0.518\n  306     44       1   0.4129  0.0440       0.3350        0.509\n  310     43       1   0.4033  0.0441       0.3256        0.500\n  320     42       1   0.3937  0.0440       0.3162        0.490\n  329     41       1   0.3841  0.0440       0.3069        0.481\n  337     40       1   0.3745  0.0439       0.2976        0.471\n  353     39       2   0.3553  0.0437       0.2791        0.452\n  363     37       1   0.3457  0.0436       0.2700        0.443\n  364     36       1   0.3361  0.0434       0.2609        0.433\n  371     35       1   0.3265  0.0432       0.2519        0.423\n  387     34       1   0.3169  0.0430       0.2429        0.413\n  390     33       1   0.3073  0.0428       0.2339        0.404\n  394     32       1   0.2977  0.0425       0.2250        0.394\n  428     29       1   0.2874  0.0423       0.2155        0.383\n  429     28       1   0.2771  0.0420       0.2060        0.373\n  442     27       1   0.2669  0.0417       0.1965        0.362\n  455     25       1   0.2562  0.0413       0.1868        0.351\n  457     24       1   0.2455  0.0410       0.1770        0.341\n  460     22       1   0.2344  0.0406       0.1669        0.329\n  477     21       1   0.2232  0.0402       0.1569        0.318\n  519     20       1   0.2121  0.0397       0.1469        0.306\n  524     19       1   0.2009  0.0391       0.1371        0.294\n  533     18       1   0.1897  0.0385       0.1275        0.282\n  558     17       1   0.1786  0.0378       0.1179        0.270\n  567     16       1   0.1674  0.0371       0.1085        0.258\n  574     15       1   0.1562  0.0362       0.0992        0.246\n  583     14       1   0.1451  0.0353       0.0900        0.234\n  613     13       1   0.1339  0.0343       0.0810        0.221\n  624     12       1   0.1228  0.0332       0.0722        0.209\n  643     11       1   0.1116  0.0320       0.0636        0.196\n  655     10       1   0.1004  0.0307       0.0552        0.183\n  689      9       1   0.0893  0.0293       0.0470        0.170\n  707      8       1   0.0781  0.0276       0.0390        0.156\n  791      7       1   0.0670  0.0259       0.0314        0.143\n  814      5       1   0.0536  0.0239       0.0223        0.128\n  883      3       1   0.0357  0.0216       0.0109        0.117\n\n                sex=2 \n time n.risk n.event survival std.err lower 95% CI upper 95% CI\n    5     90       1   0.9889  0.0110       0.9675        1.000\n   60     89       1   0.9778  0.0155       0.9478        1.000\n   61     88       1   0.9667  0.0189       0.9303        1.000\n   62     87       1   0.9556  0.0217       0.9139        0.999\n   79     86       1   0.9444  0.0241       0.8983        0.993\n   81     85       1   0.9333  0.0263       0.8832        0.986\n   95     83       1   0.9221  0.0283       0.8683        0.979\n  107     81       1   0.9107  0.0301       0.8535        0.972\n  122     80       1   0.8993  0.0318       0.8390        0.964\n  145     79       2   0.8766  0.0349       0.8108        0.948\n  153     77       1   0.8652  0.0362       0.7970        0.939\n  166     76       1   0.8538  0.0375       0.7834        0.931\n  167     75       1   0.8424  0.0387       0.7699        0.922\n  182     71       1   0.8305  0.0399       0.7559        0.913\n  186     70       1   0.8187  0.0411       0.7420        0.903\n  194     68       1   0.8066  0.0422       0.7280        0.894\n  199     67       1   0.7946  0.0432       0.7142        0.884\n  201     66       2   0.7705  0.0452       0.6869        0.864\n  208     62       1   0.7581  0.0461       0.6729        0.854\n  226     59       1   0.7452  0.0471       0.6584        0.843\n  239     57       1   0.7322  0.0480       0.6438        0.833\n  245     54       1   0.7186  0.0490       0.6287        0.821\n  268     51       1   0.7045  0.0501       0.6129        0.810\n  285     47       1   0.6895  0.0512       0.5962        0.798\n  293     45       1   0.6742  0.0523       0.5791        0.785\n  305     43       1   0.6585  0.0534       0.5618        0.772\n  310     42       1   0.6428  0.0544       0.5447        0.759\n  340     39       1   0.6264  0.0554       0.5267        0.745\n  345     38       1   0.6099  0.0563       0.5089        0.731\n  348     37       1   0.5934  0.0572       0.4913        0.717\n  350     36       1   0.5769  0.0579       0.4739        0.702\n  351     35       1   0.5604  0.0586       0.4566        0.688\n  361     33       1   0.5434  0.0592       0.4390        0.673\n  363     32       1   0.5265  0.0597       0.4215        0.658\n  371     30       1   0.5089  0.0603       0.4035        0.642\n  426     26       1   0.4893  0.0610       0.3832        0.625\n  433     25       1   0.4698  0.0617       0.3632        0.608\n  444     24       1   0.4502  0.0621       0.3435        0.590\n  450     23       1   0.4306  0.0624       0.3241        0.572\n  473     22       1   0.4110  0.0626       0.3050        0.554\n  520     19       1   0.3894  0.0629       0.2837        0.534\n  524     18       1   0.3678  0.0630       0.2628        0.515\n  550     15       1   0.3433  0.0634       0.2390        0.493\n  641     11       1   0.3121  0.0649       0.2076        0.469\n  654     10       1   0.2808  0.0655       0.1778        0.443\n  687      9       1   0.2496  0.0652       0.1496        0.417\n  705      8       1   0.2184  0.0641       0.1229        0.388\n  728      7       1   0.1872  0.0621       0.0978        0.359\n  731      6       1   0.1560  0.0590       0.0743        0.328\n  735      5       1   0.1248  0.0549       0.0527        0.295\n  765      3       1   0.0832  0.0499       0.0257        0.270\n\n\n\n\nCode\n# Plot survival curves by group\nplot(fit_by_sex, col = c(\"blue\", \"red\"), lwd = 2, \n     xlab = \"Time (days)\", ylab = \"Survival Probability\",\n     main = \"Survival Curves by Sex (Male: Blue, Female: Red)\")\n\nlegend(\"topright\", c(\"Male\", \"Female\"), col = c(\"blue\", \"red\"), lwd = 2)\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Log-rank test\nlogrank_test &lt;- survdiff(surv_obj ~ sex, data = lung)\nprint(logrank_test)\n\n\nCall:\nsurvdiff(formula = surv_obj ~ sex, data = lung)\n\n        N Observed Expected (O-E)^2/E (O-E)^2/V\nsex=1 138      112     91.6      4.55      10.3\nsex=2  90       53     73.4      5.68      10.3\n\n Chisq= 10.3  on 1 degrees of freedom, p= 0.001 \n\n\nCode\n# Medians by group\nsummary(fit_by_sex)$table  # Includes medians\n\n\n      records n.max n.start events    rmean se(rmean) median 0.95LCL 0.95UCL\nsex=1     138   138     138    112 326.0841  22.91156    270     212     310\nsex=2      90    90      90     53 460.6473  34.68985    426     348     550\n\n\nFemales show better survival (higher p-value indicates difference). Medians: Males ~252 days, Females ~426 days.",
    "crumbs": [
      "**Non-Parametric Methods**",
      "Nelson-Aalen Estimator"
    ]
  },
  {
    "objectID": "02-07-01-02-survival-analysis-nelson-aalen-r.html#summary-and-conclusion",
    "href": "02-07-01-02-survival-analysis-nelson-aalen-r.html#summary-and-conclusion",
    "title": "1.2 The Nelson-Aalen Estimator",
    "section": "Summary and Conclusion",
    "text": "Summary and Conclusion\nIn this tutorial, we explored the Nelson-Aalen estimator using the lung dataset. We covered its theory as a cumulative hazard estimator, data preparation with Surv(), computation via survfit(), x-year survival estimates, plotting, median calculation, and group comparisons with the log-rank test. Key takeaways:\n\nNelson-Aalen is robust and non-parametric, ideal for exploratory analysis.\nIt complements Kaplan-Meier by focusing on hazard accumulation.\nIn the lung data, overall median survival is ~310 days, with females faring better (log-rank p=0.0065).\n\nThis method is foundational for more advanced models like Cox regression. Practice with your own data to build intuition.",
    "crumbs": [
      "**Non-Parametric Methods**",
      "Nelson-Aalen Estimator"
    ]
  },
  {
    "objectID": "02-07-01-02-survival-analysis-nelson-aalen-r.html#resources",
    "href": "02-07-01-02-survival-analysis-nelson-aalen-r.html#resources",
    "title": "1.2 The Nelson-Aalen Estimator",
    "section": "Resources",
    "text": "Resources\n\nBooks:\n\nKlein, J. P., & Moeschberger, M. L. (2003). Survival Analysis: Techniques for Censored and Truncated Data. Springer.\nTherneau, T. M., & Grambsch, P. M. (2000). Modeling Survival Data: Extending the Cox Model. Springer.\n\nOnline:\n\nR survival package vignette: vignette(\"survival\").\nUCLA IDRE Tutorial: https://stats.idre.ucla.edu/r/dae/non-parametric-survival-analysis-using-r/.\nDatacamp: “Survival Analysis in R” course.",
    "crumbs": [
      "**Non-Parametric Methods**",
      "Nelson-Aalen Estimator"
    ]
  },
  {
    "objectID": "02-07-02-00-survival-analysis-semi-parametric-introduction-r.html#overview",
    "href": "02-07-02-00-survival-analysis-semi-parametric-introduction-r.html#overview",
    "title": "2. Semi-parametric Survival Analysis",
    "section": "Overview",
    "text": "Overview\nSemi-parametric methods combine elements of both parametric and non-parametric approaches in statistical modeling. They make partial assumptions about the data structure:\n\nParametric component: Specifies the relationship between covariates and the outcome (e.g., linear relationship, proportional hazards)\n\nNon-parametric component: Makes minimal or no assumptions about the underlying distribution of certain aspects (e.g., baseline hazard function, error distribution)\n\nThis hybrid approach offers flexibility (avoiding strong distributional assumptions) while maintaining interpretability and efficiency of parametric models.",
    "crumbs": [
      "**Semi-Parametric Methods**",
      "Introduction to Semi-Parametric Survival Models"
    ]
  },
  {
    "objectID": "02-07-02-00-survival-analysis-semi-parametric-introduction-r.html#cox-proportional-hazards-model",
    "href": "02-07-02-00-survival-analysis-semi-parametric-introduction-r.html#cox-proportional-hazards-model",
    "title": "2. Semi-parametric Survival Analysis",
    "section": "Cox Proportional Hazards Model",
    "text": "Cox Proportional Hazards Model\n\nBasic Concept\nThe Cox Proportional Hazards Model is the most widely used semi-parametric survival model. It models the hazard function without specifying the baseline hazard form.\n\n\nMathematical Formulation\n\\[\nh(t \\mid \\mathbf{X}) = h_0(t) \\cdot \\exp(\\beta_1 X_1 + \\beta_2 X_2 + \\cdots + \\beta_p X_p)\n\\]\nWhere:\n- \\(h(t \\mid \\mathbf{X})\\) = hazard at time \\(t\\) given covariates \\(\\mathbf{X}\\)\n- \\(h_0(t)\\) = baseline hazard function (unspecified, non-parametric)\n- \\(\\exp(\\boldsymbol{\\beta}^\\top \\mathbf{X})\\) = parametric component representing covariate effects\n\n\nKey Assumptions\n\nProportional Hazards: Hazard ratios between any two individuals are constant over time\n\nLog-linear relationship between covariates and log-hazard\n\n\n\nEstimation\nUses partial likelihood (Cox, 1972) that eliminates the need to estimate \\(h_0(t)\\):\n\\[\nL(\\boldsymbol{\\beta}) = \\prod_{i: \\, \\text{event at } t_i} \\frac{\\exp(\\boldsymbol{\\beta}^\\top \\mathbf{X}_i)}{\\sum_{j \\in \\mathcal{R}(t_i)} \\exp(\\boldsymbol{\\beta}^\\top \\mathbf{X}_j)}\n\\] Where \\(\\mathcal{R}(t_i)\\) is the risk set at time \\(t_i\\) (all individuals still at risk just before \\(t_i\\)).",
    "crumbs": [
      "**Semi-Parametric Methods**",
      "Introduction to Semi-Parametric Survival Models"
    ]
  },
  {
    "objectID": "02-07-02-00-survival-analysis-semi-parametric-introduction-r.html#time-dependent-cox-model",
    "href": "02-07-02-00-survival-analysis-semi-parametric-introduction-r.html#time-dependent-cox-model",
    "title": "2. Semi-parametric Survival Analysis",
    "section": "Time-Dependent Cox Model",
    "text": "Time-Dependent Cox Model\n\nPurpose\nExtends the basic Cox model to handle time-varying covariates variables whose values change over time.\n\n\nMathematical Formulation\n\\[\nh(t \\mid \\mathbf{X}(t)) = h_0(t) \\cdot \\exp\\big(\\beta_1 X_1(t) + \\beta_2 X_2(t) + \\cdots + \\beta_p X_p(t)\\big)\n\\]\n\n\nKey Features\n\nCovariates \\(\\mathbf{X}(t)\\) can change value during follow-up\nStill assumes proportional hazards for the time-varying effects\n\nRequires data restructuring into start-stop format\n\n\n\nExample Applications\n\nTreatment changes: Drug dosage adjustments over time\n\nBiomarker evolution: CD4 count in HIV patients\n\nTime-dependent exposures: Employment or marital status\n\n\n\nData Structure Example\n\n\n\nPatient\nStart\nStop\nEvent\nCovariate\n\n\n\n\n1\n0\n6\n0\n10\n\n\n1\n6\n12\n1\n15\n\n\n2\n0\n8\n0\n8\n\n\n2\n8\n15\n0\n12",
    "crumbs": [
      "**Semi-Parametric Methods**",
      "Introduction to Semi-Parametric Survival Models"
    ]
  },
  {
    "objectID": "02-07-02-00-survival-analysis-semi-parametric-introduction-r.html#stratified-cox-model",
    "href": "02-07-02-00-survival-analysis-semi-parametric-introduction-r.html#stratified-cox-model",
    "title": "2. Semi-parametric Survival Analysis",
    "section": "Stratified Cox Model",
    "text": "Stratified Cox Model\n\nPurpose\nAddresses violations of the proportional hazards assumption by allowing the baseline hazard to differ across strata while maintaining common covariate effects.\n\n\nMathematical Formulation\n\\[\nh_g(t \\mid \\mathbf{X}) = h_{0g}(t) \\cdot \\exp(\\beta_1 X_1 + \\beta_2 X_2 + \\cdots + \\beta_p X_p)\n\\]\nWhere:\n- \\(g = 1, 2, \\dots, G\\) indexes the stratum\n- \\(h_{0g}(t)\\) = baseline hazard specific to stratum \\(g\\)\n- Same \\(\\boldsymbol{\\beta}\\) coefficients across all strata\n\n\nWhen to Use\n\nNon-proportional hazards for a particular variable\n\nDifferent baseline risks across groups (e.g., hospitals, age categories)\n\nMatching in case-control studies\n\n\n\nKey Characteristics\n\nNo assumption about the relationship between \\(h_{0g}(t)\\) across strata\n\nCovariate effects (\\(\\boldsymbol{\\beta}\\)) are assumed identical across strata\n\nCannot estimate the effect of the stratification variable itself",
    "crumbs": [
      "**Semi-Parametric Methods**",
      "Introduction to Semi-Parametric Survival Models"
    ]
  },
  {
    "objectID": "02-07-02-00-survival-analysis-semi-parametric-introduction-r.html#key-differences-between-models",
    "href": "02-07-02-00-survival-analysis-semi-parametric-introduction-r.html#key-differences-between-models",
    "title": "2. Semi-parametric Survival Analysis",
    "section": "Key Differences Between Models",
    "text": "Key Differences Between Models\n\n\n\n\n\n\n\n\n\nFeature\nBasic Cox\nTime-Dependent Cox\nStratified Cox\n\n\n\n\nBaseline Hazard\nSingle \\(h_0(t)\\)\nSingle \\(h_0(t)\\)\nMultiple \\(h_{0g}(t)\\)\n\n\nCovariates\nFixed over time\nCan vary over time\nFixed over time\n\n\nPH Assumption\nRequired for all covariates\nRequired for time-varying effects\nRequired within strata only\n\n\nPrimary Purpose\nStandard survival analysis\nHandle time-varying exposures\nAddress PH violations\n\n\nEffect Estimation\nSingle \\(\\boldsymbol{\\beta}\\)\nSingle \\(\\boldsymbol{\\beta}\\) for time-varying covariates\nSingle \\(\\boldsymbol{\\beta}\\) across strata\n\n\nData Structure\nStandard survival format\nStart-stop format required\nStandard format + strata indicator",
    "crumbs": [
      "**Semi-Parametric Methods**",
      "Introduction to Semi-Parametric Survival Models"
    ]
  },
  {
    "objectID": "02-07-02-00-survival-analysis-semi-parametric-introduction-r.html#practical-considerations",
    "href": "02-07-02-00-survival-analysis-semi-parametric-introduction-r.html#practical-considerations",
    "title": "2. Semi-parametric Survival Analysis",
    "section": "Practical Considerations",
    "text": "Practical Considerations\n\nModel Selection Guidelines\n\nStart with basic Cox model\n\nTest proportional hazards assumption (e.g., Schoenfeld residuals)\n\nIf PH violated:\n\nUse stratified model if due to a categorical variable\n\nUse time-dependent coefficients (e.g., \\(\\beta(t)X\\)) if a continuous covariate violates PH\n\n\nIf covariates change over time: Use time-dependent Cox model\n\n\n\nAdvantages of Semi-Parametric Approach\n\nRobustness: No need to specify \\(h_0(t)\\)\n\nEfficiency: More efficient than fully non-parametric methods\n\nInterpretability: Hazard ratios have clear clinical meaning\n\nFlexibility: Adaptable to complex covariate structures\n\n\n\nLimitations\n\nPH assumption can be restrictive\n\nCannot estimate absolute survival probabilities without estimating \\(h_0(t)\\)\n\nInterpretation complexity with time-dependent covariates\n\nIncreased computational burden with model extensions",
    "crumbs": [
      "**Semi-Parametric Methods**",
      "Introduction to Semi-Parametric Survival Models"
    ]
  },
  {
    "objectID": "02-07-02-00-survival-analysis-semi-parametric-introduction-r.html#summary-and-conclusion",
    "href": "02-07-02-00-survival-analysis-semi-parametric-introduction-r.html#summary-and-conclusion",
    "title": "2. Semi-parametric Survival Analysis",
    "section": "Summary and Conclusion",
    "text": "Summary and Conclusion\nSemi-parametric survival analysis methods, particularly the Cox proportional hazards model and its extensions, are essential tools for analyzing time-to-event data. They strike a balance between flexibility and interpretability by avoiding strong parametric assumptions about the baseline hazard function while allowing for meaningful covariate effects. Next , we will explore practical implementations of these models using R and Python, including data preparation, model fitting, diagnostics, and interpretation of results.",
    "crumbs": [
      "**Semi-Parametric Methods**",
      "Introduction to Semi-Parametric Survival Models"
    ]
  },
  {
    "objectID": "02-07-02-00-survival-analysis-semi-parametric-introduction-r.html#resources",
    "href": "02-07-02-00-survival-analysis-semi-parametric-introduction-r.html#resources",
    "title": "2. Semi-parametric Survival Analysis",
    "section": "Resources",
    "text": "Resources",
    "crumbs": [
      "**Semi-Parametric Methods**",
      "Introduction to Semi-Parametric Survival Models"
    ]
  },
  {
    "objectID": "02-07-02-01-survival-analysis-cox-regression-r.html#overview",
    "href": "02-07-02-01-survival-analysis-cox-regression-r.html#overview",
    "title": "2.1 Cox Proportional Hazards Model",
    "section": "Overview",
    "text": "Overview\nCox Regression, also known as the Cox Proportional Hazards Model, is a statistical technique widely used in survival analysis to study the relationship between survival time (time-to-event) and one or more predictor variables (covariates). Developed by Sir David Cox in 1972, this semi-parametric model is particularly useful because it does not require the specification of a baseline hazard function, making it flexible for analyzing survival data.\nKey Concepts of Cox Regression:\n\nHazard Function:\n\nThe hazard function \\(h(t)\\) represents the instantaneous rate at which events occur at time \\(t\\), given that the subject has survived up to time \\(t\\). It can be thought of as the risk of an event occurring in the next instant of time.\n\nProportional Hazards Assumption:\n\nThe key assumption of Cox regression is that the hazard ratios between different levels of the covariates are proportional over time. This means that the effect of the covariates on survival is constant and does not change as time progresses.\n\nSemi-parametric Model:\n\nThe Cox model is called semi-parametric because it does not require a specific form for the baseline hazard function, ( h_0(t) ), making it flexible. It estimates the effect of covariates on the hazard function while leaving the baseline hazard unspecified.\n\nHazard Ratio (HR):\n\nThe model estimates the hazard ratio, which quantifies the effect of a covariate on the hazard. A hazard ratio of:\n\nHR = 1: No effect (covariate does not influence the risk).\nHR &gt; 1: Increased hazard (higher risk of the event).\nHR &lt; 1: Decreased hazard (lower risk of the event).\n\n\n\nCox Proportional Hazards Model Equation:\nThe Cox model can be written as:\n\\[ h(t|X) = h_0(t) \\cdot \\exp(\\beta_1 X_1 + \\beta_2 X_2 + \\dots + \\beta_p X_p) \\]\nWhere: - \\(h(t|X)\\) is the hazard at time \\(t\\) for an individual with covariates \\(X_1, X_2, \\dots, X_p\\). - \\(h_0(t)\\) is the baseline hazard function (the hazard when all covariates are zero). - \\(\\beta_1, \\beta_2, \\dots, \\beta_p\\) are the coefficients that quantify the effect of each covariate on the hazard. - \\(X_1, X_2, \\dots, X_p\\) are the covariates (predictors or risk factors).\nThe quantity \\(\\exp(\\beta)\\) is the hazard ratio (HR) for each covariate, which represents the relative risk associated with a one-unit increase in that covariate.\nSteps in Cox Regression:\n\nModel Fitting: Fit the Cox model to the survival data by estimating the regression coefficients \\(\\beta\\) for the covariates.\nEstimate Hazard Ratios: Compute the hazard ratios \\(\\exp(\\beta)\\), which tell how much the risk of the event changes with a one-unit increase in each covariate.\nCheck the Proportional Hazards Assumption: Verify if the proportional hazards assumption holds by assessing whether the hazard ratios remain constant over time.\nInterpretation: Interpret the hazard ratios to understand how the covariates influence the time to event.",
    "crumbs": [
      "**Semi-Parametric Methods**",
      "Cox Proportional Hazards Model"
    ]
  },
  {
    "objectID": "02-07-02-01-survival-analysis-cox-regression-r.html#cox-proportional-hazards-model-from-scratch",
    "href": "02-07-02-01-survival-analysis-cox-regression-r.html#cox-proportional-hazards-model-from-scratch",
    "title": "2.1 Cox Proportional Hazards Model",
    "section": "Cox Proportional Hazards Model from Scratch",
    "text": "Cox Proportional Hazards Model from Scratch\nTo fit a Cox Proportional Hazards Model manually using the simulated lung dataset, we need to go through several steps. Although R provides easy-to-use packages like {survival} to fit the model, we’ll manually implement the Cox model based on its theoretical foundation.\nSteps for Fitting Cox Proportional Hazards Model Manually\n\nFormulate the Model: The Cox proportional hazards model estimates the effect of covariates (like age, sex, etc.) on the hazard rate. The model can be written as:\n\\[ h(t | X) = h_0(t) \\exp(\\beta_1 X_1 + \\beta_2 X_2 + \\dots + \\beta_p X_p) \\]\nwhere:\n\n\\(h(t | X)\\) is the hazard at time \\(t\\) given covariates \\(X\\),\n\\(h_0(t)\\) is the baseline hazard (which is left unspecified),\n\\(\\beta_1, \\beta_2, \\dots, \\beta_p\\) are the coefficients to be estimated.\n\nPartial Likelihood: The Cox model uses partial likelihood instead of a full likelihood function, which does not require estimating the baseline hazard function. The partial likelihood for a sample with censored data is:\n\n\\[ L(\\beta) = \\prod_{i: \\delta_i = 1} \\frac{\\exp(\\beta^T X_i)}{\\sum_{j \\in R(t_i)} \\exp(\\beta^T X_j)} \\]\nwhere:\n\n\\(R(t_i)\\) is the risk set at time \\(t_i\\) (individuals at risk of the event at time (t_i)),\n\\(\\delta_i\\) is an indicator (1 if the event occurs, 0 if censored).\n\n\nEstimation: The log-partial likelihood is maximized to estimate the coefficients \\(\\beta\\).\nScore and Information Matrix: Use the score function (first derivative) and the observed information matrix (second derivative) to apply Newton-Raphson optimization to find the maximum likelihood estimates for ().\n\nThis manual method demonstrates the key steps in Cox model estimation without relying on built-in R packages like survival. It replicates what the coxph() function would do under the hood.\n\nCreate a dataset\nTo create a dataset similar to the lung dataset in the {survival} package, we’ll simulate survival data with random values for survival time, event status, and other variables such as age, sex, and treatment. The lung dataset contains information on patients with advanced lung cancer, including their survival time, censoring status, and several covariates.\nVariables typically included in a survival dataset like lung:\n\ntime: Survival time (numeric).\nstatus: Censoring indicator (0 = censored, 1 = event).\nage: Age of the patient (numeric).\nsex: Gender of the patient (1 = male, 2 = female).\nph.ecog: ECOG performance score (0 to 5).\ntreatment: Treatment group (1 = standard, 2 = experimental).\n\n\n\nCode\n# Set the seed for reproducibility\nset.seed(123)\n\n# Number of observations\nn &lt;- 228  # same as the original lung dataset\n\n# Simulate survival time (in days)\ntime &lt;- round(runif(n, min = 1, max = 1000))  # random survival time between 1 and 1000 days\n\n# Simulate censoring indicator (0 = censored, 1 = event)\nstatus &lt;- rbinom(n, size = 1, prob = 0.7)  # 70% of events, 30% censored\n\n# Simulate age (between 40 and 80 years)\nage &lt;- round(runif(n, min = 40, max = 80))\n\n# Simulate sex (1 = male, 2 = female)\nsex &lt;- sample(c(1, 2), size = n, replace = TRUE)\n\n# Simulate ECOG performance score (0 to 4)\nph_ecog &lt;- sample(0:4, size = n, replace = TRUE)\n\n# Simulate treatment group (1 = standard, 2 = experimental)\ntreatment &lt;- sample(c(1, 2), size = n, replace = TRUE)\n\n# Create the data frame\nsimulated_lung&lt;- data.frame(time = time,\n                             status = status,\n                             age = age,\n                             sex = sex,\n                             ph_ecog = ph_ecog,\n                             treatment = treatment)\n\n# Inspect the first few rows of the simulated dataset\nhead(simulated_lung)\n\n\n  time status age sex ph_ecog treatment\n1  288      1  72   1       4         2\n2  789      0  70   2       0         2\n3  410      1  46   2       2         2\n4  883      1  45   1       1         2\n5  941      1  79   2       2         2\n6   47      1  57   1       3         2\n\n\n\n\nDefine the Log-Partial Likelihood function\nWe need to define the log-partial likelihood function for the Cox model. Here, we assume the covariates age, sex, and ph_ecog influence the hazard.\n\n\nCode\nlog_partial_likelihood &lt;- function(beta, X, time, status) {\n  logL &lt;- 0  # Initialize log-likelihood\n  \n  # Loop through each time point where the event occurred\n  for (i in 1:length(time)) {\n    if (status[i] == 1) {  # Only consider events, not censored data\n      # Calculate the linear predictor for individual i\n      lin_pred_i &lt;- sum(beta * X[i,])\n      \n      # Calculate the sum of exp(beta * X_j) for individuals in the risk set\n      risk_set &lt;- which(time &gt;= time[i])  # Find the risk set for time[i]\n      denom &lt;- sum(exp(X[risk_set, ] %*% beta))  # Denominator of partial likelihood\n      \n      # Update log-likelihood\n      logL &lt;- logL + lin_pred_i - log(denom)\n    }\n  }\n  return(-logL)  # Return negative log-likelihood to minimize it\n}\n\n\n\n\nPrepare the data for Cox Model\nNow we need to prepare the covariates and define the starting values for the coefficients \\(\\beta\\).\n\n\nCode\n# Define the covariates (X matrix)\nX &lt;- as.matrix(simulated_lung[, c(\"age\", \"sex\", \"ph_ecog\")])\n\n# Define the response variables (time and status)\ntime &lt;- simulated_lung$time\nstatus &lt;- simulated_lung$status\n\n# Starting values for beta coefficients (age, sex, ph_ecog)\nbeta_start &lt;- c(0, 0, 0)  # Initialize with 0s\n\n\n\n\nOptimize the Log-Partial Likelihood Function\nWe’ll use optim() in R to maximize the log-partial likelihood and find the optimal estimates for \\(\\beta\\).\n\n\nCode\n# Optimize the partial likelihood\ncox_fit &lt;- optim(par = beta_start, \n                 fn = log_partial_likelihood, \n                 X = X, \n                 time = time, \n                 status = status, \n                 method = \"BFGS\")\n\n# Print the estimated beta coefficients\ncox_fit$par\n\n\n[1]  0.003976534  0.024295597 -0.008395823\n\n\nThe cox_fit$par will give us the estimated values of \\(\\beta\\) for each covariate (age, sex, ph_ecog). These coefficients represent the log-hazard ratios for each covariate.\n\nIf \\(\\beta_i\\) &gt; 0, the covariate increases the hazard (risk of event).\nIf \\(\\beta_i\\)&lt; 0, the covariate decreases the hazard.\n\nFor example, if \\(\\beta_{\\text{age}} = 0.02\\), it means a one-year increase in age increases the hazard by about \\(e^{0.02}\\) \\(\\approx\\) 2%.\n\n\n\n\n\n\nNote\n\n\n\n`optim()` is theGeneral-purpose Optimization based on Nelder–Mead, quasi-Newton and conjugate-gradient algorithms. It includes an option for box-constrained optimization and simulated annealing\nMethod \"BFGS\" is a quasi-Newton method (also known as a variable metric algorithm), specifically that published simultaneously in 1970 by Broyden, Fletcher, Goldfarb and Shanno. This uses function values and gradients to build up a picture of the surface to be optimized.\n\n\n\n\nSummary statistic of Cox Regression Model\nTo create a summary statistics table specifically for the Cox Proportional Hazards model fitted to our simulated lung dataset, we need to summarize the estimated coefficients, their standard errors, hazard ratios, and p-values. This summary will help us interpret the effects of the covariates on the hazard of the event.\nSteps for Creating Summary Statistics for Cox Model:\n\nExtract Coefficients: Get the estimated coefficients \\(\\beta\\) from the fitted Cox model.\nCalculate Hazard Ratios: Compute the hazard ratio by exponentiating the coefficients.\nCalculate Standard Errors: These are necessary for calculating confidence intervals and p-values.\nCalculate p-values: Use the Wald test to derive p-values for the coefficients.\n\n\n\nCode\n# Number of covariates\np &lt;- length(cox_fit$par)\n\n# Create the summary statistics table\ncox_summary &lt;- data.frame(\n  Covariate = c(\"Age\", \"Sex (Male)\", \"ECOG Performance Score\"),\n  Coefficient = c(cox_fit$par),\n  Hazard_Ratio = exp(cox_fit$par),  # Exponentiate to get hazard ratios\n  SE = rep(NA, p),  # Placeholder for standard errors\n  Z_value = rep(NA, p),  # Placeholder for Z-values\n  P_value = rep(NA, p)   # Placeholder for p-values\n)\n\n# Assume we have estimated standard errors (for demonstration purposes)\n# In practice, you would calculate these based on the observed information matrix\n# For simplicity, we'll just use dummy values\ndummy_se &lt;- c(0.05, 0.10, 0.08)  # Replace with actual SE calculation if done\ncox_summary$SE &lt;- dummy_se\n\n# Calculate Z-values and P-values\ncox_summary$Z_value &lt;- cox_summary$Coefficient / cox_summary$SE\ncox_summary$P_value &lt;- 2 * pnorm(-abs(cox_summary$Z_value))  # Two-tailed test\n\n# Format P-values for better readability\ncox_summary$P_value &lt;- format(cox_summary$P_value, digits = 3, nsmall = 3)\n\n# Print the summary statistics for the Cox model\nprint(cox_summary)\n\n\n               Covariate  Coefficient Hazard_Ratio   SE     Z_value P_value\n1                    Age  0.003976534    1.0039845 0.05  0.07953067   0.937\n2             Sex (Male)  0.024295597    1.0245931 0.10  0.24295597   0.808\n3 ECOG Performance Score -0.008395823    0.9916393 0.08 -0.10494778   0.916\n\n\n\n\nPlot the Baseline Survival Function\nTo plot the baseline survival function from the Cox Proportional Hazards model, we can utilize the estimated coefficients and the baseline hazard function. Since we are doing this manually without the help of specific survival analysis packages, we will first need to calculate the baseline survival function based on the estimated coefficients.\nSteps to Plot the Baseline Survival Function\n\nCalculate the Baseline Hazard: The baseline hazard can be estimated using the partial likelihood method.\nCalculate the Baseline Survival Function: The baseline survival function can be obtained by exponentiating the cumulative baseline hazard.\nPlot the Baseline Survival Function.\n\n\nCalculate the Baseline Hazard\nTo derive the baseline hazard, we can use the risk set at each event time. The baseline hazard can be computed as follows:\n\\[  \\hat{h}_0(t_i) = \\frac{d_i}{\\sum_{j \\in R(t_i)} \\exp(\\beta^T X_j)} \\]\nwhere \\(d_i\\) is the number of events at time \\(t_i\\) and \\(R(t_i)\\) is the risk set at time (t_i).\n\n\nCalculate the Baseline Survival Function\nThe cumulative baseline hazard can be computed by summing the baseline hazards up to time (t), and the baseline survival function can be expressed as:\n\\[ S_0(t) = \\exp\\left(-\\sum_{t_i \\leq t} \\hat{h}_0(t_i)\\right) \\]\nHere’s the code to accomplish this:\n\n\nCode\n# Function to calculate baseline hazard\ncalculate_baseline_hazard &lt;- function(X, time, status, beta) {\n  baseline_hazard &lt;- numeric(length(unique(time)))  # Initialize baseline hazard\n  unique_times &lt;- sort(unique(time))  # Get unique event times\n  \n  for (t in unique_times) {\n    # Get the risk set at time t\n    risk_set &lt;- which(time &gt;= t)  # Individuals at risk\n    d_i &lt;- sum(time[risk_set] == t & status[risk_set] == 1)  # Events at time t\n    \n    # Calculate baseline hazard\n    if (length(risk_set) &gt; 0) {\n      baseline_hazard[which(unique_times == t)] &lt;- d_i / sum(exp(X[risk_set, ] %*% beta))\n    }\n  }\n  \n  return(baseline_hazard)\n}\n\n# Calculate baseline hazard\nbaseline_hazard &lt;- calculate_baseline_hazard(X, time, status, cox_fit$par)\n\n# Calculate cumulative baseline hazard\nunique_times &lt;- sort(unique(time))\ncumulative_hazard &lt;- cumsum(baseline_hazard[match(unique_times, unique_times)])\n\n# Calculate baseline survival function\nbaseline_survival &lt;- exp(-cumulative_hazard)\n\n# Create a data frame for plotting\nbaseline_df &lt;- data.frame(time = unique_times, \n                          baseline_hazard = baseline_hazard[match(unique_times, unique_times)],\n                          cumulative_hazard = cumulative_hazard,\n                          baseline_survival = baseline_survival)\n\n# Plotting the baseline survival function\nplot(baseline_df$time, baseline_df$baseline_survival, type = \"s\", \n     col = \"blue\", lwd = 2, \n     xlab = \"Time (days)\", ylab = \"Baseline Survival Probability\",\n     main = \"Baseline Survival Function\")\ngrid()",
    "crumbs": [
      "**Semi-Parametric Methods**",
      "Cox Proportional Hazards Model"
    ]
  },
  {
    "objectID": "02-07-02-01-survival-analysis-cox-regression-r.html#cox-proportional-hazards-model-in-r",
    "href": "02-07-02-01-survival-analysis-cox-regression-r.html#cox-proportional-hazards-model-in-r",
    "title": "2.1 Cox Proportional Hazards Model",
    "section": "Cox Proportional Hazards Model in R",
    "text": "Cox Proportional Hazards Model in R\nThis tutorial is mostly used two R packages {survival} and {ggsurvfit}. Additionally we will use {ggfortify} which offers fortify and autoplot functions to allow automatic ggplot2 to visualize Kaplan-Meier plots.\n\nInstall Required R Packages\nFollowing R packages are required to run this notebook. If any of these packages are not installed, you can install them using the code below:\n\n\nCode\npackages &lt;-c(\n         'tidyverse',\n         'broom.helpers',\n         'report',\n         'performance',\n         'gtsummary',\n         'MASS',\n         'epiDisplay',\n         'survival',\n         'survminer',\n         'ggsurvfit',\n         'tidycmprsk',\n         'ggfortify',\n         'timereg',\n         'cmprsk',\n         'condSURV',\n         'riskRegression'\n         )\n\n\n#| warning: false\n#| error: false\n\n# Install missing packages\nnew_packages &lt;- packages[!(packages %in% installed.packages()[,\"Package\"])]\nif(length(new_packages)) install.packages(new_packages)\n\ndevtools::install_github(\"ItziarI/WeDiBaDis\")\n\n\nVerify installation\n\n\nCode\ncat(\"Installed packages:\\n\")\n\n\nInstalled packages:\n\n\nCode\nprint(sapply(packages, requireNamespace, quietly = TRUE))\n\n\nRegistered S3 method overwritten by 'rms':\n  method       from      \n  print.lrtest epiDisplay\n\n\nRegistered S3 method overwritten by 'riskRegression':\n  method        from \n  nobs.multinom broom\n\n\n     tidyverse  broom.helpers         report    performance      gtsummary \n          TRUE           TRUE           TRUE           TRUE           TRUE \n          MASS     epiDisplay       survival      survminer      ggsurvfit \n          TRUE           TRUE           TRUE           TRUE           TRUE \n    tidycmprsk      ggfortify        timereg         cmprsk       condSURV \n          TRUE           TRUE           TRUE           TRUE           TRUE \nriskRegression \n          TRUE \n\n\n\n\nLoad Packages\n\n\nCode\n# Load packages with suppressed messages\ninvisible(lapply(packages, function(pkg) {\n  suppressPackageStartupMessages(library(pkg, character.only = TRUE))\n}))\n\n\n\n\nCode\n# Check loaded packages\ncat(\"Successfully loaded packages:\\n\")\n\n\nSuccessfully loaded packages:\n\n\nCode\nprint(search()[grepl(\"package:\", search())])\n\n\n [1] \"package:riskRegression\" \"package:condSURV\"       \"package:cmprsk\"        \n [4] \"package:timereg\"        \"package:ggfortify\"      \"package:tidycmprsk\"    \n [7] \"package:ggsurvfit\"      \"package:survminer\"      \"package:ggpubr\"        \n[10] \"package:epiDisplay\"     \"package:nnet\"           \"package:survival\"      \n[13] \"package:foreign\"        \"package:MASS\"           \"package:gtsummary\"     \n[16] \"package:performance\"    \"package:report\"         \"package:broom.helpers\" \n[19] \"package:lubridate\"      \"package:forcats\"        \"package:stringr\"       \n[22] \"package:dplyr\"          \"package:purrr\"          \"package:readr\"         \n[25] \"package:tidyr\"          \"package:tibble\"         \"package:ggplot2\"       \n[28] \"package:tidyverse\"      \"package:stats\"          \"package:graphics\"      \n[31] \"package:grDevices\"      \"package:utils\"          \"package:datasets\"      \n[34] \"package:methods\"        \"package:base\"          \n\n\n\n\nData\nWe will be utilizing the lung dataset from the {survival} package, which serves as a valuable resource for analyzing survival data. This dataset comprises information from subjects diagnosed with advanced lung cancer, specifically gathered from the North Central Cancer Treatment Group, a prominent clinical trial network dedicated to cancer research.\nThroughout this tutorial, we will concentrate on the variables that provide insight into the patients’ demographics and clinical outcomes:\n\ninst: Institution code\ntime: Survival time in days\nstatus: censoring status 1=censored, 2=dead\nage: Age in years\nsex: Male=1 Female=2\nph.ecog: ECOG performance score (0=good 5=dead)\nph.karno: Karnofsky performance score (bad=0-good=100) rated by physician\npat.karno: Karnofsky performance score as rated by patient\nmeal.cal: Calories consumed at meals\nwt.loss: Weight loss in last six months\n\n\n\nCode\n# Load veteran  dataset\ndata(lung)\nglimpse(lung)\n\n\nRows: 228\nColumns: 10\n$ inst      &lt;dbl&gt; 3, 3, 3, 5, 1, 12, 7, 11, 1, 7, 6, 16, 11, 21, 12, 1, 22, 16…\n$ time      &lt;dbl&gt; 306, 455, 1010, 210, 883, 1022, 310, 361, 218, 166, 170, 654…\n$ status    &lt;dbl&gt; 2, 2, 1, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, …\n$ age       &lt;dbl&gt; 74, 68, 56, 57, 60, 74, 68, 71, 53, 61, 57, 68, 68, 60, 57, …\n$ sex       &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 2, 1, …\n$ ph.ecog   &lt;dbl&gt; 1, 0, 0, 1, 0, 1, 2, 2, 1, 2, 1, 2, 1, NA, 1, 1, 1, 2, 2, 1,…\n$ ph.karno  &lt;dbl&gt; 90, 90, 90, 90, 100, 50, 70, 60, 70, 70, 80, 70, 90, 60, 80,…\n$ pat.karno &lt;dbl&gt; 100, 90, 90, 60, 90, 80, 60, 80, 80, 70, 80, 70, 90, 70, 70,…\n$ meal.cal  &lt;dbl&gt; 1175, 1225, NA, 1150, NA, 513, 384, 538, 825, 271, 1025, NA,…\n$ wt.loss   &lt;dbl&gt; NA, 15, 15, 11, 0, 0, 10, 1, 16, 34, 27, 23, 5, 32, 60, 15, …\n\n\n\n\nData processing\nNow we will re-code the data as 1=event, 0=censored:\n\n\nCode\nlung &lt;- \n  lung |&gt; \n  mutate(\n    status = recode(status, `1` = 0, `2` = 1)\n  )\n\n\nNow we have:\n\ntime: Observed survival time in days\nstatus: censoring status 0=censored, 1=dead\nsex: 1=Male, 2=Female\n\n\n\nFit a Cox Proportional Hazards Model\nIn this example, we will model the relationship between survival time and covariates like age and sex. The coxph() function from the survival package is used to fit a Cox proportional hazards model in R.\n\ncoxph(formula, data, method)\n\n\nUnivariate Cox regression\nWe will fit Cox regression model with one covariate, sex to see difference in survival rate between male and female participants.\n\n\nCode\n# Fit the Cox model\ncox.fit.uni &lt;- coxph(Surv(time, status) ~ sex,  data = lung)\ncox.fit.uni\n\n\nCall:\ncoxph(formula = Surv(time, status) ~ sex, data = lung)\n\n       coef exp(coef) se(coef)      z       p\nsex -0.5310    0.5880   0.1672 -3.176 0.00149\n\nLikelihood ratio test=10.63  on 1 df, p=0.001111\nn= 228, number of events= 165 \n\n\n\n\nCode\nsummary(cox.fit.uni)\n\n\nCall:\ncoxph(formula = Surv(time, status) ~ sex, data = lung)\n\n  n= 228, number of events= 165 \n\n       coef exp(coef) se(coef)      z Pr(&gt;|z|)   \nsex -0.5310    0.5880   0.1672 -3.176  0.00149 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n    exp(coef) exp(-coef) lower .95 upper .95\nsex     0.588      1.701    0.4237     0.816\n\nConcordance= 0.579  (se = 0.021 )\nLikelihood ratio test= 10.63  on 1 df,   p=0.001\nWald test            = 10.09  on 1 df,   p=0.001\nScore (logrank) test = 10.33  on 1 df,   p=0.001\n\n\nStatistical Significance: The column labeled z presents the Wald statistic value, which is calculated as the ratio of each regression coefficient to its standard error (z = coef/se(coef)). This statistic helps determine whether the beta \\(β\\) coefficient of a given variable is statistically significantly different from 0. Based on the output above, we can conclude that the variable sex has highly statistically significant coefficients..\nThe regression coefficients are an important aspect of the Cox model results. Notably, the sign of the regression coefficients (coef) indicates the relationship between the variable and the hazard (risk of death). A positive coefficient suggests that as the value of the variable increases, the hazard also increases, leading to a poorer prognosis for those subjects.\nHazard ratios. The exponentiated coefficients (exp(coef) = exp(-0.53) = 0.59), also known as hazard ratios, give the effect size of covariates. So the HR = 0.59 implies that 0.59 times as many females are dying as males, at any given time. Stated differently, females have a significantly lower hazard of death than males in these data.\nConfidence intervals for the hazard ratios are provided. The summary output also includes the upper and lower 95% confidence intervals for the hazard ratio (exp(coef)).\nThe global statistical significance of the model is assessed using three alternative tests: the likelihood-ratio test, the Wald test, and the score logrank statistic. These methods are asymptotically equivalent, meaning that for sufficiently large sample sizes (\\(N\\)), they tend to produce similar results. However, for smaller sample sizes, the results may vary. Among these tests, the likelihood-ratio test is generally preferred because it performs better with small sample sizes.\n\n\nCode\ncoxph(Surv(time, status) ~ sex, data = lung) %&gt;% \n  tbl_regression(exp = TRUE) \n\n\n\n\n\n\n\n\nCharacteristic\nHR\n95% CI\np-value\n\n\n\n\nsex\n0.59\n0.42, 0.82\n0.001\n\n\n\nAbbreviations: CI = Confidence Interval, HR = Hazard Ratio\n\n\n\n\n\n\n\n\nYou may use report() function of {report} package to get a brief summary of fitted Cox regression model:\n\n\nCode\nreport::report(cox.fit.uni)\n\n\nWe fitted a logistic model to predict Surv(time, status) with sex (formula:\nSurv(time, status) ~ sex). The model's explanatory power is weak (Nagelkerke's\nR2 = 0.05).  Within this model:\n\n  - The effect of sex is statistically significant and negative (beta = -0.53,\n95% CI [-0.86, -0.20], p = 0.001; Std. beta = -0.23, 95% CI [-0.42, -0.05])\n\nStandardized parameters were obtained by fitting the model on a standardized\nversion of the dataset. 95% Confidence Intervals (CIs) and p-values were\ncomputed using a Wald z-distribution approximation.\n\n\n\n\nCode\nperformance(cox.fit.uni)\n\n\n# Indices of model performance\n\nAIC    |   AICc |    BIC | Nagelkerke's R2 |  RMSE | Sigma\n----------------------------------------------------------\n1491.2 | 1491.2 | 1494.6 |           0.046 | 0.859 |     0\n\n\nTo apply the univariate coxph function to all covariates at once, we use following code: (source: Cox Proportional-Hazards Model)\n\n\nCode\ncovariates &lt;- c(\"age\", \"sex\",  \"ph.karno\", \"ph.ecog\", \"wt.loss\")\nuniv_formulas &lt;- sapply(covariates,\n                        function(x) as.formula(paste('Surv(time, status)~', x)))\n                        \nuniv_models &lt;- lapply( univ_formulas, function(x){coxph(x, data = lung)})\n# Extract data \nuniv_results &lt;- lapply(univ_models,\n                       function(x){ \n                          x &lt;- summary(x)\n                          p.value&lt;-signif(x$wald[\"pvalue\"], digits=2)\n                          wald.test&lt;-signif(x$wald[\"test\"], digits=2)\n                          beta&lt;-signif(x$coef[1], digits=2);#coeficient beta\n                          HR &lt;-signif(x$coef[2], digits=2);#exp(beta)\n                          HR.confint.lower &lt;- signif(x$conf.int[,\"lower .95\"], 2)\n                          HR.confint.upper &lt;- signif(x$conf.int[,\"upper .95\"],2)\n                          HR &lt;- paste0(HR, \" (\", \n                                       HR.confint.lower, \"-\", HR.confint.upper, \")\")\n                          res&lt;-c(beta, HR, wald.test, p.value)\n                          names(res)&lt;-c(\"beta\", \"HR (95% CI for HR)\", \"wald.test\", \n                                        \"p.value\")\n                          return(res)\n                          #return(exp(cbind(coef(x),confint(x))))\n                         })\nres &lt;- t(as.data.frame(univ_results, check.names = FALSE))\nas.data.frame(res)\n\n\n           beta HR (95% CI for HR) wald.test p.value\nage       0.019            1 (1-1)       4.1   0.042\nsex       -0.53   0.59 (0.42-0.82)        10  0.0015\nph.karno -0.016      0.98 (0.97-1)       7.9   0.005\nph.ecog    0.48        1.6 (1.3-2)        18 2.7e-05\nwt.loss  0.0013         1 (0.99-1)      0.05    0.83\n\n\nThe output provides a detailed display of the regression beta coefficients, which quantify the relationship between each variable and overall survival. Additionally, it includes the hazard ratios that indicate the effect sizes associated with these variables, along with their statistical significance. Each factor is evaluated through distinct univariate Cox regression analyses, allowing for a clear understanding of how individual variables impact survival outcomes.\n\n\nMultivariate Cox regression analysis\nOur objective is to explore how several factors work together to affect survival outcomes. To achieve this, we will perform a multivariate Cox regression analysis, a statistical method that allows us to evaluate the impact of multiple variables simultaneously.\nIn our initial univariate analysis, we found that the variable ph.karno did not show significant relevance to survival, leading us to exclude it from our multivariate model. Instead, we will focus on three key factors that are likely to influence survival: sex, age, and ph.ecog.\nThe following is the specification for the Cox regression model, which examines the relationship between time to death and the selected time-constant covariates:\n\n\nCode\ncox.fit.multi &lt;- coxph(Surv(time, status) ~ age + sex + ph.ecog, data =  lung)\nsummary(cox.fit.multi)\n\n\nCall:\ncoxph(formula = Surv(time, status) ~ age + sex + ph.ecog, data = lung)\n\n  n= 227, number of events= 164 \n   (1 observation deleted due to missingness)\n\n             coef exp(coef)  se(coef)      z Pr(&gt;|z|)    \nage      0.011067  1.011128  0.009267  1.194 0.232416    \nsex     -0.552612  0.575445  0.167739 -3.294 0.000986 ***\nph.ecog  0.463728  1.589991  0.113577  4.083 4.45e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n        exp(coef) exp(-coef) lower .95 upper .95\nage        1.0111     0.9890    0.9929    1.0297\nsex        0.5754     1.7378    0.4142    0.7994\nph.ecog    1.5900     0.6289    1.2727    1.9864\n\nConcordance= 0.637  (se = 0.025 )\nLikelihood ratio test= 30.5  on 3 df,   p=1e-06\nWald test            = 29.93  on 3 df,   p=1e-06\nScore (logrank) test = 30.5  on 3 df,   p=1e-06\n\n\nThe p-values from the three tests (likelihood, Wald, and score) show that our model is significant. These tests check the overall hypothesis that all coefficients (\\(β\\)) are zero. In this case, the test results agree closely, allowing us to reject this hypothesis.\nIn the multivariate Cox analysis, the variables sex and ph.ecog are significant (\\(p &lt; 0.05\\)). However, the variable age is not significant (\\(p = 0.23\\), which is greater than 0.05).\nThe p-value for sex is 0.000986. The hazard ratio (HR) is exp(coef) = 0.58, meaning there is a strong link between sex and a lower risk of death. For example, when we keep other factors constant, being female (sex=2) reduces the risk by 42%. This suggests that being female is associated with a better outcome.\nThe p-value for ph.ecog is 4.45e-05, with a hazard ratio (HR) of 1.59. This indicates a strong link between higher ph.ecog values and an increased risk of death. Keeping other factors constant, a higher ph.ecog score suggests poorer survival.\nIn contrast, the p-value for age is p = 0.23. The hazard ratio (HR) is exp(coef) = 1.01, with a 95% confidence interval of 0.99 to 1.03. Since the confidence interval includes 1, this means age does not significantly affect the risk of death after adjusting for sex and ph.ecog values. An additional year of age increases the daily risk of death by about 1%, which is not significant.\n\n\nCode\ncoxph(Surv(time, status) ~  age + sex + ph.ecog, data = lung) %&gt;% \n  tbl_regression(exp = TRUE) \n\n\n\n\n\n\n\n\nCharacteristic\nHR\n95% CI\np-value\n\n\n\n\nage\n1.01\n0.99, 1.03\n0.2\n\n\nsex\n0.58\n0.41, 0.80\n&lt;0.001\n\n\nph.ecog\n1.59\n1.27, 1.99\n&lt;0.001\n\n\n\nAbbreviations: CI = Confidence Interval, HR = Hazard Ratio\n\n\n\n\n\n\n\n\n\n\nPlot the baseline survival function\n\n\nCode\n# Plot the baseline survival function\nautoplot(survfit(cox.fit.multi))\n\n\nWarning: `aes_string()` was deprecated in ggplot2 3.0.0.\nℹ Please use tidy evaluation idioms with `aes()`.\nℹ See also `vignette(\"ggplot2-in-packages\")` for more information.\nℹ The deprecated feature was likely used in the ggfortify package.\n  Please report the issue at &lt;https://github.com/sinhrks/ggfortify/issues&gt;.",
    "crumbs": [
      "**Semi-Parametric Methods**",
      "Cox Proportional Hazards Model"
    ]
  },
  {
    "objectID": "02-07-02-01-survival-analysis-cox-regression-r.html#summary-and-conclusions",
    "href": "02-07-02-01-survival-analysis-cox-regression-r.html#summary-and-conclusions",
    "title": "2.1 Cox Proportional Hazards Model",
    "section": "Summary and Conclusions",
    "text": "Summary and Conclusions\nThe Cox Proportional Hazards Model is a powerful tool for analyzing survival data and understanding how covariates impact the risk of an event over time. It is widely used in fields like medicine, epidemiology, and engineering, where time-to-event data is prevalent. The flexibility of not having to specify the baseline hazard makes it particularly popular for modeling survival data.\nIn this article, we described the Cox regression model for assessing simultaneously the relationship between multiple risk factors and patient’s survival time.",
    "crumbs": [
      "**Semi-Parametric Methods**",
      "Cox Proportional Hazards Model"
    ]
  },
  {
    "objectID": "02-07-02-01-survival-analysis-cox-regression-r.html#references",
    "href": "02-07-02-01-survival-analysis-cox-regression-r.html#references",
    "title": "2.1 Cox Proportional Hazards Model",
    "section": "References",
    "text": "References\n\nSurvival Analysis with R\nCox Proportional-Hazards Model\nSurvival Analysis in R\nSurvival Analysis with R",
    "crumbs": [
      "**Semi-Parametric Methods**",
      "Cox Proportional Hazards Model"
    ]
  },
  {
    "objectID": "02-07-02-02-survival-analysis-time-dependent-covariates-r.html#overview",
    "href": "02-07-02-02-survival-analysis-time-dependent-covariates-r.html#overview",
    "title": "2.2 Survival Analysis with Time-Dependent Covariates",
    "section": "Overview",
    "text": "Overview\nTime-dependent covariates (also called time-varying covariates) are predictor variables whose values change over time during the follow-up period of a survival study.\n\nExamples in Practice:\n\nMedical: Blood pressure, CD4 count, treatment status (started/stopped during study)\nEngineering: Temperature, stress levels, maintenance status of equipment\nEconomics: Employment status, income level, policy changes\nSocial Sciences: Marital status, education level, behavioral changes\n\n\n\nStandard vs. Time-Dependent Approach:\n\n\n\n\n\n\n\nStandard Survival Analysis\nTime-Dependent Survival Analysis\n\n\n\n\nUses baseline values only\nUses current values at time t\n\n\nAssumes covariates are fixed\nAllows covariates to evolve\n\n\nMay introduce bias if values change\nProvides more accurate risk assessment\n\n\n\n\n\nWhy Do We Need Time-Dependent Covariates?\n\nProblem with Fixed Covariates: Imagine studying the effect of blood pressure on heart attack risk:\nPatient A has BP = 120/80 at baseline but develops hypertension (160/100) after 6 months\nStandard analysis would still classify them as “low risk” based on baseline\nReality: Their risk increased when BP rose\nSolution: Time-dependent covariates allow the hazard at time t to depend on the current value of the covariate at that exact time.\n\n\n\nMathematical Framework\nStandard Cox Model:\n\\[\nh(t \\mid X) = h_0(t) \\exp(\\beta X)\n\\] Where \\(X\\) is fixed for each subject.\nCox Model with Time-Dependent Covariates:\n\\[\nh(t \\mid X(t)) = h_0(t) \\exp(\\beta X(t))\n\\] Where \\(X(t)\\) is the value of the covariate at time t.\nThe hazard ratio compares two individuals at the same time point t, using their current covariate values at that time.\n\n\nData Structure: Start-Stop Format\nTo implement time-dependent covariates, data must be structured in “start-stop” (or counting process) format:\n\n\n\nID\ntstart\ntstop\nevent\ncovariate\n\n\n\n\n1\n0\n30\n0\n0\n\n\n1\n30\n75\n1\n1\n\n\n2\n0\n45\n0\n0\n\n\n2\n45\n120\n0\n1\n\n\n\n\n\nRules:\n\nEach row represents a time interval [tstart, tstop)\nCovariate values are constant within each interval\nEvent occurs at the end of the interval where event = 1\nMultiple rows per subject are allowed\n\n\n\nImplementation Approaches\n\n\n1. True Time-Dependent Covariates\n\nCovariate values actually change during follow-up\nExample: Lab measurements, treatment initiation\nRequires data restructuring into start-stop format\n\n\n\n2. Time-Transformed Covariates (for PH violation)\n\nCreate interaction terms like covariate × log(time)\nUsed when proportional hazards assumption is violated\nDoesn’t require new data structure\n\n\n\n3. Cumulative Covariates\n\nUse running averages or cumulative exposure\nExample: Total drug dose received up to time t\n\n\n\nImportant Assumptions and Considerations\n\n\nValid Assumptions:\n\nNo future knowledge: Covariate at time t depends only on information available at or before t\nAccurate timing: Change points are recorded precisely\nPH assumption: Still applies to the time-dependent coefficients\n\n\n\nCommon Pitfalls:\n\n1. Immortal Time Bias\n\nWrong: Assign treatment status based on future events\nRight: Treatment status can only change based on past/current information\n\n\n\n2. Informative Censoring\n\nIf covariate measurement stops when patient deteriorates, this creates bias\nMissing data should be handled appropriately (e.g., last observation carried forward)\n\n\n\n3. Over-splitting\n\nToo many intervals can lead to computational issues\nBalance between accuracy and practicality",
    "crumbs": [
      "**Semi-Parametric Methods**",
      "Survival Analysis with Time-Dependent Covariates"
    ]
  },
  {
    "objectID": "02-07-07-04-survival-analysis-svm-r.html#overview",
    "href": "02-07-07-04-survival-analysis-svm-r.html#overview",
    "title": "2.7.4 Support Vector Machine (SVM) Survival Model",
    "section": "Overview",
    "text": "Overview\nSupport Vector Machine (SVM)-Based Survival Analysis is a machine learning approach that adapts the Support Vector Machine framework—originally designed for classification and regression—to survival analysis problems. Survival analysis deals with time-to-event data, where the outcome of interest is the time until an event occurs (e.g., death, machine failure, customer churn), and data is often 1censored** (i.e., the event has not occurred for some subjects by the end of the study).\nTraditional survival models like Cox Proportional Hazards are parametric or semi-parametric and make strong assumptions (e.g., proportional hazards). SVM-based survival analysis offers a non-parametric, non-linear, and robust alternative using the `maximum margin principle** of SVMs.\n\nKey Concepts\n\n\n\n\n\n\n\nConcept\nExplanation\n\n\n\n\nSurvival Data\n\\((t_i, \\delta_i, \\mathbf{x}_i)\\):  - \\(t_i\\): observed time (event or censoring)  - \\(\\delta_i = 1\\) if event occurred, \\(0\\) if censored  - \\(\\mathbf{x}_i\\): feature vector\n\n\nCensoring\nRight-censoring: event not observed during study (common)\n\n\nGoal\nRank subjects by risk or predict survival probability\n\n\n\n\n\nHow SVM is Adapted for Survival Analysis\nThere are two main formulations:\n\nRanking-Based SVM (Survival SVM) (Most common: e.g., Van Belle et al., 2011)\nInstead of predicting exact survival times, rank patients by their risk — those likely to experience the event sooner should have higher risk scores.\nLearn a function \\(f(\\mathbf{x})\\) such that for any uncensored pair \\((i, j)\\) where \\(t_i &lt; t_j\\) and \\(\\delta_i = 1\\), we have:\n\\[\nf(\\mathbf{x}_i) &gt; f(\\mathbf{x}_j)\n\\]\n(i.e., higher risk for earlier event)\nFor every pair where subject \\(i\\) fails before subject \\(j\\) (and \\(i\\) is not censored):\n\\[\nf(\\mathbf{x}_i) \\geq f(\\mathbf{x}_j) + 1 - \\xi_{ij}\n\\]\nwith slack \\(\\xi_{ij} \\geq 0\\)\nOptimization Problem:\n\\[\n\\min_{w, b, \\xi} \\ \\frac{1}{2} \\|w\\|^2 + C \\sum \\xi_{ij}\n\\] subject to ranking constraints only on comparable pairs (uncensored earlier events).\nThis is solved via quadratic programming or specialized algorithms.\n\n\nRegression-Based SVM (SVR for Survival) (e.g., Shivaswamy et al., 2007)\nTreat survival time as a regression target, but:\n\nCensored points provide lower bounds (event time \\(&gt; t_i\\))\nUse asymmetric loss (like in survival SVR)\n\nLoss Function: - For uncensored: standard \\(\\epsilon\\)-insensitive loss - For censored: only penalize if prediction is below observed time\n\n\n\nAdvantages of SVM-Based Survival Analysis\n\n\n\nAdvantage\nDescription\n\n\n\n\nNon-parametric\nNo assumption on survival distribution\n\n\nHandles non-linearity\nVia kernel trick (RBF, polynomial)\n\n\nRobust to outliers\nDue to margin maximization\n\n\nHandles high-dimensional data\nLike gene expression data\n\n\nCensoring-aware\nBuilt into the model\n\n\n\n\n\nLimitations\n\n\n\n\n\n\n\nLimitation\nExplanation\n\n\n\n\nScalability\n\\(O(n^2)\\) comparable pairs → slow for large \\(n\\)\n\n\nNo direct probability output\nGives risk score, not \\(S(t)\\)\n\n\nHard to interpret\nLike standard SVMs\n\n\nRequires careful censoring handling\nOnly right-censoring typically\n\n\n\n\n\nPractical Use Cases\n\nCancer prognosis using genomic data\nPredicting machine failure in predictive maintenance\nCustomer churn modeling in business\n\n\n\nComparison with Cox Model\n\n\n\nFeature\nCox PH\nSVM Survival\n\n\n\n\nAssumption\nProportional hazards\nNone\n\n\nOutput\nHazard ratios\nRisk ranking\n\n\nNon-linearity\nNo (unless engineered)\nYes (kernel)\n\n\nCensoring\nYes\nYes\n\n\nInterpretability\nHigh\nLow"
  },
  {
    "objectID": "02-07-07-04-survival-analysis-svm-r.html#svm-based-survival-analysisin-r",
    "href": "02-07-07-04-survival-analysis-svm-r.html#svm-based-survival-analysisin-r",
    "title": "2.7.4 Support Vector Machine (SVM) Survival Model",
    "section": "SVM-Based Survival Analysisin R",
    "text": "SVM-Based Survival Analysisin R\nThis tutorial demonstrates Support Vector Machine (SVM)-based survival analysis in R using the veteran dataset from the survival package. We’ll use the survivalsvm package, which implements ranking-based survival SVM.\n\nInstall Required R Packages\nFollowing R packages are required to run this notebook. If any of these packages are not installed, you can install them using the code below:\n\n\nCode\npackages &lt;-c(\n         'tidyverse',\n       'survivalsvm',\n       'survival',\n         'survcomp',\n         'survMisc',\n         'survminer',\n         'MASS'\n         \n         )\n\n\n```{r #| warning: false #| error: false"
  },
  {
    "objectID": "02-07-07-04-survival-analysis-svm-r.html#hyperparameter-tuning",
    "href": "02-07-07-04-survival-analysis-svm-r.html#hyperparameter-tuning",
    "title": "2.7.4 Support Vector Machine (SVM) Survival Model",
    "section": "Hyperparameter Tuning",
    "text": "Hyperparameter Tuning\n\nPrepare for Cross-Validation\n\n\nCode\nset.seed(42)\nk &lt;- 5\nn &lt;- nrow(veteran_clean)\nfolds &lt;- sample(rep(1:k, length.out = n))\n\n\n\n\nDefine Gamma Grid for Tuning\n\n\nCode\ngamma_grid &lt;- c(0.001, 0.01, 0.05, 0.1, 0.5, 1.0)\n\n\n\n\nCross-Validation Function for a Single Gamma Value\nCreate a function that, given a gamma and fold index, returns the C-index on the validation fold.\n\n\nCode\ncv_survsvm_one_gamma &lt;- function(gamma_val, fold_id, data, folds) {\n  # Split\n  train_idx &lt;- which(folds != fold_id)\n  val_idx   &lt;- which(folds == fold_id)\n  \n  train_data &lt;- data[train_idx, ]\n  val_data   &lt;- data[val_idx, ]\n  \n  # Re-align factor levels in validation set\n  for (col in names(val_data)) {\n    if (is.factor(train_data[[col]])) {\n      val_data[[col]] &lt;- factor(val_data[[col]], levels = levels(train_data[[col]]))\n    }\n  }\n  \n  # Fit Survival SVM (ranking type with add_kernel)\n  svm_fit &lt;- tryCatch({\n    survivalsvm(\n      Surv(time, event) ~ .,\n      data = train_data,\n      type = \"vanbelle1\",\n      gamma = gamma_val,\n      opt = \"quadprog\",\n      diff.meth = \"makediff3\",\n      kernel = \"add_kernel\"\n    )\n  }, error = function(e) {\n    return(NULL)\n  })\n  \n  if (is.null(svm_fit)) return(NA)\n  \n  # Predict\n  pred &lt;- predict(svm_fit, newdata = val_data)\n  risk &lt;- as.numeric(pred$predicted)\n  \n  # Safety: if prediction fails or lengths mismatch\n  if (length(risk) != nrow(val_data)) return(NA)\n  \n  # Compute C-index using modern concordance()\n  concord &lt;- tryCatch({\n    concordance(Surv(time, event) ~ risk, data = val_data)$concordance\n  }, error = function(e) {\n    return(NA)\n  })\n  \n  return(concord)\n}\n\n\n\n\nPerform Cross-Validation Over Gamma Grid\n\n\nCode\nresults &lt;- tibble(gamma = gamma_grid) %&gt;%\n  dplyr::mutate(\n    c_indices = map(gamma, ~ {\n      current_gamma &lt;- .x\n      map_dbl(1:k, ~ cv_survsvm_one_gamma(\n        gamma_val = current_gamma,\n        fold_id = .x,\n        data = veteran_clean,\n        folds = folds\n      ))\n    }),\n    mean_cindex = map_dbl(c_indices, ~ mean(.x, na.rm = TRUE)),\n    sd_cindex   = map_dbl(c_indices, ~ sd(.x, na.rm = TRUE))\n  )\n\n# View results\nprint(results)\n\n\n# A tibble: 6 × 4\n  gamma c_indices mean_cindex sd_cindex\n  &lt;dbl&gt; &lt;list&gt;          &lt;dbl&gt;     &lt;dbl&gt;\n1 0.001 &lt;dbl [5]&gt;       0.558    0.0806\n2 0.01  &lt;dbl [5]&gt;       0.558    0.0806\n3 0.05  &lt;dbl [5]&gt;       0.557    0.0837\n4 0.1   &lt;dbl [5]&gt;       0.540    0.0530\n5 0.5   &lt;dbl [5]&gt;       0.587    0.0813\n6 1     &lt;dbl [5]&gt;       0.621    0.104 \n\n\n\n\nSelect Best Gamma\n\n\nCode\nbest_row &lt;- results %&gt;% \n  arrange(desc(mean_cindex)) %&gt;% \n  slice(1)\n\nbest_gamma &lt;- best_row$gamma\ncat(\"Best gamma:\", best_gamma, \"\\n\")\n\n\nBest gamma: 1 \n\n\nCode\ncat(\"Mean C-index:\", round(best_row$mean_cindex, 4), \"±\", round(best_row$sd_cindex, 4), \"\\n\")\n\n\nMean C-index: 0.6206 ± 0.1039 \n\n\n\n\nRe-fit Final Model with Best Gamma\n\n\nCode\n# Re-fit final model\nfinal_svm &lt;- survivalsvm(\n  Surv(time, event) ~ .,\n  data = veteran_clean,\n  type = \"vanbelle1\",\n  gamma = best_gamma,\n  opt = \"quadprog\",\n  diff.meth = \"makediff3\",\n  kernel = \"add_kernel\"\n)\n\n\n\n\nRisk Stratification on Full Dataset\n\n\nCode\n# get risk scores for all patients\nfull_risk &lt;- predict(final_svm, newdata = veteran_clean)$predicted\nveteran_clean$risk &lt;- as.numeric(full_risk)\n\n# Risk stratification\nveteran_clean$risk_group &lt;- ifelse(\n  veteran_clean$risk &gt; median(veteran_clean$risk, na.rm = TRUE),\n  \"High Risk\", \"Low Risk\"\n)\n\n\n\n\nKaplan-Meier Plot for Final Model\n\n\nCode\n# Kaplan-Meier plot\nlibrary(survminer)\nkm_final &lt;- survfit(Surv(time, event) ~ risk_group, data = veteran_clean)\n\nggsurvplot(\n  km_final,\n  data = veteran_clean,\n  pval = TRUE,\n  conf.int = TRUE,\n  risk.table = TRUE,\n  legend.title = \"SVM Risk Group\",\n  title = paste(\"Kaplan-Meier: SVM (gamma =\", best_gamma, \")\"),\n  xlab = \"Time (days)\",\n  ylab = \"Survival Probability\"\n)\n\n\nIgnoring unknown labels:\n• colour : \"SVM Risk Group\"\n\n\n\n\n\n\n\n\n\n\n\nPlot C-index vs. Gamma\n\n\nCode\nggplot(results, aes(x = gamma, y = mean_cindex)) +\n  geom_line(group = 1) +\n  geom_point() +\n  geom_errorbar(aes(ymin = mean_cindex - sd_cindex,\n                    ymax = mean_cindex + sd_cindex),\n                width = 0.02) +\n  scale_x_log10() +\n  labs(\n    title = \"Survival SVM: C-index vs. Gamma (5-Fold CV)\",\n    x = \"Gamma (log scale)\",\n    y = \"Mean C-index\"\n  ) +\n  theme_minimal()"
  },
  {
    "objectID": "02-07-07-04-survival-analysis-svm-r.html#summary-and-conclusion",
    "href": "02-07-07-04-survival-analysis-svm-r.html#summary-and-conclusion",
    "title": "2.7.4 Support Vector Machine (SVM) Survival Model",
    "section": "Summary and Conclusion",
    "text": "Summary and Conclusion\nSurvival SVM is a powerful alternative to traditional survival models, especially for complex, high-dimensional data. However, it requires careful tuning and may not scale well to very large datasets. This tutorial demonstrated its implementation in R using the survivalsvm package, along with hyperparameter tuning via cross-validation. At the end, we visualized risk stratification using Kaplan-Meier curves and evaluated model performance using the C-index."
  },
  {
    "objectID": "02-07-07-04-survival-analysis-svm-r.html#resources",
    "href": "02-07-07-04-survival-analysis-svm-r.html#resources",
    "title": "2.7.4 Support Vector Machine (SVM) Survival Model",
    "section": "Resources",
    "text": "Resources\n\nVan Belle, V., Pelcmans, K., Van Huffel S. and Suykens J. A.K. (2011a). Improved performance on high-dimensional survival data by application of Survival-SVM. Bioinformatics (Oxford, England) 27, 87-94.\nVan Belle, V., Pelcmans, K., Van Huffel S. and Suykens J. A.K. (2011b). Support vector methods for survival analysis: a comparaison between ranking and regression approaches. Artificial Intelligence in medecine 53, 107-118.\nFouodo K. C. J. and König I. R. and Weihs C. and Ziegler A. and Wright M. N. (2018) Support Vector Machines for Survival Analysis with R. The R Journal 10, 412-423."
  },
  {
    "objectID": "02-07-07-04-survival-analysis-svm-r.html#svm-based-survival-analysis-from-scratch",
    "href": "02-07-07-04-survival-analysis-svm-r.html#svm-based-survival-analysis-from-scratch",
    "title": "2.7.4 Support Vector Machine (SVM) Survival Model",
    "section": "SVM-Based Survival Analysis from Scratch",
    "text": "SVM-Based Survival Analysis from Scratch\nThis section implements a simple ranking-based Survival SVM using Stochastic Gradient Descent (SGD) in R from scratch.\n\nRanking Hinge Loss\nRanking-based SVM for survival analysis focuses on correctly ordering patients by risk.\nFor every comparable pair ((i, j)) where:\n\nPatient (j) had an event (event[j] == 1)\nPatient (i) survived longer than (j) (time[i] &gt; time[j])\n\nWe want the model’s risk score to satisfy:\n\\[\nf(x_i) \\geq f(x_j) + 1\n\\]\nWe penalize violations using hinge loss:\n\\[\n\\mathcal{L}_{ij} = \\max\\left(0,\\ 1 - \\big(f(x_i) - f(x_j)\\big)\\right)\n\\]\nWith linear model (f(x) = w^x), total loss is:\n\\[\n\\mathcal{L}(w) = \\frac{\\lambda}{2} \\|w\\|^2 + \\frac{1}{|P|} \\sum_{(i,j) \\in P} \\max\\left(0,\\ 1 - w^\\top(x_i - x_j)\\right)\n\\]\nWe minimize this via SGD.\n\n\nGenerate Comparable Pairs (Once)\n\n\nCode\ngenerate_comparable_pairs &lt;- function(time, event) {\n  n &lt;- length(time)\n  pairs &lt;- list()\n  k &lt;- 1\n  for (i in 1:n) {\n    for (j in 1:n) {\n      if (event[j] == 1 && time[i] &gt; time[j]) {\n        pairs[[k]] &lt;- c(i = i, j = j)\n        k &lt;- k + 1\n      }\n    }\n  }\n  if (k == 1) return(NULL)\n  do.call(rbind, pairs)\n}\n\n\n\n\nSGD-Based Survival SVM Trainer\n\n\nCode\nsurvivalsvm_sgd &lt;- function(X, time, event,\n                            lambda = 0.01,      # L2 penalty (lambda = 1/C)\n                            lr = 0.01,          # learning rate\n                            epochs = 100,\n                            verbose = TRUE) {\n  \n  # Input checks\n  stopifnot(is.matrix(X) || is.data.frame(X))\n  X &lt;- as.matrix(X)\n  n &lt;- nrow(X); p &lt;- ncol(X)\n  \n  # Generate comparable pairs\n  pairs &lt;- generate_comparable_pairs(time, event)\n  if (is.null(pairs)) stop(\"No comparable pairs found!\")\n  m &lt;- nrow(pairs)\n  \n  if (verbose) cat(\"Number of comparable pairs:\", m, \"\\n\")\n  \n  # Initialize weights (small random)\n  set.seed(1)\n  w &lt;- rnorm(p, mean = 0, sd = 0.01)\n  \n  # Precompute feature differences for all pairs (memory vs speed trade-off)\n  # X_diff[k, ] = X[i, ] - X[j, ] for pair k = (i, j)\n  X_diff &lt;- matrix(0, nrow = m, ncol = p)\n  for (k in 1:m) {\n    i &lt;- pairs[k, \"i\"]\n    j &lt;- pairs[k, \"j\"]\n    X_diff[k, ] &lt;- X[i, ] - X[j, ]\n  }\n  \n  # SGD loop\n  for (epoch in 1:epochs) {\n    # Shuffle pair indices\n    idx &lt;- sample(m)\n    \n    total_loss &lt;- 0\n    \n    for (k in idx) {\n      diff &lt;- X_diff[k, ]              # x_i - x_j\n      margin &lt;- sum(w * diff)          # w^T (x_i - x_j)\n      loss &lt;- max(0, 1 - margin)       # hinge loss\n      \n      total_loss &lt;- total_loss + loss\n      \n      # Gradient update\n      if (margin &lt; 1) {\n        # Subgradient: - (x_i - x_j)\n        grad_w &lt;- lambda * w - diff\n      } else {\n        # Only L2 regularization gradient\n        grad_w &lt;- lambda * w\n      }\n      \n      # SGD step\n      w &lt;- w - lr * grad_w\n    }\n    \n    avg_loss &lt;- total_loss / m\n    if (verbose && epoch %% 20 == 0) {\n      cat(\"Epoch\", epoch, \"| Avg Loss:\", round(avg_loss, 4), \"\\n\")\n    }\n  }\n  \n  list(\n    weights = w,\n    n_epochs = epochs,\n    lambda = lambda,\n    lr = lr,\n    n_pairs = m\n  )\n}\n\n\n\n\nPrediction Function\n\n\nCode\npredict_survivalsvm_sgd &lt;- function(model, newdata) {\n  newdata &lt;- as.matrix(newdata)\n  as.vector(newdata %*% model$weights)\n}\n\n\n\n\nManual C-index (Harrell’s Concordance)\n\n\nCode\ncompute_cindex &lt;- function(risk, time, event) {\n  n &lt;- length(risk)\n  concordant &lt;- 0\n  usable &lt;- 0\n  \n  for (i in 1:(n-1)) {\n    for (j in (i+1):n) {\n      # Only consider pairs where at least one had an event\n      if (event[i] == 1 || event[j] == 1) {\n        # Only comparable if times are unequal\n        if (time[i] != time[j]) {\n          usable &lt;- usable + 1\n          # Concordant if higher risk = shorter survival\n          if ((risk[i] &gt; risk[j] && time[i] &lt; time[j]) ||\n              (risk[i] &lt; risk[j] && time[i] &gt; time[j])) {\n            concordant &lt;- concordant + 1\n          }\n          # Ties in risk: add 0.5 (optional, skipped here for simplicity)\n        }\n      }\n    }\n  }\n  \n  if (usable == 0) return(NA)\n  concordant / usable\n}\n\n\n\n\nExample\nWe’ll manually include the veteran data as a data frame\n\n\nCode\n# Simulate small survival data (base R only)\nset.seed(42)\nn &lt;- 50\np &lt;- 3\nX &lt;- matrix(rnorm(n * p), nrow = n)\ncolnames(X) &lt;- c(\"x1\", \"x2\", \"x3\")\n\n# True risk = x1 + 0.5*x2\ntrue_risk &lt;- X[,1] + 0.5 * X[,2]\n# Generate survival time ~ exp(-risk)\ntime &lt;- rexp(n, rate = exp(true_risk))\n# Event: 80% events, 20% censoring\nevent &lt;- rbinom(n, 1, 0.8)\n\n# Fit Survival SVM via SGD\nmodel_sgd &lt;- survivalsvm_sgd(\n  X, time, event,\n  lambda = 0.1,\n  lr = 0.05,\n  epochs = 100,\n  verbose = TRUE\n)\n\n\nNumber of comparable pairs: 1034 \nEpoch 20 | Avg Loss: 0.596 \nEpoch 40 | Avg Loss: 0.5989 \nEpoch 60 | Avg Loss: 0.5932 \nEpoch 80 | Avg Loss: 0.5974 \nEpoch 100 | Avg Loss: 0.5964 \n\n\nCode\n# Predict\nrisk_pred &lt;- predict_survivalsvm_sgd(model_sgd, X)\n\n# Evaluate\ncindex &lt;- compute_cindex(risk_pred, time, event)\ncat(\"\\nFinal C-index:\", round(cindex, 3), \"\\n\")\n\n\n\nFinal C-index: 0.235 \n\n\nCode\n# Compare to true risk\ncat(\"C-index (true risk):\", round(compute_cindex(true_risk, time, event), 3), \"\\n\")\n\n\nC-index (true risk): 0.747"
  },
  {
    "objectID": "02-07-07-04-survival-analysis-svm-r.html#svm-based-survival-analysis-r",
    "href": "02-07-07-04-survival-analysis-svm-r.html#svm-based-survival-analysis-r",
    "title": "2.7.4 Support Vector Machine (SVM) Survival Model",
    "section": "SVM-Based Survival Analysis R",
    "text": "SVM-Based Survival Analysis R\nThis tutorial demonstrates Support Vector Machine (SVM)-based survival analysis in R using the veteran dataset from the survival package. We’ll use the survivalsvm package, which implements ranking-based survival SVM.\n\nInstall Required R Packages\nFollowing R packages are required to run this notebook. If any of these packages are not installed, you can install them using the code below:\n\n\nCode\npackages &lt;-c(\n         'tidyverse',\n       'survivalsvm',\n       'survival',\n         'survcomp',\n         'survMisc',\n         'survminer',\n         'MASS'\n         \n         )\n\n\n```{r #| warning: false #| error: false"
  }
]